<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Meets LLMs: Towards Large Graph Models</title>
				<funder ref="#_SDKu2zc">
					<orgName type="full">China National Postdoctoral Program for Innovative Talents</orgName>
				</funder>
				<funder ref="#_4jJgfXS">
					<orgName type="full">China Postdoctoral Science Foundation</orgName>
				</funder>
				<funder ref="#_EEjjNnM">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
				<funder ref="#_jr5xBHH #_22yyCRA">
					<orgName type="full">Beijing National Research Center for Information Science and Technology</orgName>
				</funder>
				<funder>
					<orgName type="full">Beijing Key Lab of Networked Multimedia</orgName>
				</funder>
				<funder ref="#_vd4V8Hh #_rq45qsA #_BMJw6qB #_JZReUjC">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University. Beijing</orgName>
								<address>
									<postCode>100084</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University. Beijing</orgName>
								<address>
									<postCode>100084</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University. Beijing</orgName>
								<address>
									<postCode>100084</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yijian</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University. Beijing</orgName>
								<address>
									<postCode>100084</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University. Beijing</orgName>
								<address>
									<postCode>100084</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
							<email>wwzhu@tsinghua.edu.cnlihy18</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University. Beijing</orgName>
								<address>
									<postCode>100084</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Meets LLMs: Towards Large Graph Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5E2B12338871C2BFDADDB1D5F3D93945</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-01-20T08:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large models have emerged as the most recent groundbreaking achievements in artificial intelligence, and particularly machine learning. However, when it comes to graphs, large models have not achieved the same level of success as in other fields, such as natural language processing and computer vision. In order to promote applying large models for graphs forward, we present a perspective paper to discuss the challenges and opportunities associated with developing large graph models 1 . First, we discuss the desired characteristics of large graph models. Then, we present detailed discussions from three key perspectives: representation basis, graph data, and graph models. In each category, we provide a brief overview of recent advances and highlight the remaining challenges together with our visions. Finally, we discuss valuable applications of large graph models. We believe this perspective can encourage further investigations into large graph models, ultimately pushing us one step closer towards artificial general intelligence (AGI). We are the first to comprehensively study large graph models, to the best of our knowledge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, there has been a growing interesting in large models for both research and practical applications. Large models have been particularly revolutionary in fields such as natural language processing (NLP) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> and computer vision (CV) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, where pre-training extremely large models on large-scale unlabeled data has yielded significant breakthroughs. However, graphs, which are commonly used to represent relationships between entities in various domains such as social networks, molecule graphs, and transportation networks, have not yet seen the same level of success with large models as other domains. In this paper, we present a perspective about the challenges and opportunities associated with developing large graph models. First, we introduce large graph models and outline four key desired characteristics, including graph models with scaling laws, graph foundation model, in-context graph understanding and processing abilities, and versatile graph reasoning capabilities. Then, we offer detailed perspectives from three aspects: (1) For graph representation basis, we discuss graph domains and transferability, as well as the alignment of graphs with natural languages. Our key takeaway is the significance of identifying a suitable and unified representation basis that spans diverse graph domains, which serves as a fundamental step towards constructing effective large graph models; <ref type="bibr" target="#b1">(2)</ref> For graph data, we summarize and compare the existing graph datasets with other domains, and highlight that the availability of more large-scale high-quality graph data is resource-intensive yet indispensable; (3) For models, we systematically discuss backbone architectures, including graph neural networks and graph Transformers, as well as pre-training and post-processing techniques, such as prompting, parameter-efficient fine-tuning, and model compression. We also discuss LLMs as graph models, which is a newly trending direction. Finally, we discuss the significant impact that large graph models can have on various graph applications, including recommendation systems, knowledge graphs, molecules, finance, code and program, and urban computing and transportation. We hope that our paper can inspire further research into large graph models<ref type="foot" target="#foot_2">foot_2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Desired Characteristics of Large Graph Models</head><p>Similar to large language models (LLMs) <ref type="bibr" target="#b2">[3]</ref>, a large graph model can be characterized as a graph model with a vast number of parameters which empower it with abilities that are substantially more powerful than smaller models, thereby promoting the understanding, analyses, and processing of graph-related tasks. Apart from having numerous parameters, we summarize the key desired characteristics of an ideal large graph model from the following perspectives. An illustration of these characteristics is provided in Appendix A.</p><p>1. Graph models with scaling laws: The scaling laws indicate an empirical phenomenon where the performance of LLMs continues to improve as the model size, dataset size, and training computation increase <ref type="bibr" target="#b15">[16]</ref>. This phenomenon offers a clear direction for enhancing performance and empowering the model to capture complex patterns and relationships within graph data. By emulating the success of LLMs <ref type="bibr" target="#b16">[17]</ref>, a large graph model is expected to exhibit emergent abilities that smaller models lack. However, accomplishing this objective in large graph models is highly non-trivial, with difficulties span from collecting more graph data to solving technical problems such as addressing the over-smoothing and over-squashing problem of graph neural networks, along with engineering and system challenges.</p><p>2. Graph foundation model: A large graph model holds greater value when it can serve as a graph foundation model, i.e., capable of handling different graph tasks across various domains. This requires the model to gain understandings of the inherent structural information and properties of graphs to be equipped with "commonsense knowledge" of graphs. The graph pre-training paradigm is a highly promising path to develop graph foundation models, as it can expose the model to large-scale unlabeled graph data and reduce the reliance on expensive and laborious collection of graph labels. Besides, a generative pre-training paradigm can potentially empower the model with the ability to generate graphs, thereby opening up possibilities for valuable applications like drug synthesis, code modeling, and network evolution analysis <ref type="bibr" target="#b17">[18]</ref>. It is worthy clarifying that, since graphs serve as general data representations with extreme diversity, it is exceedingly challenging, if not unlikely, to develop a "universal graph model" for all graph domains. Therefore, multiple graph foundation models may be necessary for different "clusters of domains", which is somewhat different from LLMs or foundation models in computer vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>In-context graph understanding and processing abilities: An effective large graph model is expected to comprehend graph contexts, including nodes, edges, subgraphs, and entire graphs, and process novel graph datasets and tasks during testing with minimum samples as well as without intensive model modifications or changes in the paradigm. This characteristic is also closed related to and can facilitate capabilities of few-shot/zero-shot graph learning <ref type="bibr" target="#b18">[19]</ref>, multi-task graph learning <ref type="bibr" target="#b19">[20]</ref>, and graph out-of-distribution generalization <ref type="bibr" target="#b20">[21]</ref>. Moreover, these abilities are vital when the input graph data and task are different between the training and testing stages. In-context learning abilities can enable large graph models to leverage knowledge learned in the pre-training stage and quickly adapt to the testing stage with desired performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Versatile graph reasoning capabilities: Although graphs span diverse domains, there exist common and fundamental graph tasks. We generally refer to handling of these tasks as "graph reasoning". While there is no clear consensus on what these tasks are, some representative examples are provided as follows. Firstly, a large graph model should understand basic topological graph properties, such as graph sizes, node degrees, node connectivity, etc. These properties form the foundation for a deeper understanding of graph structures. Secondly, a large graph model should be able to flexibly and explicitly reason over multi-hop neighborhoods, enabling it to perform more sophisticated tasks. Such capabilities, akin to the chain-of-thought of LLMs <ref type="bibr" target="#b21">[22]</ref> in principle, can also enhance transparency in the graph decision-making process and improve model explainability <ref type="bibr" target="#b22">[23]</ref>. Lastly, besides local information, a large graph model should be able to understand and handle graph tasks that involve global properties and patterns, such as the centrality and position of nodes, overall properties of graphs, the evolution laws of dynamic graphs, etc.</p><p>3 Graph Representation Basis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph Domains and Transferability</head><p>Large models, LLMs, serve as foundation models <ref type="bibr" target="#b23">[24]</ref>, as they can be adapted to a wide range of downstream tasks after being pre-trained. The remarkable ability of LLMs stems from the underlying assumption of the existence of a common representation basis for various NLP tasks. For instance, word tokens for natural language processing are universal and information-preserving data representations that do not rely on specific tasks. In contrast, graphs are general data structures that span a multitude of domains. Therefore, the raw input data, i.e., nodes and edges, may not always be the most suitable representation basis for handling all graph data. Nodes and edges in social networks, molecule graphs, and knowledge graphs, for instance, have distinct meanings with their unique feature and topological space. Thus, directly sharing information and transferring knowledge based on input graph data often poses significant challenges.</p><p>It is widely believed that there exist more high-level or abstract common graph patterns, which can be shared across different graphs and tasks within a certain domain. For example, many human interpretable patterns have been identified in classical network science <ref type="bibr" target="#b24">[25]</ref>, such as homophily, small-world phenomenon, power-law distribution of node degrees, etc. Nevertheless, even with these high-level shared knowledge, creating effective large models that can perform well across diverse graph domains is still non-trivial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Aligning with Natural Languages</head><p>Another key competency of recent large models is their ability to interact with humans and follow instructions, as we are naturally capable of understanding languages and visual perceptions. In contrast, humans are less capable of handling graphs, especially more complex reasoning problems. As a result, communicating and instructing large models to behave for graph tasks the way we desire, especially using natural languages, is particularly challenging. We summarize three categories of strategies worth exploring to overcome this obstacle.</p><p>The first strategy is to align the representation basis of graphs and text through a large amount of paired data, similar to computer vision in principle. If successful, we will be able to interact with graph models using natural languages. For example, we can ask the model to generate molecule graphs with desired properties or ask the model to perform challenging graph reasoning tasks. Some initial attempts have been made for text-attributed graphs <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>, which serve as a good starting point. However, collecting such data for general graphs is much more costly and challenging than image-text pairs.</p><p>The second strategy is to transform graphs into natural languages, and then work solely in the language basis. Some initial attempts using this strategy have been developed, where graph structures are transformed into text representations, such as the adjacency list or the edge list, and inserted into LLMs as prompts. Then, natural languages are used to perform graph analytical tasks. We provide more detailed discussions in Section 5.4. However, directly transforming graph data and tasks into languages may lose the inner structure and inductive bias for graphs, resulting in unsatisfactory task performance. More delicate designs, such as effective prompts to convert graph structures and tasks into texts, are required to further advance this strategy.</p><p>The last category is to find other representation basis as a middle ground for different graph tasks and natural languages. The most straight-forward way is to use some hidden space of neural networks. However, it faces the challenge that deep neural networks are largely not explainable at the moment, not to mention that finding the desired shared hidden space can be frustratingly challenging. On the other hand, although humans are not capable of directly handling graph data, we can design appropriate algorithms to solve graph tasks, including many well-known algorithms in graph theory such as finding shortest paths, dynamic programming, etc. Therefore, if we can align the behavior of graph models with these algorithms, we can understand and control the behaviors of these models to a certain extent. Some efforts have been devoted in this direction, known as algorithmic reasoning <ref type="bibr" target="#b27">[28]</ref>, which we believe contains rich potentials.</p><p>In summary, finding the suitable representation basis, potentially aligning with natural languages, and unifying various graph tasks across different domains is one fundamental step towards building successful large graph models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Graph Data</head><p>The success of big models is largely dependent on the availability of high-quality, large-scale datasets.</p><p>For instance, GPT-3 was pre-trained on a corpus of approximately 500 billion tokens <ref type="bibr" target="#b0">[1]</ref>, while CLIP, a representative model that bridges natural language processing and computer vision, was trained on 400 million image-text pairs. It is reasonable to assume that even more data has been utilized in more recent large models, such as GPT-4 <ref type="bibr" target="#b28">[29]</ref>. This massive amount of data for NLP and CV tasks is typically sourced from publicly accessible human-generated content, such as web pages in CommonCrawl or user-posted photos in social media, which are easily collected from the web.</p><p>In contrast, large-scale graph data is not as easily accessible. There are typically two scenarios for graph data: numerous small-scale graphs, such as molecules, or a single/few large-scale graphs, such as social networks or citation graphs. For example, Open Graph Benchmark <ref type="bibr" target="#b29">[30]</ref>, one of the most representative public benchmarks for graph machine learning, includes two large graph datasets: MAG240M, which contains a large citation graph with approximately 240 million nodes and 1.3 billion edges, and PCQM4M, which contains approximately 4 million molecules. However, their scale is considerably lower than the datasets used in NLP or CV. If we treat each node in MAG240M as a token (though a node may contain arguably more information) or each graph in PCQM4M as an image, these graph datasets are at least 10 3 to 10 4 times smaller than their NLP or CV counterparts.</p><p>In addition to the data utilized for pre-training, commonly accepted and widely adopted benchmarks, such as SuperGLUE <ref type="bibr" target="#b30">[31]</ref> and BIG-bench <ref type="bibr" target="#b31">[32]</ref> for NLP and ImageNet <ref type="bibr" target="#b32">[33]</ref> for CV, have been found to be beneficial in the development of large models. These benchmarks are especially useful in assessing model quality and determining the most promising technical routes during the early stages. Although there are numerous benchmarks available for graph learning, such as Open Graph Benchmark <ref type="bibr" target="#b29">[30]</ref> and Benchmarking GNN <ref type="bibr" target="#b33">[34]</ref>, it is likely that their scope, including factors like scale, task and domain diversity, and evaluation protocols, may not be suitable or sufficient for evaluating large graph models. Therefore, the creation of more specialized benchmarks can further facilitate the progress of large graph models.</p><p>In summary, the availability of high-quality graph data is critical to the development of large graph models, which requires more resources and efforts. Since collecting such graph data is difficult and costly, community-wide collaboration may be essential to accelerate this process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Graph Models</head><p>In this section, we continue the discussion from the graph model aspect. Similar to large models in other domains, we divide our discussion into three topics: backbone architecture, pre-training, and post-processing. We also discuss LLMs as graph models, which is a recently trending direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Backbone Architecture</head><p>To date, Transformers <ref type="bibr" target="#b34">[35]</ref> have been the de facto standards for NLP and CV. However, no similar consensus has been reached for the graph domain. We briefly discuss two promising deep learning architectures for graphs: graph neural networks (GNNs) and graph transformers.</p><p>GNNs are the most popular deep learning architectures for graphs <ref type="bibr" target="#b35">[36]</ref> and have been extensively studied. Most representative GNNs adopt a message-passing paradigm, where nodes exchange messages with their neighbors to update their representations. GNNs can incorporate both structural information and semantic information such as node and edge attributes in an end-to-end manner. However, despite achieving considerable successes in many graph tasks, one key obstacle for further advancing GNNs into large models is their limited model capacity. As opposed to the scaling law in large models <ref type="bibr" target="#b15">[16]</ref>, the performance of GNNs saturates or even dramatically drops as the model size grows. Many research efforts have been devoted to explain this problem, such as over-smoothing <ref type="bibr" target="#b36">[37]</ref> and over-squashing <ref type="bibr" target="#b37">[38]</ref>, as well as strategies to alleviate it. Nevertheless, progress has not been groundbreaking. To date, most successful GNNs only have at most millions of parameters, and further scaling to billions of parameters leads to minimum or no additional improvement.</p><p>Graph Transformer is another architecture that extends and adapts the typical Transformers for graph data <ref type="bibr" target="#b38">[39]</ref>. In a nutshell, since classical Transformers cannot naturally process graph structures, Graph Transformer adopts various structure-encoding strategies to add graph structures to the input of Transformers <ref type="bibr" target="#b39">[40]</ref>. Graph Transformers evaluate the importance of each neighboring node, giving larger weights to nodes that provide more pertinent information. The self-attention mechanism empowers Graph Transformers the ability to dynamically adapt. One of the most successful graph Transformers is Graphormer <ref type="bibr" target="#b40">[41]</ref>, which ranked first in the PCQM4M molecule property prediction task of OGB Large-Scale Challenge <ref type="bibr" target="#b41">[42]</ref> in 2021. More efforts further improve Graph Transformer from various aspects including architecture designs, efficiency, model expressiveness, etc. For example, Structure-Aware Transformer (SAT) <ref type="bibr" target="#b42">[43]</ref> proposes a new self-attention mechanism to capture the structural similarity between nodes more effectively. AutoGT <ref type="bibr" target="#b39">[40]</ref> proposes a unified graph transformer formulation for existing graph transformer architectures and enhances the model performance using AutoML. To improve efficiency, General, Powerful, and Scalable graph Transformer (GPS) <ref type="bibr" target="#b43">[44]</ref> introduces a general framework with linear complexity by decoupling the local edge aggregation from the fully-connected Transformer. NAGphormer <ref type="bibr" target="#b44">[45]</ref> also aims to address the complexity challenge of graph Transformers for large graphs by treating different hops of neighbors as a sequence of token vectors. For the expressiveness, SEG-WL test <ref type="bibr" target="#b45">[46]</ref> introduces a graph isomorphism test algorithm, which can be used for assessing the structural discriminative power of graph Transformers. FeTA <ref type="bibr" target="#b46">[47]</ref> analyzes the expressiveness of graph Transformers in the spectral domain and proposes to perform attention on the entire graph spectrum.</p><p>We briefly summarize the key differences between GNNs and graph Transformers, while more discussions for the relationships between transformers and GNNs can be found <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref>:</p><p>• Aggregation vs. Attention: GNNs employ message passing functions to aggregate information from neighboring nodes, whereas Graph Transformers weigh contributions from neighbors using self-attentions, potentially enhancing the flexibility for large graph models.</p><p>• Modeling structures: GNNs naturally incorporate graph structures in the message passing functions as an inductive bias, while graph Transformers adopt pre-processing strategies, such as structureencoding, to incorporate structures.</p><p>• Depth and Over-smoothing: As aforementioned, deep GNNs may suffer from over-smoothing, leading to a decrease in their discriminative power. Graph Transformers, on the other hand, do not exhibit similar issues empirically. One plausible explanation is that Graph Transformers adaptively focus on more relevant nodes, enabling them to effectively filter and capture informative patterns.</p><p>• Scalability and Efficiency: GNNs, with their relatively simpler operations, may offer computational benefits for certain tasks. In contrast, the self-attention mechanism between node pairs in Graph Transformers can be computationally intensive, especially for large graphs. Considerable efforts have been dedicated to further enhancing the scalability and efficiency for both methods.</p><p>While both GNNs and Graph Transformers have made remarkable progress, it is not very clear which one, or some other architectures, may be best suited as the backbone for large graph models. Besides empirical evidence from trials and errors, further research into how large models work and what graph problems they may solve could bring principled advancements. It is also worth noting that most graph tasks relate to reasoning rather than perception. Therefore, the inductive bias in architecture designs usually does not come from mimicking human brains.</p><p>In our opinion, given the scale of existing graph datasets, GNNs are still a strong backbone model thanks to their strong inductive bias and expressive power. However, as the size of the training graph datasets continues to increase, graph Transformers may become more powerful through increasing the number of parameters and gradually become the prevailing approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Pre-training</head><p>Pre-training, as a widely adopted practice in NLP with well-known models like BERT <ref type="bibr" target="#b1">[2]</ref> and GPT <ref type="bibr" target="#b51">[52]</ref>, involves training a model on a massive dataset before applying it for specific tasks. The primary objective is to capture general patterns or knowledge present in the data and subsequently adapt the pre-trained model to meet downstream requirements. Graph pre-training, also known as unsupervised or self-supervised graph learning, has received significant attention in recent years <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref>. It aims to capture the inherent structural patterns within the training graph data, analogous to how language models capture the syntax and semantics of languages. As explained in Section 2, we recognize pre-training as an essential paradigm for large graph models. Next, we provide a more detailed discussion of graph pre-training.</p><p>Compared to the straightforward yet effective masking operation used in language modeling, graph pre-training strategies are more diverse and complicated, ranging from contrastive to predictive/generative approaches. Generally, graph pre-training methods leverage the rich structural and semantic information in the graph to introduce pretext learning tasks. Through these tasks, the pre-trained model learns useful node, edge, or graph-level representations without relying on explicitly annotated labels. In contrastive pre-training methods, positive and negative graph samples are constructed through various graph data augmentation techniques, followed by optimizing contrastive objectives, such as maximizing the mutual information between positive and negative pairs. On the other hand, in generative and predictive methods, specific components of the graph data, such as node features and edges, are first hide by masking. Then, the graph model aims to reconstruct the masked portions, which serve as pseudo-labels for pre-training. For more details, we refer readers to dedicated surveys <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref>.</p><p>We summarize the desired benefits of graph pre-training using the following "four-E" principle:</p><p>• Encoding structural information: Unlike pre-training methods for other types of data, such as languages and images, which focus primarily on semantic information, graphs contain rich structural information. Pre-training large graph models essentially needs to integrate structural and semantic information from diverse graph datasets. This also highlights the unique challenges and opportunities of graph pre-training.</p><p>• Easing data sparsity and label scarcity: Large graph models, with their substantial model capacity, are prone to overfitting when confronted with specific tasks that have limited labeled data. Pretraining on a wide range of graph datasets and tasks can act as a regularizing mechanism, preventing the model from overfitting to a specific task and improving generalization performance.</p><p>• Expanding applicability domains: One of the hallmarks of pre-training is the ability to transfer learned knowledge across various domains. By pre-training large graph models on diverse graph datasets, they should be able to capture a wide range of structural patterns, which can then be applied, adapted, or fine-tuned to graph data in similar domains, maximizing the model's utility.</p><p>• Enhancing robustness and generalization. Pre-training methods can expose large graph models to diverse graphs with distinct characteristics, including varying sizes, structures, and complexities. This exposure can potentially lead to more robust models that are less sensitive to adversarial perturbations <ref type="bibr" target="#b54">[55]</ref>. Moreover, models trained in this manner are more likely to generalize well to unseen graph data or novel graph tasks.</p><p>In summary, graph pre-training is not merely a beneficial or supplementary step, but a pivotal and necessary paradigm for large graph models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Post-processing</head><p>After obtaining a substantial amount of knowledge through pre-training, LLMs still require postprocessing to enhance their adaptability to downstream tasks. Representative post-processing techniques include prompting <ref type="bibr" target="#b55">[56]</ref>, parameter-efficient fine-tuning <ref type="bibr" target="#b56">[57]</ref>, reinforcement learning with human feedbacks <ref type="bibr" target="#b57">[58]</ref>, and model compression <ref type="bibr" target="#b58">[59]</ref>. For graphs, some recent efforts have also been devoted to study post-processing techniques for pre-trained models.</p><p>Prompting originally refers to methods that provide specific instructions to language models for generating desired contents for downstream tasks. Recently, constructing prompts with an in-context learning template demonstrates great effectiveness in LLMs <ref type="bibr" target="#b59">[60]</ref>. Language prompts usually contain a task description and a few examples to illustrate the downstream tasks. Graph prompts, which mimic natural language prompts to enhance downstream task performance with limited labels and enable interaction with the model to extract valuable knowledge, have been extensively studied <ref type="bibr" target="#b60">[61]</ref>. One significant challenge for graph prompts is the unification of diverse graph tasks, spanning from nodelevel and link-level to graph-level tasks. In contrast, tasks in natural language can be easily unified as language modeling under specific constraints. To tackle this challenge, GPPT <ref type="bibr" target="#b61">[62]</ref> unifies graph tasks into edge prediction, considering that a typical node classification task can be reformulated as the link prediction task between the structure-token and the task-token. Each structure-token represents a node in the graph data, and each task-token corresponds to a class. GraphPrompt <ref type="bibr" target="#b60">[61]</ref> further extends the idea and unifies link prediction, node classification, and graph classification as subgraph similarity calculation by describing node and graph classes as prototypical subgraphs. Similarly, ProG <ref type="bibr" target="#b62">[63]</ref> reformulates node and edge-level tasks as graph-level tasks and further proposes multi-task prompting by realizing prompting as a learnable token that is directly added to the node feature, mirroring the prefix phrase prompting technique in NLP. ProG also employs meta learning to learn prompting for different tasks. Other graph prompts such as PRODIGY <ref type="bibr" target="#b63">[64]</ref>, GPF <ref type="bibr" target="#b64">[65]</ref>, Gare <ref type="bibr" target="#b65">[66]</ref>, SGL-PT <ref type="bibr" target="#b66">[67]</ref>, DeepGPT <ref type="bibr" target="#b67">[68]</ref>, and G-Prompt <ref type="bibr" target="#b68">[69]</ref> follow similar principles.</p><p>Parameter-efficient fine-tuning refers to techniques where only a small portions of model parameters are optimized, while the rest is kept fixed. Besides reducing computational costs, it also helps to enable the model to adapt to new tasks without forgetting the knowledge obtained in pre-training, preserving the general capabilities of the model while allowing for task-specific adaptation. Graph parameterefficient fine-tuning has also recently begun to received attention. For example, AdapterGNN <ref type="bibr" target="#b69">[70]</ref> and G-Adapter <ref type="bibr" target="#b70">[71]</ref> both investigate adapter-based fine-tuning techniques for graph models, aiming to reduce the number of tuneable parameters while preserving comparable accuracy. Specifically, AdapterGNN tunes GNNs by incorporating two adapters, one inserted before and another one after the message passing process. On the other hand, G-Adapter focuses on graph transformers and introduces a message passing process within the adapter to better utilize graph structural information. S2PGNN <ref type="bibr" target="#b71">[72]</ref> further proposes to search for architecture modifications to improve the adaptivity of the fine-tuning stage.</p><p>Model compression aims to reduce the memory and computational demands of models through various techniques, including knowledge distillation, pruning, and quantization, which are particularly valuable when deploying large models in resource-constrained environments. Here, we focus on quantization, which has gained popularity and proven effectiveness in LLMs <ref type="bibr" target="#b2">[3]</ref>, and refer readers to dedicated surveys for other methods <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75]</ref>. Quantization entails reducing the precision of numerical values used by the model while preserving model performance to the greatest extent possible. In the case of large models, post-training quantization (PTQ) is particularly preferred, as it does not require retraining. PTQ in graph learning has also been explored in SGQuant <ref type="bibr" target="#b75">[76]</ref>, which proposes a multi-granularity quantization technique that operates at various levels, including graph topology, layers, and components within a layer. Other methods such as Degree-Quant <ref type="bibr" target="#b76">[77]</ref>, BiFeat <ref type="bibr" target="#b77">[78]</ref>, Tango <ref type="bibr" target="#b78">[79]</ref>, VQGraph <ref type="bibr" target="#b79">[80]</ref>,</p><formula xml:id="formula_0">A 2 Q [81],</formula><p>and AdaQP <ref type="bibr" target="#b81">[82]</ref> adopt a quantization-aware training scheme, which are inspiring but cannot be used standalone during the post-processing stage.</p><p>In summary, the success of post-processing techniques shown in LLMs has sparked interest in similar research in the graph domain. However, due to the unavailability of large graph models at present, the assessment of these methods is limited to relatively small models. Therefore, it is crucial to further verify their effectiveness when applied to large graph models, and more research challenges and opportunities may arise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">LLMs as Graph Models</head><p>Recent research has also explored the potential of directly utilizing LLMs for solving graph tasks. The essential idea is to transform graph data, including both graph structures and features, as well as graph tasks, into natural language representations, thereby treating graph problems as regular NLP problems. In the following discussion, we provide a brief overview of these advancements. More detailed discussions are provided at Appendix B.</p><p>NLGraph <ref type="bibr" target="#b82">[83]</ref> conducts a systematic evaluation of LLMs, such as GPT-3 and GPT-4, on eight graph reasoning tasks in natural language, spanning varying levels of complexity, including connectivity, shortest path, maximum flow, simulating GNNs, etc. It empirically finds that LLMs show preliminary graph reasoning abilities, but struggle with more complex graph problems, potentially because they solely capture spurious correlations within the problem settings. Meanwhile, GPT4Graph <ref type="bibr" target="#b83">[84]</ref> also conducts extensive experiments to evaluate the graph understanding capabilities of LLMs across ten distinct tasks, such as graph size and degree detection, neighbor and attribute retrieval, etc. It reveals the limitations of LLMs in graph reasoning and emphasizes the necessity of enhancing their structural understanding capabilities. LLMtoGraph <ref type="bibr" target="#b84">[85]</ref> also tests GPT-3.5 and GPT-4 for various graph tasks and makes some interesting observations.</p><p>More recently, Graph-LLM <ref type="bibr" target="#b85">[86]</ref> systematically investigates the utilization of LLMs in text-attributed graph through two strategies: LLMs-as-Enhancers, where LLMs enhance the representations of text attributes of nodes before passing them to GNNs, and LLMs-as-Predictors, where LLMs are directly employed as predictors. Comprehensive studies have been conducted on these two pipelines across various settings, and the empirical results provide valuable insights into further leveraging LLMs for graph machine learning. InstructGLM <ref type="bibr" target="#b86">[87]</ref> further introduces scalable prompts designed to describe the graph structures and features for LLM instruction tuning, which enables tuned LLMs to perform various graph tasks during the inference stage in a generative manner. Experiments conducted on GNN benchmarks empirically show the strong potential of adopting LLMs for graph machine learning.</p><p>Although still in their early stages, these works highlight that LLMs also represent a promising avenue for developing large graph models, which is worthy further exploration and investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Summary</head><p>To summarize, substantial research efforts have been devoted to studying various aspects of graph models. However, there is currently no clear framework for effectively integrating these techniques into large graph models. Consequently, more efforts are required to compare existing methods and develop advanced models. In this endeavor, automated graph machine learning techniques <ref type="bibr" target="#b87">[88]</ref>, such as graph neural architecture search, can be valuable in reducing human effort and accelerating the trial-and-error process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Applications</head><p>Instead of attempting to overwhelmingly handle various graph domains and tasks, it may be more effective to focus on specific graph-related vertical fields by leveraging domain knowledge and domain-specific datasets. In this section, we highlight several graph application scenarios that can significantly benefit from large graph models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Recommendation System</head><p>Graph data naturally exists in recommendation systems. For example, the interaction between users and items can be modeled as a bipartite graph or more complex heterogeneous graphs that include clicks, buys, reviews, and more. Currently, LLMs for recommendation systems focus on modeling semantic information <ref type="bibr" target="#b88">[89]</ref>, while explicitly utilizing the structural information of graphs has the potential to yield better results <ref type="bibr" target="#b89">[90]</ref>. A potential challenge is that graphs in recommendation system are usually multi-modal <ref type="bibr" target="#b90">[91]</ref>, covering text, images, interactions, etc. Since large models for multimodal data are not yet mature, significant efforts are needed to develop truly effective large graph models for recommendation systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Knowledge Graph</head><p>Knowledge graphs are widely adopted to store and utilize ubiquitous knowledge in human society.</p><p>LLMs have been used for various knowledge graph tasks <ref type="bibr" target="#b91">[92,</ref><ref type="bibr" target="#b86">87]</ref>, including construction, completion, and question answering. Despite their achievements, most of these methods focus primarily on the textual information, leaving the structural and relational information of knowledge graphs underexplored. Large graph models, potentially combined with existing LLMs, can greatly complement the status quo and further promote research and application of knowledge graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Molecules</head><p>Graphs are natural representations for molecules, where nodes represent atoms and edges indicate bonds. Building effective graph models for molecules can advance various applications, including molecular property prediction and molecular dynamics simulations, ultimately benefiting drug discovery. Currently, some variants of LLMs are applied to molecules <ref type="bibr" target="#b92">[93,</ref><ref type="bibr" target="#b93">94]</ref> by first transforming molecules into strings using SMILES <ref type="bibr" target="#b94">[95]</ref>, which allows molecules to be represented and generated as regular texts. Nevertheless, graphs serve as a more natural way to represent the structural information of molecules with numerous modeling advantages <ref type="bibr" target="#b95">[96]</ref>. Meanwhile, a great number of graphbased pre-training techniques have also been developed for molecules <ref type="bibr" target="#b96">[97]</ref>, including multi-modal strategies <ref type="bibr" target="#b97">[98]</ref>. Besides, molecule data is relatively easier to collect, e.g., ZINC20 <ref type="bibr" target="#b98">[99]</ref> contains millions of purchasable compounds. Therefore, we believe graph-based or graph-enhanced large models for molecule modeling can soon to be expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Finance</head><p>Graph machine learning has proven to be beneficial for multiple financial tasks such as stock movement prediction and loan risk prediction <ref type="bibr" target="#b99">[100]</ref>. Moreover, the large abundance of financial data makes it possible to construct domain-specific large models, exemplified by BloombergGPT <ref type="bibr" target="#b100">[101]</ref>. By combining the strengths of both worlds, the application of large graph models in the field of finance holds great promise. A potential challenge lies in the sensitive and private nature of most financial data, making industries reluctant to release related models and data to the public. Efforts are required to promote open-source initiatives and democratization <ref type="bibr" target="#b101">[102,</ref><ref type="bibr" target="#b102">103]</ref> to fully unleash the potential of large graph models in the finance area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Code and Program</head><p>Thanks to the large amount of code data available on repository hosting platforms such as GitHub, LLMs show remarkable ability in understanding and generating codes and programs. Notable examples include CodeX <ref type="bibr" target="#b103">[104]</ref>, AlphaCode <ref type="bibr" target="#b104">[105]</ref>, and GPT-4 <ref type="bibr" target="#b28">[29]</ref>, which have exerted a significant impact on the programming landscape, potentially even reshaping it. In addition to treating codes and programs as textual data, graphs offer a natural means to represent the structural aspects of codes. For example, abstract syntax trees, including control flow graph, data flow graph, etc., effectively capture the syntactic structure of source codes <ref type="bibr" target="#b105">[106]</ref>. Studies have demonstrated that the integration of graphs can further enhance the performance of LLMs by providing complementary information <ref type="bibr" target="#b106">[107]</ref>. Therefore, large graph models hold valuable potential for a wide range of code and program-related tasks, including code completion and generation, code search, code review, program analysis and testing, among others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Urban Computing and Transportation</head><p>Graph data is pervasive in the domains of urban computing and transportation, such as road networks. Therefore, graph machine learning can benefit many applications, including traffic forecasting, various urban planning and management tasks, crime prediction, and epidemic control <ref type="bibr" target="#b107">[108,</ref><ref type="bibr" target="#b108">109]</ref>. Moreover, large-scale urban data naturally exists, such as mobility data collected from GPS and diverse sensors. Currently, some LLM-based large models have been explored for urban computing and transportation, such as TransGPT <ref type="bibr">[110]</ref>. Nevertheless, their focus has primarily revolved around natural language related applications, leaving developing large graph models for broader and more comprehensive utilization still an open opportunity. One major technical challenge in the process lies in that graph data in urban and transportation contexts is dynamic in nature, containing complicated spatial-temporal patterns. Thus, a large graph model needs to effectively capture both structural and temporal information to achieve satisfactory performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Beyond</head><p>The application scenarios we have outlined above are by no means exhaustive. Considering that graph machine learning has been widely adopted across diverse domains ranging from industrial applications, such as fault diagnosis <ref type="bibr" target="#b109">[111]</ref>, IoT <ref type="bibr" target="#b110">[112]</ref>, power systems <ref type="bibr" target="#b111">[113]</ref>, and time-series analysis <ref type="bibr" target="#b112">[114]</ref>, to AI for science <ref type="bibr" target="#b113">[115]</ref>, such as physics <ref type="bibr" target="#b114">[116,</ref><ref type="bibr" target="#b115">117]</ref>, combinatorial optimization <ref type="bibr" target="#b116">[118]</ref>, material science <ref type="bibr" target="#b117">[119]</ref>, and neural science <ref type="bibr" target="#b118">[120]</ref>, exploring the usage of large graph models holds extremely rich potentials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In summary, large graph models can potentially revolutionize the field of graph machine learning, but they also give rise to a multitude of challenges, ranging from the representation basis, graph data, graph models, and applications. Meanwhile, promising endeavors are being undertaken to tackle these challenges, creating exciting opportunities for both researchers and practitioners. We hope that our perspective will inspire continued efforts and advancements for large graph models.</p><p>• Tunable Components: which parts of the model can be fine-tuned, such as GNNs, graph Transformers, and LLMs. • Receptive Field: how many hops of neighbors can be perceived when making predictions. K-hop indicates the receptive field is determined by the architectures, e.g., the number of layers in GNNs.</p><p>One key challenge of using LLMs as graph models is to model graph structures and inject them into LLMs. As this is usually achieved through prompts, we make the following summarization:</p><p>• Prompt Type: whether the model uses textual prompts (i.e., descriptions in texts) or neural prompts (e.g., through hidden layers in neural networks). • Prompt Details: the details of the prompt. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Graph Data</head><p>In this section, we summarize some principles that are helpful while collecting more graph data.</p><p>• Domain diversity: To enable large graph models handle different graph applications, it is crucial to expose the model to different domains of interests, so that large graph model can be adopted across various fields. • Type diversity: Graphs have rich types, including homogeneous and heterogeneous, homophily and heterophily, static and dynamic, directed and undirected, weighted and unweighted, signed and unsigned, etc. The diversity of graph type is also important to empower the large graph model handle diverse downstream graphs. • Statistics diversity: Graphs also have varying statistics, e.g., size, density, degree distribution, etc.</p><p>Such diversity should be considered to ensure the effectiveness of large graph model. • Task diversity: Graph tasks are also distinct, ranging from node-level, edge-level to graph-level, and from discriminative tasks such as classification and prediction to generative tasks such as graph generation. Increasing the task diversity in pre-training or post-processing phase can help developing effective large graph models. • Modality diversity: Graphs can combine different modalities of data, such as text, images, and tabular data, which can enrich the utility of the large graph model.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Common textual prompts include adjacency lists and neighborhood descriptions, and typical neural prompts include GNNs and graph Transformers. • Advanced graph-specific prompt: the types of advanced graph-specific prompts, if they are proposed in the paper. Lastly, we summarize the graph data used in the experiments. Note that we focus on the experiments conducted in the original papers, but extensions are possible, e.g., handling larger graphs by using more computational resources or applying the model to other tasks through minor modifications. • Dataset Type: what type of graphs are utilized in the experiments, including synthetic graphs, TAGs, knowledge graphs (KGs), and general graphs. • Tasks: what type of tasks are considered in the experiments, including algorithmic tasks (various from degree counting to finding shortest paths, etc.), node classification, link prediction, question answering (QA), etc. • #Nodes: the approximate number of nodes handled by the model. If sampling is applied, we only count the sampled nodes. • Node Feature: whether and what type of node features can be utilized in the model, including no attributes, text attributes, and general attributes.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>A summarization of different models related to LLMs as graph models.</figDesc><table><row><cell></cell><cell>Node</cell><cell>Feature</cell></row><row><cell></cell><cell cols="2"># Nodes</cell></row><row><cell>Graph Data</cell><cell cols="2">Tasks</cell></row><row><cell></cell><cell>Dataset</cell><cell>Type</cell></row><row><cell>Modeling Graph Structure for LLMs</cell><cell cols="2">Prompt Details Advanced Graph-specific Prompt</cell></row><row><cell></cell><cell>Prompt</cell><cell>Type</cell></row><row><cell></cell><cell>Receptive</cell><cell>Field</cell></row><row><cell></cell><cell>Tunable</cell><cell>Components</cell></row><row><cell>Architecture</cell><cell>Need</cell><cell>Finetune</cell></row><row><cell></cell><cell cols="2">LLMs</cell></row><row><cell></cell><cell>Final</cell><cell>Predictor</cell></row><row><cell></cell><cell>Method</cell><cell>NLGraph [83]</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We maintain a curated paper list at https://github.com/THUMNLab/awesome-large-graph-model.NeurIPS</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2023" xml:id="foot_1"><p>New Frontiers in Graph Learning Workshop (NeurIPS GLFrontiers 2023).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>There are also works to use graphs to improve large language models, such as enhancing their reasoning ability<ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> or using graphs as tools<ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, which is beyond the scope of this paper.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work was supported by the <rs type="funder">National Key Research and Development Program of China</rs> No. <rs type="grantNumber">2020AAA0106300</rs>, <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62222209</rs>, <rs type="grantNumber">62250008</rs>, <rs type="grantNumber">62102222</rs>, <rs type="grantNumber">62206149</rs>), <rs type="funder">Beijing National Research Center for Information Science and Technology</rs> under Grant No. <rs type="grantNumber">BNR2023RC01003</rs>, <rs type="grantNumber">BNR2023TD03006</rs>, <rs type="funder">China National Postdoctoral Program for Innovative Talents</rs> No. <rs type="grantNumber">BX20220185</rs>, <rs type="funder">China Postdoctoral Science Foundation</rs> No. <rs type="grantNumber">2022M711813</rs>, and <rs type="funder">Beijing Key Lab of Networked Multimedia</rs>. All opinions, findings, conclusions, and recommendations in this paper are those of the authors and do not necessarily reflect the views of the funding agencies. <rs type="person">Xin Wang</rs> and <rs type="person">Wenwu Zhu</rs> are corresponding authors.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_EEjjNnM">
					<idno type="grant-number">2020AAA0106300</idno>
				</org>
				<org type="funding" xml:id="_vd4V8Hh">
					<idno type="grant-number">62222209</idno>
				</org>
				<org type="funding" xml:id="_rq45qsA">
					<idno type="grant-number">62250008</idno>
				</org>
				<org type="funding" xml:id="_BMJw6qB">
					<idno type="grant-number">62102222</idno>
				</org>
				<org type="funding" xml:id="_JZReUjC">
					<idno type="grant-number">62206149</idno>
				</org>
				<org type="funding" xml:id="_jr5xBHH">
					<idno type="grant-number">BNR2023RC01003</idno>
				</org>
				<org type="funding" xml:id="_22yyCRA">
					<idno type="grant-number">BNR2023TD03006</idno>
				</org>
				<org type="funding" xml:id="_SDKu2zc">
					<idno type="grant-number">BX20220185</idno>
				</org>
				<org type="funding" xml:id="_4jJgfXS">
					<idno type="grant-number">2022M711813</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A An Illustration of Desired Characteristics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Detailed Discussions for LLMs as Graph Models</head><p>In this section, we summarize and compare different models related to LLMs as graph models. The overall summarization is shown in Table <ref type="table">1</ref>. Specifically, we category the key features into three groups: model architectures, modeling Graph structure for LLMs, and graph data.</p><p>For model architectures, we summarize the following designs:</p><p>• Final Predictor: whether the model utilizes GNNs or LLMs to get the final prediction.</p><p>• LLMs: which LLMs are utilized in the model. Typical examples include GPT-3 <ref type="bibr" target="#b0">[1]</ref>, GPT-4 <ref type="bibr" target="#b28">[29]</ref>, Llama 2 <ref type="bibr" target="#b119">[121]</ref>, etc.</p><p>• Need Fine-tuning: whether the model needs to be fine-tuned. Note that if the model does not necessarily require fine-tuning, but could be fine-tuned to further improve the performance, we mark it as no. Note that close-sourced LLMs such as GPT-3 and GPT-4 are not tunable.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqian</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beichen</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zican</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.18223</idno>
		<title level="m">A survey of large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhila</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Rolland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spencer</forename><surname>Whitehead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.02643</idno>
		<title level="m">Segment anything</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph</title>
		<author>
			<persName><forename type="first">Jiashuo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengjin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lumingyuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saizhuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.07697</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Beyond chain-of-thought, effective graph-of-thought reasoning in large language models</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.16582</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Thinking like an expert: Multimodal hypergraph-of-thought (hot) reasoning to boost foundation modals</title>
		<author>
			<persName><forename type="first">Fanglong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changyuan</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jintao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zequn</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.06207</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Enhancing reasoning capabilities of large language models: A graph-based verification approach</title>
		<author>
			<persName><forename type="first">Lang</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.09267</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Boosting logical reasoning in large language models through a new framework: The graph of thought</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiwen</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><surname>Ding</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.08614</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Graph of thoughts: Solving elaborate problems with large language models</title>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Besta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Blach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ales</forename><surname>Kubicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Gerstenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Gianinazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><surname>Gajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Podstawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Niewiadomski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Nyczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.09687</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Mindmap: Knowledge graph prompting sparks graph of thoughts in large language models</title>
		<author>
			<persName><forename type="first">Yilin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.09729</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.11116</idno>
		<title level="m">Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Structgpt: A general framework for large language model to reason over structured data</title>
		<author>
			<persName><forename type="first">Jinhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zican</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.09645</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Scaling laws for neural language models</title>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Emergent abilities of large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A systematic survey on deep generative models for graph generation</title>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="5370" to="5390" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Few-shot learning on graphs</title>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaize</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st International Joint Conference on Artificial Intelligence, IJCAI 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5662" to="5669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised graph neural networks enable stronger task generalization</title>
		<author>
			<persName><forename type="first">Mingxuan</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianlong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Out-of-distribution generalization on graphs: A survey</title>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07987</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24824" to="24837" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Explainability in graph neural networks: A taxonomic survey</title>
		<author>
			<persName><forename type="first">Haiyang</forename><surname>Hao Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shurui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="5782" to="5799" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simran</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Sydney Von Arx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeannette</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Brunskill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyamal</forename><surname>Brynjolfsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dallas</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niladri</forename><surname>Castellon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annie</forename><surname>Chatterji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">Quincy</forename><surname>Creel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dora</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Demszky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moussa</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esin</forename><surname>Doumbouya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kawin</forename><surname>Etchemendy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Ethayarajh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauren</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Gillespie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shelby</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">E</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saahil</forename><surname>Icard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratyusha</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Kalluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Karamcheti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fereshte</forename><surname>Keeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Khani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ranjay</forename><surname>Krass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohith</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Kuditipudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mina</forename><surname>Ladhak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><surname>Levent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suvir</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mirchandani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zanele</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suraj</forename><surname>Munyikwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avanika</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allen</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Nilforoshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giray</forename><surname>Nyarko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurel</forename><surname>Ogut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Piech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Portelance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Reich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frieda</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuf</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Camilo</forename><surname>Roohani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dorsa</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiori</forename><surname>Sadigh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keshav</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnan</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Tamkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armin</forename><forename type="middle">W</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rose</forename><forename type="middle">E</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xikun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaitlyn</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.18223</idno>
		<title level="m">On the opportunities and risks of foundation models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Networks</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Newman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Oxford university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Explanations as features: Llm-based features for text-attributed graphs</title>
		<author>
			<persName><forename type="first">Xiaoxin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.19523</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Simteg: A frustratingly simple approach improves textual graph learning</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Keyu Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><forename type="middle">Tsang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.02565</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural algorithmic reasoning</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patterns</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">100273</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22118" to="22133" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Beyond the imitation game: Quantifying and extrapolating the capabilities of language models</title>
		<author>
			<persName><surname>Big</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Benchmarking graph neural networks</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">43</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning on graphs: A survey</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="249" to="270" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A survey on oversmoothing in graph neural networks</title>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SAM Research Report</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Understanding over-squashing and bottlenecks on graphs via curvature</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Topping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><forename type="middle">Di</forename><surname>Giovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Paul Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Erxue</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runfa</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangfei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.08455</idno>
		<title level="m">Transformer for graphs: An overview from architecture perspective</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Autogt: Automated graph transformer architecture search</title>
		<author>
			<persName><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoyu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Do transformers really perform badly for graph representation</title>
		<author>
			<persName><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="28877" to="28888" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ogb-lsc: A large-scale challenge for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maho</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Structure-aware transformer for graph representation learning</title>
		<author>
			<persName><forename type="first">Dexiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Bray</surname></persName>
		</author>
		<author>
			<persName><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Machine Learning</title>
		<meeting>the 39th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="3469" to="3489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recipe for a general, powerful, scalable graph transformer</title>
		<author>
			<persName><forename type="first">Ladislav</forename><surname>Rampášek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">Prakash</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="14501" to="14515" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">NAGphormer: A tokenized graph transformer for node classification in large graphs</title>
		<author>
			<persName><forename type="first">Jinsong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On structural expressive power of graph transformers</title>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guojie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3628" to="3637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">How expressive are transformers in spectral domain for graphs</title>
		<author>
			<persName><forename type="first">Anson</forename><surname>Bastos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Nadgeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuldeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroki</forename><surname>Kanezashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toyotaro</forename><surname>Suzumura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaiah</forename><surname>Onando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mulang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Transformers are graph neural networks</title>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Gradient</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Everything is connected: Graph neural networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Structural Biology</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page">102538</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Luis</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ladislav</forename><surname>Rampášek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.04181</idno>
		<title level="m">Attending to graph transformers</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pure transformers are powerful graph learners</title>
		<author>
			<persName><forename type="first">Jinwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dat</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seonwoo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungjun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moontae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="14582" to="14595" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Graph self-supervised learning: A survey</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="5879" to="5900" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Self-supervised learning on graphs: Contrastive, generative, or predictive</title>
		<author>
			<persName><forename type="first">Lirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Using pre-training can improve model robustness and uncertainty</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2712" to="2721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Parameter-efficient fine-tuning of large-scale pre-trained language models</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Min</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weize</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="220" to="235" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">A survey on model compression for large language models</title>
		<author>
			<persName><forename type="first">Xunyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiping</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.07633</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Qingxiu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.00234</idno>
		<title level="m">A survey for in-context learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Graphprompt: Unifying pretraining and downstream tasks for graph neural networks</title>
		<author>
			<persName><forename type="first">Zemin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingtong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinming</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference</title>
		<meeting>the ACM Web Conference</meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
			<biblScope unit="page" from="417" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Gppt: Graph pre-training and prompt tuning to generalize graph neural networks</title>
		<author>
			<persName><forename type="first">Mingchen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1717" to="1727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">All in one: Multi-task prompting for graph neural networks</title>
		<author>
			<persName><forename type="first">Xiangguo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihong</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining (KDD&apos;23)</title>
		<meeting>the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining (KDD&apos;23)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Prodigy: Enabling in-context learning over graphs</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregor</forename><surname>Kržmanc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Universal prompt tuning for graph neural networks</title>
		<author>
			<persName><forename type="first">Taoran</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Deep graph reprogramming</title>
		<author>
			<persName><forename type="first">Yongcheng</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongbin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiding</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="24345" to="24354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Sgl-pt: A strong graph learner with graph prompt tuning</title>
		<author>
			<persName><forename type="first">Yun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.12449</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Deep prompt tuning for graph transformers</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Shirkavand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.10131</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Prompt-based node feature extractor for few-shot learning on text-attributed graphs</title>
		<author>
			<persName><forename type="first">Xuanwen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiqiao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dezheng</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanjin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhisheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.02848</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Adaptergnn: Efficient delta tuning improves generalization ability in graph neural networks</title>
		<author>
			<persName><forename type="first">Shengrui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueting</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.09595</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">G-adapter: Towards structure-aware parameterefficient transfer learning for graph transformer networks</title>
		<author>
			<persName><forename type="first">Anchun</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinqiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.10329</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Search to fine-tune pre-trained graph neural networks for graph-level tasks</title>
		<author>
			<persName><forename type="first">Zhili</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimin</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofang</forename><surname>Zhou</surname></persName>
		</author>
		<idno>arXiv preprint:2308.06960</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Knowledge distillation on graphs: A survey</title>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.00219</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">A survey on graph neural network acceleration: Algorithms, systems, and customized hardware</title>
		<author>
			<persName><forename type="first">Shichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atefeh</forename><surname>Sohrabizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yewen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Yingyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.14052</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Computing graph neural networks: A survey from algorithms to accelerators</title>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Sergi Abadal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Guirado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>López-Alonso</surname></persName>
		</author>
		<author>
			<persName><surname>Alarcón</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Sgquant: Squeezing the last bit on graph neural networks with specialized quantization</title>
		<author>
			<persName><forename type="first">Boyuan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqiao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 32nd International Conference on Tools with Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1044" to="1052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Degree-quant: Quantization-aware training for graph neural networks</title>
		<author>
			<persName><forename type="first">Anil</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Tailor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Fernandez-Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lane</forename><surname>Donald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Bifeat: Supercharge gnn training via graph feature quantization</title>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.14696</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Tango: rethinking quantization for graph neural network training on gpus</title>
		<author>
			<persName><forename type="first">Shiyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiwen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengying</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuede</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.00890</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Vqgraph: Graph vector-quantization for bridging gnns and mlps</title>
		<author>
			<persName><forename type="first">Ling</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenda</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.02117</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">a 2 q: Aggregation-aware quantization for graph neural networks</title>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanrong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitao</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zejian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Adaptive message quantization and parallelization for distributed full-graph gnn training</title>
		<author>
			<persName><forename type="first">Juntao</forename><surname>Borui Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Can language models solve graph problems in natural language? Thirty-seventh Conference on Neural Information Processing Systems</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangbin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoxuan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochuang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking</title>
		<author>
			<persName><forename type="first">Jiayan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengyu</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.15066</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Evaluating large language models on graphs: Performance insights and comparative analysis</title>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.11224</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Exploring the potential of large language models (llms) in learning on graphs</title>
		<author>
			<persName><forename type="first">Zhikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuaiqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.03393</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<author>
			<persName><forename type="first">Ruosong</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.07134</idno>
		<title level="m">Natural language is all a graph needs</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Automated machine learning on graphs: A survey</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</title>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">A survey on large language models for recommendation</title>
		<author>
			<persName><forename type="first">Likang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongchao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingjia</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengshu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.19860</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Graph neural networks in recommender systems: a survey</title>
		<author>
			<persName><forename type="first">Shiwen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Mgat: Multimodal graph attention network for recommendation</title>
		<author>
			<persName><forename type="first">Zhulin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinwei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianglin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">102277</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Unifying large language models and knowledge graphs: A roadmap</title>
		<author>
			<persName><forename type="first">Linhao</forename><surname>Shirui Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiapu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.08302</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Molgpt: molecular generation using a transformer-decoder model</title>
		<author>
			<persName><forename type="first">Viraj</forename><surname>Bagal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishal</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Vinod</surname></persName>
		</author>
		<author>
			<persName><surname>Priyakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2064" to="2076" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huayi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhirui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.07443</idno>
		<title level="m">Can large language models empower molecular property prediction?</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules</title>
		<author>
			<persName><forename type="first">David</forename><surname>Weininger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and computer sciences</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="36" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">A compact review of molecular property prediction with graph neural networks</title>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Wieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Kohlbacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mélaine</forename><surname>Kuenemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Garon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Ducrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Langer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Drug Discovery Today: Technologies</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanqi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.16484</idno>
		<title level="m">A systematic survey of molecular pre-trained models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Git-mol: A multi-modal large language model for molecular science with graph, image</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixiang</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.06911</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Zinc20-a free ultralarge-scale chemical database for ligand discovery</title>
		<author>
			<persName><surname>John J Irwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Khanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chinzorig</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><surname>Dandarchuluun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Munkhzul</forename><surname>Benjamin R Wong</surname></persName>
		</author>
		<author>
			<persName><surname>Khurelbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yurii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Moroz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">A</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName><surname>Sayle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="6065" to="6073" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">A review on graph neural network methods in financial applications</title>
		<author>
			<persName><forename type="first">Jianian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.15367</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Bloomberggpt: A large language model for finance</title>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vadim</forename><surname>Dabravolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prabhanjan</forename><surname>Kambadur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gideon</forename><surname>Mann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.17564</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Fingpt: Democratizing internet-scale data for financial large language models</title>
		<author>
			<persName><forename type="first">Xiao-Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.10485</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Fingpt: Open-source financial large language models</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><forename type="middle">Dan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.06031</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Evaluating large language models trained on code</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Ponde De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heidy</forename><surname>Khlaaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brooke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alethea</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fotios</forename><surname>Chantzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Hebgen Guss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Paino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolas</forename><surname>Tezak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchir</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">N</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Morikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mira</forename><surname>Murati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Competition-level code generation with alphacode</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Leblond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Eccles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Keeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agustin</forename><forename type="middle">Dal</forename><surname>Lago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyprien</forename><surname>De Masson D'autume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Cherepanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Mankowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esme</forename><surname>Sutherland Robson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Nando De Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="issue">6624</biblScope>
			<biblScope unit="page" from="1092" to="1097" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Learning to represent programs with graphs</title>
		<author>
			<persName><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmoud</forename><surname>Khademi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Graphcode{bert}: Pre-training code representations with data flow</title>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Daya Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Shujie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Svyatkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Tufano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Shao Kun Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Clement</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sundaresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Graph neural networks for intelligent transportation systems: A survey</title>
		<author>
			<persName><forename type="first">Saeed</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asiye</forename><surname>Baghbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nizar</forename><surname>Bouguila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="8846" to="8885" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Spatiotemporal graph neural networks for predictive learning in urban computing: A survey</title>
		<author>
			<persName><forename type="first">Guangyin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jincai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.14483</idno>
		<idno>110Duomo. Transgpt</idno>
		<ptr target="https://github.com/DUOMO/TransGPT" />
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Graph neural network-based fault diagnosis: a review</title>
		<author>
			<persName><forename type="first">Zhiwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiamin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cesare</forename><surname>Alippi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Steven X Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Shardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.08185</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Graph neural networks in iot: a survey</title>
		<author>
			<persName><forename type="first">Guimin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyue</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiechao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sikun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradford</forename><surname>Campbel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><forename type="middle">E</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Boukhechba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Sensor Networks</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="50" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">A review of graph neural networks and their applications in power systems</title>
		<author>
			<persName><forename type="first">Wenlong</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Birgitte</forename><surname>Bak-Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayakrishnan</forename><surname>Radhakrishna Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuelong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusen</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Modern Power Systems and Clean Energy</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="345" to="360" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">A survey on graph neural networks for time series: Forecasting, classification, imputation, and anomaly detection</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingsong</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cesare</forename><surname>Zambon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">I</forename><surname>Alippi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.03759</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Scientific discovery in the age of artificial intelligence</title>
		<author>
			<persName><forename type="first">Hanchen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianfan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanqi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Payal</forename><surname>Chandak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Van Katwyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreea</forename><surname>Deac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">620</biblScope>
			<biblScope unit="page" from="47" to="60" />
			<date type="published" when="2023">7972. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Graph neural networks in particle physics</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Shlomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Roch</forename><surname>Vlimant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning: Science and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">21001</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Graph neural networks at the large hadron collider</title>
		<author>
			<persName><forename type="first">Gage</forename><surname>Dezoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Biscarat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Roch</forename><surname>Vlimant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Physics</title>
		<imprint>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Combinatorial optimization and reasoning with graph neural networks</title>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Cappart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Didier</forename><surname>Chételat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><forename type="middle">B</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Lodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="130" to="131" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Graph neural networks for materials science and chemistry</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Reiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marlen</forename><surname>Neubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André</forename><surname>Eberhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Torresi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houssam</forename><surname>Metni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clint</forename><surname>Van Hoesel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Schopmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Sommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications Materials</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Graph neural networks in network neuroscience</title>
		<author>
			<persName><forename type="first">Alaa</forename><surname>Bessadok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ali Mahjoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Islem</forename><surname>Rekik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="5833" to="5848" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<title level="m">Llama 2: Open foundation and fine-tuned chat models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<author>
			<persName><forename type="first">Jin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.16595</idno>
		<title level="m">Can llms effectively leverage graph structural information: When and why</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<title level="m" type="main">One for all: Towards training one graph model for all classification tasks</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiarui</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lecheng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyue</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.00149</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title level="m" type="main">Graphtext: Graph reasoning in text space</title>
		<author>
			<persName><forename type="first">Jianan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.01089</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">Talk like a graph: Encoding graphs for large language models</title>
		<author>
			<persName><forename type="first">Bahare</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Halcrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.04560</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Beyond text: A deep dive into large language models&apos; ability on understanding graph data</title>
		<author>
			<persName><forename type="first">Yuntong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.04944</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">Graphllm: Boosting graph reasoning ability of large language model</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiqiao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohai</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanwen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.05845</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<author>
			<persName><forename type="first">Zhikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.04668</idno>
		<title level="m">Label-free node classification on graphs with large language models (llms)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<author>
			<persName><forename type="first">Jianxiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghua</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuecang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.09872</idno>
		<title level="m">Empower text-attributed graphs learning with large language models (llms)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Pretrained language models to solve graph tasks in natural language</title>
		<author>
			<persName><forename type="first">Frederik</forename><surname>Wenkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Knyazev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2023 Workshop on Structured Probabilistic Inference &amp; Generative Modeling</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<monogr>
		<title level="m" type="main">Graphgpt: Graph instruction tuning for large language models</title>
		<author>
			<persName><forename type="first">Jiabin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lixin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suqi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.13023</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<title level="m" type="main">Llm4dyg: Can large language models solve problems on dynamic graphs</title>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijian</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.17110</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title level="m" type="main">Disentangled representation learning with large language models for text-attributed graphs</title>
		<author>
			<persName><forename type="first">Yijian</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.18152</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">Gpt-4 doesn&apos;t know it&apos;s wrong: An analysis of iterative prompting for reasoning problems</title>
		<author>
			<persName><forename type="first">Kaya</forename><surname>Stechly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Marquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subbarao</forename><surname>Kambhampati</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.12397</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">Graph neural prompting with large language models</title>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haozhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panpan</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.15427</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<title level="m" type="main">Reasoning on graphs: Faithful and interpretable large language model reasoning</title>
		<author>
			<persName><forename type="first">Linhao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Fang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.01061</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
