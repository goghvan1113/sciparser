<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1 Graph Prompt Learning: A Comprehensive Survey and Beyond</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-11-28">28 Nov 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiangguo</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiawen</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xixi</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yun</forename><surname>Xiong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1 Graph Prompt Learning: A Comprehensive Survey and Beyond</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-11-28">28 Nov 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">9A2883325C3F1BF928DF8387C49A6356</idno>
					<idno type="arXiv">arXiv:2311.16534v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-01-20T06:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>graph prompt</term>
					<term>graph pre-training</term>
					<term>graph learning</term>
					<term>artificial general intelligence</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Artificial General Intelligence (AGI) has revolutionized numerous fields, yet its integration with graph data, a cornerstone in our interconnected world, remains nascent. This paper presents a pioneering survey on the emerging domain of graph prompts in AGI, addressing key challenges and opportunities in harnessing graph data for AGI applications. Despite substantial advancements in AGI across natural language processing and computer vision, the application to graph data is relatively underexplored. This survey critically evaluates the current landscape of AGI in handling graph data, highlighting the distinct challenges in cross-modality, cross-domain, and cross-task applications specific to graphs. Our work is the first to propose a unified framework for understanding graph prompt learning, offering clarity on prompt tokens, token structures, and insertion patterns in the graph domain. We delve into the intrinsic properties of graph prompts, exploring their flexibility, expressiveness, and interplay with existing graph models. A comprehensive taxonomy categorizes over 100 works in this field, aligning them with pre-training tasks across node-level, edge-level, and graph-level objectives. Additionally, we present, ProG, a Python library, and an accompanying website, to support and advance research in graph prompting. The survey culminates in a discussion of current challenges and future directions, offering a roadmap for research in graph prompting within AGI. Through this comprehensive analysis, we aim to catalyze further exploration and practical applications of AGI in graph data, underlining its potential to reshape AGI fields and beyond. ProG and the website can be accessed by <ref type="url" target="https://github.com/WxxShirley/Awesome-Graph-Prompt">https: //github.com/WxxShirley/Awesome-Graph-Prompt</ref>, and <ref type="url" target="https://github.com/sheldonresearch/ProG">https://github.com/sheldonresearch/ProG</ref>, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In an era marked by the rapid evolution of Artificial General Intelligence (AGI), there emerged many fantastic applications with AGI techniques such as ChatGPT in Natural Language Processing (NLP) and Midjourney in Computer Vision (CV). AGI has greatly improved our lives, making our work more efficient and freeing us from repetitive tasks to focus on more creative endeavors. However, when it comes to working with graph data, AGI applications are still in their early stages compared with the huge success in NLP <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b49">50]</ref> and CV areas <ref type="bibr" target="#b90">[91,</ref><ref type="bibr" target="#b113">114]</ref>. In our increasingly interconnected world, understanding and extracting valuable insights from graphs is crucial. This places AGI applied to graph data at the forefront of both academic and industrial interest <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b119">120,</ref><ref type="bibr" target="#b107">108]</ref>, with the potential to redefine fields like drug design <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b63">64]</ref> and battery development <ref type="bibr" target="#b89">[90]</ref>, etc.</p><p>However, realizing this vision is never easy. Figure <ref type="figure" target="#fig_0">1</ref> illustrates this landscape for recent research in Artificial General Intelligence, from which we can see that there are at least three fundamental problems in technique: How to make the model general for different modalities, different domains, and different tasks? Within the NLP and CV areas, there have been many commercial models that can</p><p>â€¢ Xiangguo Sun, Hong Cheng: Department of Systems Engineering and Engineering Management, and Shun Hing Institute of Advanced Engineering, The Chinese University of Hong Kong, Hong Kong SAR. {xgsun, hcheng}@se.cuhk.edu.hk â€¢ Jiawen Zhang, Jia Li: Hong Kong University of Science and Technology (Guangzhou), China. jzhang302@connect.hkust-gz.edu.cn,jialee@ust.hk â€¢ Xixi Wu, Yun Xiong: Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University, China. 21210240043@m.fudan.edu.cn, yunx@fudan.edu.cn understand and translate information across these modalities <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b113">114,</ref><ref type="bibr" target="#b1">2]</ref>. For example, models like BERT <ref type="bibr" target="#b8">[9]</ref> and GPT-3 <ref type="bibr" target="#b1">[2]</ref> have demonstrated the ability to perform tasks that involve both textual and visual information. However, in the context of graph data, the harmonization of information from multiple modalities remains largely uncharted territory <ref type="bibr" target="#b43">[44]</ref>. For the cross-domain issue, transfer learning has proven effective, enabling models to apply knowledge learned from images and text in one domain to another. However, transferring knowledge between different graph domains is very tough because the semantic spaces are not aligned <ref type="bibr" target="#b124">[125]</ref> and the structural patterns are also not similar <ref type="bibr" target="#b121">[122]</ref>, making graph domain adaptation remains a very frontier and not well-solved AGI problem. Currently, most graph research on transfer learning focuses on the third problem, how to leverage the pre-trained graph knowledge in the same graph domain to perform different graph tasks (like node classification, link prediction, graph classification, etc) <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b16">17]</ref>. However, compared with the huge success in NLP and CV, task transferring within the same graph domain is still primitive with far fewer instances of successful industrial applications. While the AGI research boasts notable achievements in many linear data like images, text <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b1">2]</ref>, and videos <ref type="bibr" target="#b90">[91,</ref><ref type="bibr" target="#b113">114]</ref>, the fundamental problems within the realm of graph data remain underexplored. Besides the above three foundation problems, Artificial General Intelligence has also encountered many social disputes. For example, training large foundation models consumes exorbitant amounts of energy and may yield unintended counterfactual outcomes <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b70">71]</ref>. These concerns have led to a growing consensus within the AI community on the efficient extraction of useful knowledge preserved by these large models, minimizing the need for repetitive fine-tuning across various downstream tasks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b39">40]</ref>. This consensus not only promises to alleviate the environmental impact but also offers a practical solution to the challenge of model efficiency and adaptability in an era of AGI.</p><p>At the core of recent AGI technology, prompt learning has presented huge potential to solve the above problems and demonstrated remarkable success in NLP and CV applications <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b49">50]</ref>. Prompt learning is the art of designing informative prompts to manipulate input data for the pre-trained foundation models. Figure <ref type="figure">2</ref> shows an example of a textual-format prompt applied to a pretrained language model to directly perform downstream inference tasks. By reformulating downstream tasks into pretraining tasks, this approach avoids the need for extensive model tuning and efficiently extracts preserved knowledge <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35]</ref>. Since its powerful capabilities in data manipulation, task reformulation, and extraction of significant insights, prompting is very promising to address the aforementioned cross-modalities, cross-domains, and cross-task challenges in one way. Compared with large models, the prompt is usually very lightweight and can efficiently extract useful knowledge by reducing the extensive computational resources caused by repeat tuning of these large models <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b72">73]</ref>. Intuitively, text and images can be perceived as specific instances of a more general graph data structure. For instance, a sentence can be treated as a graph path, with words as nodes, and an image can be viewed as a grid graph, where each pixel serves as a graph node. This insight encourages us to explore the transference of successful prompting techniques from text to the graph area for similar concerns.</p><p>Recently, some researchers have started to introduce prompt learning to graph data <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18]</ref>. However, some further studies have found that the graph prompt is very different from their counterparts in the NLP area <ref type="bibr" target="#b79">[80]</ref>. First, the design of graph prompts proves to be a far more intricate endeavor compared to the formulation of language prompts. Classic language prompts often comprise predefined phrases or learnable vectors appended to input text <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref>. Here, the primary focus lies in the content of the language prompt. However, we actually do not know what a graph prompt looks like. A graph prompt not only contains the prompt "content" but also includes the undefined task of determining how to structure these prompt tokens and seamlessly integrate them into the original graph. Second,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-trained Large Language Model</head><p>Help me answer a multiple choice Question: Greenhouses are great for plants like A. Pizza B. Lollipops C. French beans</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ðŸ¤–</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt</head><p>The correct answer is C. French beans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer</head><p>Fig. <ref type="figure">2</ref>: An example of the language prompt. A textual prompt is designed to let the pre-trained large language model perform the multi-choice question-answering task.</p><p>the harmonization of downstream graph problems with the pre-training task is more difficult than language tasks <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b79">80]</ref>. For example, a typical pre-training approach for a language model is to predict a masked word by the model <ref type="bibr" target="#b8">[9]</ref>. Then many downstream tasks such as question answering, and sentiment classification can be easily reformulated as wordlevel tasks <ref type="bibr" target="#b49">[50]</ref>. Unlike NLP, where pre-training tasks often share a substantial task sub-space, graph tasks span nodelevel <ref type="bibr" target="#b18">[19]</ref>, edge-level <ref type="bibr" target="#b116">[117]</ref>, and graph-level objectives <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b78">79]</ref>, making pre-training pretexts less adaptable. Third, compared with prompts in NLP which are usually some understandable phrases, graph prompts are usually less intuitive to nonspecialists. The fundamental nature and role that graph prompts play within the graph model remain somewhat elusive without comprehensive theoretical analysis. There is also a lack of clear-cut evaluation criteria for the quality of designed graph prompts. In addition, there are still many unclear questions for us to further understand graph prompting. For example, how effective are these graph prompts? What is their efficiency in terms of parameter complexity and training burden? How powerful and flexible do these prompts manipulate the original graph data? In light of these intricacies questions, there is a pressing need for delving deeper into the potential of graph prompts in AGI, thereby paving the way for a more profound understanding of this evolving frontier within the broader data science landscape.</p><p>While there have been recent endeavors to explore graph prompting, a consistent framework or clear route remains unavailable. These efforts vary significantly in terms of perspective, methodology, and target tasks, which present a fragmented landscape of graph prompting and pose a considerable obstacle to the systematical advancement of this research area. There arises an urgent need to provide a panoramic view, analysis, and synthesis of the latest advances in this realm with a unified framework. In light of this situation, we offer this survey to present how existing work on graph prompts tries to solve the three foundation problems towards AGI as previously mentioned. Beyond that, we also wish to push forward the research area by answering the following detailed research questions (RQs):</p><p>â€¢ RQ1: How to understand existing work with a unified framework since they are very different? Our main focus is to understand the various methods used in the field of graph prompting. We want to bring together all these different approaches and ideas to create a single, cohesive framework. This framework will help us thoroughly grasp the existing research and provide a strong foundation for future research in this area.</p><p>â€¢ RQ2: Why Prompt? What's the Nature of Graph Prompt? In this part of our study, we aim to understand why prompts are important. We want to uncover the fundamental aspects of graph prompts. What exactly do graph prompts do in graph problems? How do they fit into the complex graphs, and how do they help us achieve the broader goal of creating AI systems that can handle graph data effectively? These questions highlight the significant role that graph prompts play in shaping the future of AI when dealing with graph information.</p><p>â€¢ RQ3: How to Design Graph Prompts? Designing good graph prompts is a complex task. In this part, we explore the technical details of designing graph prompts: what do they look like, how do they align downstream tasks and the pre-train task, and how they are learned? These important questions focus on the skill of making prompts that work well with the complexities of graph data, helping researchers make better prompts.</p><p>â€¢ RQ4: How to Deploy Graph Prompts in Real-world Applications? At the moment, there isn't an easy-touse toolkit for creating graph prompts. The potential applications that graph prompts can be deployed are under-exploration. This research question focuses on making graph prompts practical for use in real-world situations with an easily extensible programming package.</p><p>â€¢ RQ5: What Are the Current Challenges and Future Directions in Graph Prompting? This question guides us to look at the challenges we're dealing with today and the way forward. By tackling these important questions, we hope to provide a roadmap for ongoing graph-prompting research.</p><p>To answer the first research question (RQ1), we propose a unified framework to analyze graph prompt learning work. Our framework casts the concept of a graph prompt into prompt tokens, token structures, and inserting patterns. This higher-level perspective offers clarity and comprehensiveness, providing readers with a structured understanding of this burgeoning field. To the best of our knowledge, our survey marks the first of its kind to bring together these multifaceted aspects of graph prompting within a single unified framework.</p><p>To answer the second research question (RQ2), we explore the correlations between prompts and existing graph models through the lenses of flexibility and expressiveness and then present a fresh and insightful perspective to uncover the nature of graph prompts. Unlike most prompt learning surveys in NLP areas <ref type="bibr" target="#b49">[50]</ref> that treat prompting as a trick of filling the gap between the pre-training tasks and downstream tasks, we reveal that graph prompts and graph models are interconnected on a deeper level. This novel perspective offers invaluable insights into why prompt learning holds promise in the graph area and what distinguishes it from traditional fine-tuning methods <ref type="bibr" target="#b29">[30]</ref>. To our knowledge, this is the first endeavor to offer such an illuminating perspective on graph prompting.</p><p>To answer the third research question (RQ3), we introduce a comprehensive taxonomy that includes more than 100 related works. Our taxonomy dissects these works, categorizing them according to node-level, edge-level, and graphlevel tasks, thereby aligning them with the broader context of the pre-training task. This will empower our readers with a clearer comprehension of the mechanisms underlying prompts within the whole "pre-training and prompting" workflow.</p><p>To answer the fourth research question (RQ4), we developed ProG (prompt graph) 1 , a unified Python library to support graph prompting. Additionally, we established a website 2 that serves as a repository for the latest graph prompt research. This platform curates a comprehensive collection of research papers, benchmark datasets, and readily accessible code implementations. By providing this accessible ecosystem, we aim to empower researchers and practitioners to advance this burgeoning field more effectively.</p><p>Beyond these, our survey goes a step further with an introduction of potential applications, a thoughtful analysis of the current challenges, and a discussion of future directions, thus providing a comprehensive roadmap for the evolution of this vibrant and evolving field (RQ5). Our contributions are summarised as follows:</p><p>â€¢ Enabling Comprehensive Analysis. We propose a unified framework for analyzing graph prompt learning work, providing a comprehensive view of prompt tokens, token structures, and inserting patterns.</p><p>â€¢ Novel Perspectives on Prompt-Model Interplay. We offer fresh insights into the nature of graph prompts. Unlike traditional work that simply treats prompts as a trick of filling the gap between downstream tasks and the pre-train task, we explore the flexibility and expressiveness issues of graph models and pioneer a more thorough perspective into the interplay between prompts and existing graph models.</p><p>â€¢ A Systematic Taxonomy of Graph Prompting. We systematically explore over a hundred recent works in the domain of graph prompting. This taxonomy not only organizes these contributions but also furnishes readers with a comprehensive understanding of prompt mechanisms within the overarching "pretraining and prompting" workflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢</head><p>Empowering the Graph Prompting Ecosystem. We developed ProG, a Python library supporting graph prompting, and a comprehensive website for collecting the latest graph prompt research.</p><p>â€¢ Charting a Path Forward. A detailed exploration of current challenges and future directions in the field.</p><p>Roadmap. The rest of this survey is organized as follows: we present our survey methodology in section 2, followed by preliminaries in section 3, the introduction of pre-training 1. <ref type="url" target="https://github.com/sheldonresearch/ProG">https://github.com/sheldonresearch/ProG</ref> 2. <ref type="url" target="https://github.com/WxxShirley/Awesome-Graph-Prompt">https://github.com/WxxShirley/Awesome-Graph-Prompt</ref> methods in section 4, prompting methods for graph models in section 5. We discuss potential applications of graph prompt in section 7 and present our developed library ProG in section 8. In section 9, we summarize our survey with current challenges and future directions. Section 10 concludes the survey and presents the contribution declaration of the authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SURVEY METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Research Objectives</head><p>This survey will introduce the art of prompting from a big picture of artificial general intelligence (AGI). We first present three fundamental problems towards AGI in Table <ref type="table" target="#tab_0">1</ref>. Recently, prompt learning has been demonstrated as a promising solution to these problems in many linear data such as text <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref>, images <ref type="bibr" target="#b90">[91]</ref>, etc. However, whether the prompt technique can still solve these problems in the graph area, is not clearly discussed. Through this survey, we wish to figure out how the graph prompt potentially helps graph models to be more general across various tasks and domains, and how it generalizes the foundation models to interact with other modalities (e.g. text, image, etc). Beyond the above common problems of AGI in NLP, CV, and graph areas, graph prompting is usually very different from its counterparts in NLP and CV areas, leading to many detailed questions as shown in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Taxonomy</head><p>The taxonomy of this survey is presented in Figure <ref type="figure" target="#fig_5">3</ref>, which is intricately designed to categorize graph prompts based on their specific applications and functionalities, providing a structured approach to understanding their role in AGI.</p><p>(1) Pre-training Strategies for Graph Prompt.</p><p>Since prompting techniques mostly seek to reformulate downstream tasks to the pre-training tasks, they are highly customized for detailed pre-training approaches, thus we briefly discuss the representative pre-training work in the graph area before we formally introduce graph prompt content. We split existing pre-training approaches into nodelevel, edge-level, graph-level, and multi-task pre-training strategies. Next, we present how different prompting ideas reformulate various downstream tasks to the corresponding pre-training tasks in the graph area. By revisiting existing pre-training literature, we will be more clear about the role of graph prompts in the whole "pre-training and prompting" framework.</p><p>(2) Prompting Methods in Graph Areas.</p><p>Aiming at the three foundation problems mentioned in the research objectives (P1-P3 in Table <ref type="table" target="#tab_0">1</ref>), we analyze graph prompting from three aspects: i. prompt design for graph tasks (Section 5); ii. multi-modal prompting with graphs (Section 6); and iii. graph domain adaptation with prompting techniques (Section 6.2). Within each aspect, we present a detailed discussion related to the five specific problems in the graph prompt area (Q1-Q5 in Table <ref type="table" target="#tab_0">1</ref>).</p><p>i. Prompt Design for Graph Tasks. In this section, we propose a unified framework to analyze existing works on graph prompt design. Our framework treats existing graph prompts with three key components: prompt tokens, which preserve prompt content as vectors; token structures, which indicate how multiple tokens are organized; and inserting patterns, which define how to combine graph prompt with the original graphs. Beyond that, we also carefully analyze how these works design the prompt answering function, which means how they get results for the downstream tasks from their prompts. We also summarize three representative methods to learn appropriate prompts, including metalearning techniques, task-specific tuning, and tuning in line with pretext. In the end, we further discuss these works in Section 5.4 to see their intrinsic connections with pros and cons.</p><p>ii. Multi-modal Prompting with Graphs. In this section, we briefly present how graph prompts work in a text-attributed graph, which can be seen as the fusion of text and graph modalities. With the progress of large language models (LLMs), the fusion of text and graph data has become easier and has aroused a lot of work on this topic. Since the topic of integrating LLMs with graphs has been well summarized in <ref type="bibr" target="#b45">[46]</ref>, we won't present too much in this survey. Instead, we only briefly discuss some representative works in this area that focus on the prompt area.</p><p>iii. Graph Domain Adaptation through Prompting Techniques. In this section, we introduce related work from two branches. The first branch presents works solving semantic alignment across different graph domains, and the second branch presents structural alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Literature Overview</head><p>In this survey, we carefully studied more than 100 highquality papers published within the past 5 years from reputable conferences and journals including but not limited to NeurIPS, SIGKDD, The Web Conference, ICLR, CIKM, ICML, IJCAI, EMNLP, SIGIR, ACL, AAAI, WSDM, TKDE, etc. Most of these venues are ranked as CCF A 3 or CORA A* 4 . Besides these works, we also introduce several latest works in arXiv so that our survey can catch up with the frontier and latest progress in this area. A more detailed pie chart (Figure <ref type="figure" target="#fig_6">4a</ref>) presents the distribution of collected papers Fig. <ref type="figure" target="#fig_5">3</ref>: Taxonomy of this Survey with Representative Works. over these venues. Furthermore, we conducted an analysis of the topics covered by these references. In Figure <ref type="figure" target="#fig_6">4b</ref>, we present the top 15 keywords that appeared in the titles of these papers. Notably, these keywords align closely with the focus of our survey, which is centered around graph domains and prompt learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Connection to Existing Work:</head><p>Our survey stands out from existing surveys in several notable ways. For example, Liu et al. <ref type="bibr" target="#b47">[48]</ref> primarily focuses on graph foundation models (GFMs). Their survey does not specifically target graph prompts, and only a few papers in this area are briefly discussed. Li et al. <ref type="bibr" target="#b45">[46]</ref> systematically analyze recent works that integrate graphs and LLMs, which is a detailed analysis of a small portion (Section 6.1) in our survey. We go beyond their scope by exploring various aspects of graph prompts in a more extensive manner. Meanwhile, surveys <ref type="bibr" target="#b104">[105,</ref><ref type="bibr" target="#b112">113]</ref> focus primarily on the pre-training stages, without involving the crucial aspect of graph prompt learning. While a prior survey on graph prompt learning by Wu et al. <ref type="bibr" target="#b101">[102]</ref> exists, our survey surpasses it in several key aspects. Firstly, we provide a more comprehensive analysis of related works. Their survey was published in May 2023 when there were only a few graph prompt works available <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b10">11]</ref>. In contrast, our survey encompasses a broader scope, including all relevant works in the field. Secondly, we offer a systematic analysis of existing works within a uniform framework, facilitating comprehension and comparison between different approaches. Thirdly, our survey provides deep insights into the relationship between graph pre-training and prompts, shedding light on the interplay between these critical elements. Lastly, we not only present empirical insights but also include engineering works aimed at deploying graph prompts in real-world applications, ensuring the practical applicability of our survey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Others, 5%</head><p>ICCV, 1%  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>Graph representation learning has been a topic of extensive research over the past few decades. This journey, illustrated in Figure <ref type="figure" target="#fig_2">5</ref>, has witnessed the evolution from shallow embedding methods to supervised graph neural networks, transitioning from the fine-tuning paradigm to the emerging prompting paradigm. In this section, we will provide an overview of the fundamental notations employed in this survey, delve into the historical developments of graph representation learning, explore the pre-training and finetuning paradigm, and trace the evolution of prompt-based learning. Most importantly, we will present a novel perspective focusing on flexibility and expressiveness, shedding light on why prompts offer a promising solution to address the limitations of existing graph representation learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations</head><p>Let a graph instance denoted as G = {V, E}, where V = {v 1 , v 2 , . . . , v N } represents the node set containing N nodes. The edge set E âˆˆ V Ã— V describes the connection between nodes. Each node v i is associated with a feature vector represented as x i âˆˆ R D . To characterize the connectivity within the graph, we employ the adjacency matrix denoted as A âˆˆ {0, 1} N Ã—N , where the entry</p><formula xml:id="formula_1">A ij = 1 if and only if the edge (v i , v j ) âˆˆ E.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph Representation Learning</head><p>The last decades have witnessed a notable surge in the development of graph representation learning techniques. These approaches can be broadly categorized into two main branches: shallow embedding methods and deep graph neural networks (GNNs). The shallow embedding approach is centered on mapping nodes into lower-dimensional, learnable embeddings, enhancing their applicability in various downstream tasks, as exemplified by node2vec <ref type="bibr" target="#b18">[19]</ref> and DeepWalk <ref type="bibr" target="#b62">[63]</ref>. On the other hand, the deep GNNs maintain the input node features as constants and optimize deep graph model parameters for specific tasks, leading to more expressive representation capabilities, as seen in methods such as Graph Convolution Networks (GCN) <ref type="bibr" target="#b55">[56]</ref> and GraphSAGE <ref type="bibr" target="#b20">[21]</ref>. Shallow embedding approaches make input node features learnable parameters, aiming at encoding nodes in a manner that retains the original network's similarity structure. According to node similarity definition, these methods can be categorized as factorization-based <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b115">116]</ref> and random walk approaches <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b92">93]</ref>. Despite the flexibility that shallow embedding methods offer for various downstream tasks, they are constrained by their inability to generate embeddings for nodes not encountered during training. Additionally, these approaches lack the capability to incorporate node features. Therefore, more "deeper" methods, specifically those based on graph neural networks, have been developed to address these limitations.</p><p>Most deep GNNs follow a message-passing schema and use a more complex encoder, resulting in powerful expressiveness in graph representation. The representative neural network structure is convolutional graph neural networks (ConvGNNs), which comprise spectral <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26]</ref> and spatial methods <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b106">107]</ref>. While these methods have exhibited remarkable capabilities in various graphbased applications, their reliance on task-specific supervision imposes constraints on their adaptability and generalizability, particularly when dealing with tasks that have limited labeled data.</p><p>In summary, shallow embedding methods offer flexibility, preserving network structure and node content for straightforward graph analytic tasks. However, they lack expressiveness and the ability to encapsulate additional node features. Conversely, GNNs provide more expressive graph representations but require task-specific training data, limiting their transferability. Hence, it calls for a graph learning mechanism that combines expressiveness and flexibility. This need led to the development of the pre-training and finetuning paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pre-training and Fine-tuning</head><p>To address the challenges of limited labeled data and generalization issues in GNNs, the pre-training and finetuning paradigm, thriving in the natural language processing (NLP) community, has gained widespread adoption in graph representation learning. These approaches involve pre-training models on large-scale graph data, with or without labels, followed by fine-tuning model parameters for diverse downstream tasks. This two-step process improves model initialization, yielding broader optima and enhanced generalization compared to training from scratch. Commonly employed pre-training schemes include Graph AutoEncoders (GAEs) <ref type="bibr" target="#b88">[89]</ref>, Masked Components Modeling (MCM) <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b67">68]</ref>, Graph Contrastive Learning (GCL) <ref type="bibr" target="#b87">[88,</ref><ref type="bibr" target="#b75">76]</ref>, etc. In Section 4, we will delve into a detailed discussion of the pre-training and fine-tuning method, offering a comprehensive picture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">A Brief History of Prompt Learning</head><p>Due to the growing number of model parameters, the conventional pre-training and fine-tuning process is evolving into a new approach termed pre-training, prompting, and predicting <ref type="bibr" target="#b49">[50]</ref>. In this paradigm, instead of manually adapting the pre-trained model for specific downstream tasks, these tasks are reformulated to resemble those addressed during the pre-training phase, aided by prompts. Prompts in NLP take various shapes, including cloze prompts , which complete textual strings like those used in masked language models, and prefix prompts <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43]</ref>, where the input text precedes the answer slot, as employed by autoregressive language models. Some studies involve manually designed templates based on human insights <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b70">71]</ref>, while others explore automated template learning. This includes searching for templates in a discrete space <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b15">16]</ref> or conducting prompting directly in the embedding space <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b64">65]</ref>. Such a paradigm enables a single pre-trained model to address a multitude of downstream tasks in an unsupervised manner, which has been widely demonstrated by large language models. In light of this, the application of prompting techniques in the context of graph-based tasks is currently an area of active exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Why Prompt? A New Perspective upon Flexibility and Expressiveness.</head><p>Why is prompt learning promising for the graph domain? An existing perspective that appears in most related work is that prompt can reformulate downstream tasks to the pre-training task, which might fill the gap between them. This perspective is good but still not profound enough to see the intrinsic difference from traditional fine-tuning. For example, in a similar perspective, pre-training and finetuning can be treated as using fine-tuning to reformulate the pre-training task to the downstream task. It seems that these two technique routines can both address the same problem. Why the first choice is better than the second one?</p><p>In this section, we propose a new perspective, from this view, we can further see the difference between prompting and fine-tuning. As discussed in previous sections, existing graph representation learning methods fail to achieve a satisfactory trade-off between expressiveness and flexibility. Shallow graph embedding approaches offer flexibility as they can be applied to a wide range of downstream tasks. However, they sacrifice expressiveness due to limited parameterization and the inability to incorporate original node features. Take the DeepWalk model as an example, shallow graph methods usually treat node representations as free parameters, which is very flexible because each node can learn its individual representations independently. However, for the reason of gradient, they can not rely on more complicated networks later, which might lose some expressiveness. Actually, there are many more advanced works with deep graph layers using the node representations from DeepWalk as their input features as these node representations are very general in various tasks. On the other hand, GNN-based methods treat node embedding as constant features and seek to find a powerful network for mapping node features to a specific task, which is very expressive. However, the learned feature transform pattern is applied to all the nodes, which means the model can not treat each node embedding as free parameters, and can not achieve as flexible results as the previous ones. When we have multiple tasks, we usually need to train different versions of the same GNN model, which is not as flexible as the previous one.</p><p>With the above analysis, we can find that traditional finetuning actually seeks to further improve the expressiveness of a new task with the pre-trained graph model and can not take care of node flexibility. Unlike fine-tuning, a graph prompt usually has several tokens with free parameters, which is very similar to shallow graph methods. In the  meantime, each node in the original graph has constant features for GNN models. By inserting the prompt graph to the original graph, the combined graph has both nodes with constant features and tokens with free parameters. The token parameters can be efficiently tuned, preserving node flexibility. The combined graph is sent to a frozen pre-trained GNN model to leverage the powerful expressiveness of deep graph models.</p><p>In this paper, we argue that the prompting mechanism offers a promising solution to address the limitations of existing graph representation learning methods, effectively balancing flexibility and expressiveness. Pre-trained GNNs inherently possess knowledge of both structural and semantic aspects, enabling the desired level of expressiveness. By introducing prompts, we can seamlessly apply powerful pretrained models to diverse downstream tasks across various domains in an efficient manner. This is achieved by aligning the format of downstream tasks with that of pre-trained tasks, thus leveraging the full potential of pre-trained models even with minimal supervision signals. While the fine-tuning mechanism can also facilitate domain or task adaptation of pre-trained graph models, it often necessitates a considerable amount of labeled information and requires exhaustive retraining of the pre-trained model. In comparison, the prompt mechanism offers a higher degree of flexibility and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PRE-TRAINING GNNS FOR GRAPH PROMPTING</head><p>Graph pre-training is a pivotal step of the pre-training, prompting, and predicting paradigm in graph representation learning. This approach leverages readily available information to encode the inherent graph structure, providing a robust foundation for generalization across diverse downstream tasks. By integrating these pre-training methods into the comprehensive workflow, we offer an exploration of their interplay with the subsequent prompting and predicting phases, shedding light on the strengths and limitations of this holistic approach. This unique perspective distinguishes our survey, framing graph pre-training as an integral part of the broader graph-prompting learning process. To better illustrate the motivation behind the prompting paradigm, we will now delve into four distinct pre-training strategies within the traditional pre-training and fine-tuning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Node-level Strategies</head><p>Node-level pre-training strategies empower the acquisition of valuable local structure representations that can be transferred to downstream tasks. As shown in Figure <ref type="figure" target="#fig_3">6</ref>, these strategies encompass both contrastive and predictive learning methods. In contrastive learning, self-supervised signals typically result from perturbations in the original graph structure or attributes, with the goal of maximizing Mutual Information (MI) between the original and selfsupervised views. Noteworthy node-level contrastive methods include those presented in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b122">123,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b94">95]</ref>. On the other hand, predictive models focus on reconstructing perturbed data using information from the unperturbed data, as demonstrated in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr" target="#b27">28]</ref>. However, its emphasis on partially semantic topology patterns restricts its ability to capture higher-order information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Edge-level Strategies</head><p>To enhance performance in tasks such as link prediction, diverse edge-level pre-training strategies have been developed. These strategies excel at capturing node interactions and have undergone extensive exploration. One approach involves discriminating the presence of edges between pairs of nodes, which can be regarded as contrastive methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b53">54]</ref>. Another approach focuses on reconstructing masked edges by recovering the adjacency matrix <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b38">39]</ref>. Although this pre-training strategy performs admirably in tasks closely related to predicting node relations, it concentrates solely on structural aspects, neglecting the portrayal of node properties, and may encounter challenges when applied to graph-level downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Graph-level Strategies</head><p>The necessity to improve graph-level representations for subgraph-related downstream tasks has prompted the exploration of various graph-level pre-training strategies. Similar to node-and edge-level strategies, these approaches can be broadly categorized into two main groups: graph reconstruction methods, involving the masking of graph components and their subsequent recovery <ref type="bibr" target="#b103">[104,</ref><ref type="bibr" target="#b67">68]</ref>, and contrastive methods focused on maximizing mutual information. These contrastive methods target either local patches of node features and global graph features <ref type="bibr" target="#b87">[88,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b74">75]</ref>, or positive and negative pairs of graphs <ref type="bibr" target="#b110">[111,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b65">66]</ref>. While these approaches effectively encode global information and generate valuable graph-level representations, a significant challenge lies in transferring knowledge from a specific pretext task to downstream tasks with substantial gaps, potentially resulting in negative transfer <ref type="bibr" target="#b68">[69]</ref>. This can limit the applicability and reliability of pre-trained models and potentially yield less favorable outcomes, even worse than learning from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Multi-task Pre-training</head><p>Multi-task pre-training accommodates multiple optimization objectives, addressing a broad spectrum of graph-related aspects to enhance generalization while mitigating negative transfer issues. These objectives may encompass various combinations, such as concurrent training of node attribution reconstruction and structural recovery <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b114">115,</ref><ref type="bibr" target="#b12">13]</ref>. For example, Hu et al. <ref type="bibr" target="#b29">[30]</ref> pre-trained a GNN at both the node and graph levels, enabling the GNN to learn valuable local and global representations simultaneously. Furthermore, some works have employed contrastive learning at different levels <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b105">106]</ref>, or joint optimization of both contrastive loss and graph reconstruction error <ref type="bibr" target="#b41">[42]</ref>. However, it is crucial to recognize that multi-task pre-training approaches may face optimization challenges, potentially resulting in suboptimal performance across tasks. As a result, optimizing model performance for each task while mitigating the negative transfer problem remains a significant but unresolved concern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Further Discussion</head><p>Fortunately, the prompting and predicting paradigm offers a robust solution to the challenges mentioned above. This approach can fully exploit model performance and seamlessly integrate with advanced GNN architectures. Instead of adapting pre-trained GNNs to downstream tasks through objective engineering, this paradigm reformulates downstream tasks into those solved during the pre-training phase using a graph prompt. This innovative strategy effectively bridges the gap between pretext and downstream tasks while sidestepping suboptimal performance pitfalls. Furthermore, in comparison to traditional fine-tuning approaches, the prompting paradigm showcases remarkable flexibility, enabling it to excel in scenarios demanding few-shot or even zero-shot learning, where adapting to new contexts with limited or no labeled data is paramount. In the current landscape marked by surging model volumes and an ever-expanding array of downstream tasks, the ascent of the prompting paradigm represents an irresistible and transformative trend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">PROMPT DESIGN FOR GRAPH TASKS</head><p>In this section, we propose a unified view for the graph prompt. As shown in Figure <ref type="figure">7</ref>, the graph prompt should contain at least three components: prompt tokens with prompt vector; token structures preserving inner correlations of these tokens; and inserting patterns indicating how to integrate the original graph with prompts. Beyond these details, we are particularly interested in the following questions: Question 1: How do these works design the graph prompt? Question 2: How do these works reformulate downstream tasks to the pre-training tasks? Question 3: How do these works learn an effective prompt? and Question 4: What are the inner connections of these works, their advantages and limitations? With these questions, we summarize the most representative works published recently and present them in Table <ref type="table" target="#tab_4">2</ref>. graph features <ref type="bibr" target="#b124">[125]</ref>. For example, given a graph feature matrix</p><formula xml:id="formula_2">X = {x 1 , â€¢ â€¢ â€¢ , x N } âˆˆ R N Ã—d where x i âˆˆ R 1Ã—d</formula><p>is the feature of i-th node and d is the dimension of the feature space. Fang et al. <ref type="bibr" target="#b10">[11]</ref> and Shirkavand and Huang <ref type="bibr" target="#b73">[74]</ref> treat the basic prompt as a learnable vector p âˆˆ R 1Ã—d , which can be added to all node features and make the manipulated feature matrix be</p><formula xml:id="formula_3">X * = {x 1 + p, â€¢ â€¢ â€¢ , x N + p}.</formula><p>In this way, we can use the reformulated features to replace the original features and process the graph with pre-trained graph models. A later work <ref type="bibr" target="#b11">[12]</ref> further extends one prompt token to multiple tokens and makes the performance better. PGCL <ref type="bibr" target="#b17">[18]</ref> design a prompt vector for semantic view and another prompt vector for contextual view, then they add these prompt vectors to the graph-level representations by element-wise multiplication. A similar prompt design is also adopted in VNT <ref type="bibr" target="#b82">[83]</ref>. The difference is that their inserting pattern does not add the prompt token to the original graph feature but concatenates the prompt token with the original node set and tries to integrate them by the selfattention function in a graph transformer. In GraphPrompt <ref type="bibr" target="#b51">[52]</ref>, the prompt token is similar to the format defined by Fang et al. <ref type="bibr" target="#b10">[11]</ref>. The difference thing is that previous work designed the prompt tokens in the initial feature space, while this method assumes the prompt in the hidden layer of the graph model. Usually, the hidden size will be smaller than the original feature, making their prompt token shorter than the previous one. Another different thing is that the graph prompt here is used to assist graph pooling operation (a.k.a Readout). For example, given the node set</p><formula xml:id="formula_4">V = {v 1 , â€¢ â€¢ â€¢ , v |V| }, the embedding of node v is h v âˆˆ R 1Ã—d</formula><p>, a prompt token p t âˆˆ R 1Ã—d specified to task t is inserted to the graph nodes by the element-wise multiplication (âŠ—):</p><formula xml:id="formula_5">s t = Readout({p t âŠ— h v : v âˆˆ V}).</formula><p>Similarly, SGL-PT <ref type="bibr" target="#b123">[124]</ref> creates a prompt token to connect to all nodes in the graph.</p><p>The prompt token preserves a global perception of the graph and can assist their global branch of the pre-training tasks, which can be also treated as a pooling strategy. Aiming at aligning node classification and link prediction, GPPT <ref type="bibr" target="#b77">[78]</ref> defines graph prompts as additional tokens that contain task tokens and structure tokens. Here the task token refers to the description of the downstream node label to be classified and the structure token is the representation of the subgraph surrounding the target node. By this means, predicting node </p><formula xml:id="formula_6">âˆˆ R d sv i â† f Î¸ (vi) sy,vi â† [cy, sv i ] Cross Entropy âœ“ âœ— âœ— âœ“ âœ— GPF (arXiv [11]) âœ“ âœ“ âœ“ prompt feature p âˆˆ R d si â† xi + p max p,Ï• (yi,si) p Ï€,Ï• (yi|si) âœ— âœ— âœ“ âœ— âœ“</formula><p>All in One (KDD 2023 <ref type="bibr" target="#b79">[80]</ref>) âœ— âœ— âœ“ prompt token: P = {p 1 , ..., p |P| } token structure:</p><formula xml:id="formula_7">{(p i , p j )|p i , p j âˆˆ P} w ik â† Ïƒ(p k â€¢ x T i ) if Ïƒ(p k â€¢ x T i ) &gt; Î´ else 0 siâ† xi+ |P| k=1 w ik p k Meta-Learning âœ“ âœ“ âœ“ âœ“ âœ“ GraphPrompt (WWW 2023[52]) âœ— âœ“ âœ— prompt token: p t âˆˆ R d , t âˆˆ T structure token: s âˆˆ R d task token: cy âˆˆ R d st i â† Readout({p t âŠ™ fÏ€(v)| v âˆˆ V (Si)}) cy â† Mean({s t j |yj = y}) minp t -(yi,Si) ln exp(sim(s t i ,cy i )/Ï„ ) yâˆˆY exp(sim(s t i ,cy )/Ï„ ) âœ“ âœ— âœ“ âœ“ âœ— PGCL (arXiv [18]) âœ— âœ— âœ“ semantic token: p s âˆˆ R d contextual token: p c âˆˆ R d z ps x = z s x âŠ™ p s z pc x = z c x âŠ™ p c min -(v,a,b)âˆˆT log exp(sim(z p v ,z p a )/Ï„) uâˆˆ{x,y} exp(sim(z p v ,z p u )/Ï„) âœ“ âœ“ âœ“ âœ“ âœ— PRODIGY (NeurIPS 2023 [32]) âœ— âœ“ âœ— data graph: G D âˆ¼ âŠ• k i=1 N (Vi, G) âŸ© task graph: G T si â† f T Ï€ T (G T |f D Ï€ D (G D )) Fixed âœ“ âœ“ âœ“ âœ“ âœ—</formula><p>SGL-PT (arXiv <ref type="bibr" target="#b123">[124]</ref>) âœ“ âœ— âœ“ prompt token: one vector for each graph connect to all nodes in the graph contrastive loss and reconstruction loss</p><formula xml:id="formula_8">âœ“ âœ— âœ— âœ“ âœ— GPF-Plus (NeurIPS 2023 [12]) âœ“ âœ“ âœ“ prompt features p 1 , â€¢ â€¢ â€¢ , p k âˆˆ R d si â† xi + Ïƒ(p 1 , â€¢ â€¢ â€¢ , p k ) max p,Ï• (yi,si) p Ï€,Ï• (yi|si) âœ— âœ— âœ“ âœ— âœ“ DeepGPT (arXiv [74]) âœ“ âœ“ âœ“ prompt token: p âˆˆ R d prefix token: P âˆˆ R |P|Ã—d xi â† xi + p si â† fP,Ï€(G, xi) min p,P,Ï• (yi,vi) L(p Ï• (si), yi) âœ— âœ— âœ“ âœ— âœ“ ULTRA-DP (arXiv [4]) âœ— âœ“ âœ—</formula><p>prompt token:</p><formula xml:id="formula_9">p i = p task + w pos p pos i , p pos i denotes vi's positional embedding create a virtual node v p i for target node vi, G â€² â† (V âˆª {v p i }, E âˆª {(v p i , vi)}, X âˆª {p i }) Multi-task-Learning âœ“ âœ— âœ— âœ— âœ“ HetGPT (arXiv [55]) âœ— âœ“ âœ— prompt token: F = {f A i } K i=1 for A âˆˆ A task token: cy âˆˆ R d sA i â† x A i + K k=1 w ik f A k , zi = fÏ€(G, sA i ) minC,F -(yi,vi) log exp(sim(zi,cy i )/Ï„ ) yâˆˆY exp(sim(zi,cy )/Ï„ ) âœ“ âœ— âœ— âœ“ âœ— SAP (arXiv [17]) âœ“ âœ— âœ—</formula><p>task token: P = {cy}yâˆˆY structure token:</p><formula xml:id="formula_10">W = {(vi, cj )}v iâˆˆV ,cj âˆˆP G â€² â† (V âˆª P, E âˆª W)</formula><p>Z (1) = MLP Ï€ â€² (X) Z (2) = GNN Ï€ â€²â€² ([X, P],</p><p>[A, W])</p><p>minW -(yi,vi) log exp(sim(z (1)   i ,z (2)   y i</p><p>)/Ï„ ) yâˆˆY exp(sim(z</p><formula xml:id="formula_11">(1) i ,z (2) y )/Ï„ ) âœ“ âœ— âœ“ âœ“ âœ— VNT (KDD 2023 [83]) âœ“ âœ“ âœ— P = [p1; . . . ; pP ], pp âˆˆ R F E 1 âˆ¥Z 1 = L 1 E 0 âˆ¥P âˆˆ R (V +P )Ã—F Cross Entropy âœ“ âœ— âœ— âœ— âœ“</formula><p>v's label can be reformulated to predict a potential link between node v's structure token and the label task token. Aiming at the node classification task, ULTRA-DP <ref type="bibr" target="#b3">[4]</ref> creates a prompt token for each target node, where the token feature is the weighted sum of position embedding of the target node and a task embedding of the pre-training task. HetGPT <ref type="bibr" target="#b54">[55]</ref> design a prompt with node tokens and class tokens, which are organized in a similar way to GPPT. The difference is that they also add a type-specific feature token to make graph prompts sensitive to different node types, by which they can extend existing graph prompts to heterogeneous graphs. B. Prompt as Graphs. The graph prompt in All in One <ref type="bibr" target="#b79">[80]</ref> is an additional subgraph that can be learned by efficient tuning. The prompt tokens are some additional nodes that have the same size of node representation as the original nodes. They assume the prompt tokens should be in the same semantic space as the original node features so that we can easily manipulate node features with these tokens. The token structures include two parts. The first part is the inner links among different tokens, and the second part is the cross-links between the prompt graph and the original graph. These links can be pre-calculated by the dot product between one token to another token (inner links) or one token to another original node (cross links). The inserting pattern is to add the prompt graph to the original graph by the cross-links and then treat the combined graph as a new graph, and send it to the pre-trained graph model to get the graph-level representation. The prompt graph in PRODIGY <ref type="bibr" target="#b31">[32]</ref> includes data graphs and a task graph. The data graphs can be treated as subgraphs surrounding the target nodes (for node classification task), node pairs (for edge classification task), or just denoted as the graph classification instance. Here the prompt tokens and prompt structures are just the same as in the original graph. The task graph contains data tokens and label tokens where each data token connects to one data graph and is further connected by label tokens. Unlike previous works that aim at reformulating downstream tasks to the pre-training task by prompting the downstream data, PRODIGY leverages the prompt graph to unify all the upstream and downstream tasks. Their pre-training strategy is a set of tasks including neighboring matching and label matching, which can be reformulated as predicting the similarity between the data token and the label token in the prompt graph. SAP <ref type="bibr" target="#b16">[17]</ref> also connects several prompt tokens (each token corresponds to one class) to the original graph by cross-links defined in All in One. The difference is that their prompting task is a node-level contrastive task, in which they use MLP to encode the node features as the first view and they use a GNN to encode the prompted graph as the second view, which is consistent with their pre-training task.  <ref type="bibr" target="#b79">[80]</ref> pretrains the graph model via graph-level contrastive learning. The pre-training task aims to learn a robust graph encoder over different graph views generated by the perturbations of the original graph. To reformulate various graph tasks to this graph-level pretext, they first unify node-level, edge-level, and graph-level tasks to the graph-level task by induced subgraphs, which are also introduced in PRODIGY <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b17">18]</ref>. Then they claim that the graph prompt added to the original graph is in nature the simulation of any graph operations such as node feature masking, node or edge perturbations, subgraph removing, etc. With this opinion, we just need to add an appropriate graph prompt to downstream graph datasets then it will be promising to further reformulate the downstream task to the pretext.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Aligning Tasks by</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learnable Answering Function.</head><p>To output the results of downstream tasks, Sun et al. <ref type="bibr" target="#b79">[80]</ref> design two types of answering functions. The first one is a learnable task head (such as an MLP mapping function) that can be easily tuned with very limited data. It takes the graph-level representation generated by the pre-trained graph model and then outputs the downstream result. For example, if the downstream is a three-class node classification, we can simply use a dense layer with three hidden units to connect the graph representation, which is generated by the pre-trained model on the combined graph with node included graph and a graph prompt. In this case, both the graph prompt and the task head are tunable, so we can adjust them alternately. Similar learnable answering functions are also adopted in other works like <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b82">83]</ref>. The good point is that they are very easy to align two tasks, however, it also increases the tuning workflow.</p><p>C. Hand-crafted Answering Function. To further reduce the tuning burden, All in One <ref type="bibr" target="#b79">[80]</ref> also proposes a second answering function, which is hand-crafted without any trainable task head. For example, for a node classification task, we can set up K unique sub-prompts, each aligning with a different node type, where K represents the total number of node categories. If the pre-training involves a task like GraphCL <ref type="bibr" target="#b110">[111]</ref>, which aims to maximize similarity at the graph level between pairs of graph views, then the target node can be classified with label â„“, (â„“ = 1, 2, â€¢ â€¢ â€¢ , K) if the â„“-th graph most closely resembles the original nodeinclusive graph. Similarly, GPPT <ref type="bibr" target="#b77">[78]</ref> and HetGPT <ref type="bibr" target="#b54">[55]</ref> use link prediction as their pre-training task and reformulate downstream node classification by unifying it as the same task template. For example, by treating the node label as an additional token, we can use the pre-trained model to directly output the possibility of an edge between the label token and the target node. The pre-training strategy in SAP <ref type="bibr" target="#b16">[17]</ref> is to compare node representations from two graph views, the first of which is generated by node feature encoding and the second of which is encoded by a graph model. To this end, their prompt tokens denote class information and they compare node representation with each class token to find the class with the largest similarity as the inference results. By designing a unified task template, PRODIGY <ref type="bibr" target="#b31">[32]</ref> uses a hand-crafted graph prompt to describe all node, edge, and graph classification tasks by predicting the link between data tokens and label tokens. Liu et al. <ref type="bibr" target="#b51">[52]</ref> extend the link prediction task as graph pair similarity and treat it as their pre-training task, to align the downstream node classification and graph classification task, they design a unified answering template making the downstream side aligned with the pretraining side. Specifically, given a triplet of induced graphs (g 1 , g 2 , g 3 ) where g 1 and g 2 have the same label, g 1 and g 3 have different labels. In particular, when the target task is node classification, the induced graph refers to the contextual subgraph of the target node. The unified answering template is defined as sim(g 1 , g 2 ) &gt; sim(g 1 , g 3 ). A. Meta-Learning Technique. To learn appropriate prompts, Sun et al. <ref type="bibr" target="#b79">[80]</ref> leverage meta-learning techniques (such as MAML <ref type="bibr" target="#b14">[15]</ref> model) to obtain a robust starting point for the prompt parameters. Since the support set and query set include various graph tasks (such as node classification, link prediction, graph classification, etc), the learned graph prompt is expected to be more general on various downstream tasks.</p><p>B. Task-specific Tuning. Besides All in One <ref type="bibr" target="#b79">[80]</ref>, which aims to learn a general prompt on various downstream tasks, there are also some works that target specific downstream tasks such as graph classification. In this case, the prompt tuning can be more task-directed. For example, GPF <ref type="bibr" target="#b10">[11]</ref> aims at a graph classification task, so it just needs to tune the prompt token p and the task head Ï• by maximizing the likelihood of predicted correct graph labels given the prompted graph representation si from the pre-trained graph model Ï€. In this case, the task head tuning and the prompt tuning share the same objectives, which can be formulated by: max p,Ï• (yi,si) p Ï€,Ï• (y i |s i ).</p><p>C. Tuning in Line with Pretext. Intuitively, prompting aims at reformulating downstream tasks to the pre-training task. Therefore, it would be more natural if the prompt tuning shares the same objective with the pre-training task. As suggested in GraphPrompt <ref type="bibr" target="#b51">[52]</ref>, the authors use a similar loss function to learn prompts. Similarly, GPPT <ref type="bibr" target="#b77">[78]</ref> and VNT <ref type="bibr" target="#b82">[83]</ref> adopt the same loss function (Cross-Entropy) as their link prediction and node classification tasks, respectively. HetGPT <ref type="bibr" target="#b54">[55]</ref> and SAP <ref type="bibr" target="#b16">[17]</ref> use a node-level contrastive loss to learn their prompt tokens because their pre-training task is also conducted by the same contrastive task (node pair comparison). PGCL <ref type="bibr" target="#b17">[18]</ref> introduces graph-level loss to align with the pre-training task. ULTRA-DP <ref type="bibr" target="#b3">[4]</ref> develop two pretraining tasks including edge prediction and neighboring prediction, each of which corresponds to one task embedding. In the pre-training phase, they randomly select a task and then integrate specific task-related embeddings into the prompt tokens. These learnable task embeddings are then trained with the graph model. The good point of GPF <ref type="bibr" target="#b10">[11]</ref> is that they propose a very simple prompt that can be easily used in various pre-training tasks and downstream tasks. However, a single prompt token added to all nodes is very limited in expressiveness and generalization. A potential solution is to learn an independent prompt token for each node, which means one node corresponds to one prompt token, but this will cause low efficiency in parameters. To this end, we can train Kindependent basis vectors and use them to compound each node token (GPF-Plus <ref type="bibr" target="#b11">[12]</ref>). This improvement makes their work have more similar insights with All in One <ref type="bibr" target="#b79">[80]</ref>.</p><p>HetGPT <ref type="bibr" target="#b54">[55]</ref> extends prompt tokens to type-specific format, which can deal with graph prompting in heterogeneous data. However, they can only deal with node classification tasks. To this end, GraphPrompt <ref type="bibr" target="#b51">[52]</ref> reformulates link prediction to graph pair similarity task. It is worth noticing that the role of their prompt token is very similar to the project vector in the graph attention network. There are also some attention-based graph-pooling methods, which share the same motivation. The difference is that the authors claim the graph-pooling component in the pre-training stage might not fit other downstream tasks, thus needing additional prompts to redirect the graph-pooling component in the graph model.</p><p>GPPT <ref type="bibr" target="#b77">[78]</ref> represents a specific instance within the broader framework of All in One <ref type="bibr" target="#b79">[80]</ref>. For instance, if we minimize the prompt graph to isolated tokens that correlate with node classes and substitute the resulting graphs with a complete graph, the All in One prompt structure can be simplified into the GPPT format. This allows for the utilization of edge-level pretexts in node classification tasks within the GPPT framework. The shortcoming of GPPT might be that it is restricted to binary edge prediction pretexts and is solely effective for node classification in downstream tasks. In comparison, frameworks like GraphPrompt and All in One are designed to accommodate a wider array of graphrelated tasks, extending beyond just node-level classification. The good point is that when adapting models for different tasks, GraphPrompt, GPF, and GPF-Plus often require the tuning of an extra task-specific module. In contrast, All in One, and GPPT utilize task templates that focus more on the manipulation of input data and are less dependent on the specifics of downstream tasks.</p><p>Intuitively, the data graphs, one of the components in PRODIGY <ref type="bibr" target="#b31">[32]</ref>, are very similar to the induced graph in All in One and GraphPrompt. The pre-training task in PRODIGY can be seen as predicting a link between the data token and the label token, which shares a similar insight with GPPT. The good thing is that their prompts have no trainable parameters, which means they do not need to tune the prompt and are more efficient in the in-context learning area. PRODIGY does not need any tuning work and can be used in knowledge transferring between different datasets. However, a non-tunable prompt is usually not flexible enough, which might also limit the generalization of the pre-trained model when the downstream tasks to be transferred are located in a different domain from the pretraining one. In contrast, ULTRA-DP <ref type="bibr" target="#b3">[4]</ref> tune prompt both in the pre-training stage and the downstream tasks. It first put the prompt tuning work in the pre-training stage to obtain the task embeddings, which are one of the main components in their prompt. Then they use these task embeddings to initialize a downstream prompt. Intuitively, their prompts are not used to reformulate downstream tasks to the pretext. Instead, these prompt tokens are used to select suitable pretraining tasks from a task pool to fit the downstream task. It is still an interesting question of how to achieve the optimal balance given efficiency, generalization, and the flexibility of prompt.</p><p>Compared with other works that usually define clear inserting patterns, VNT <ref type="bibr" target="#b82">[83]</ref> concatenates prompt tokens with the original node set and then puts all of them into the graph transformer. Actually, the graph transformer will leverage a self-attention function to further calculate the correlations among them, which can also be treated as a variant of inserting patterns defined in All in One. The good thing is that we do not need to design a threshold to tailor the connection but the shortcoming is that it can only use a graph transformer as its backbone and can not applied to more message-passing-based graph models. In addition, there are also some more advanced variants of graph transformers requiring additional position embedding as one of their input. However, the prompt tokens in VNT have no clear inserting links to the original graph, which might not make it easy to apply existing position encoding approaches for these graph transformer variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">GRAPH PROMPTING IN MULTI-MODAL AND MULTI-DOMAIN AREAS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Multi-Modal Prompting with Graphs</head><p>The fusion of images, sound, and text has been widely studied and achieved remarkable success. However, most of these modalities are described by linear data structure. In our real-world life, there are more kinds of data in non-linear structures like graphs. How to connect these linear modalities (e.g. text, images, sound, etc) to the non-linear modalities (e.g. graphs) has become a very attractive research topic because it is a bigger move towards artificial general intelligence. Unfortunately, reaching this vision is very tough. Currently, we only see some hard progress in the fusion of text and graphs, especially in the text-attributed graphs. With the help of recent large language models, the fusion of text and graph has achieved even more notable performance. Since there have already been some informative surveys on this topic, we next briefly discuss some representative works that are closely related to ........ prompt ............. techniques. We suggest readers refer to the mentioned literature <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b47">48]</ref> to require more detailed information further.</p><p>A. Prompt in Text-Attributed Graphs. Wen and Fang <ref type="bibr" target="#b96">[97]</ref> ventured into enhancing text classification in scenarios with limited data resources by the proposed model, Graph-Grounded Pre-training and Prompting (G2P2). Their work identifies the issue of insufficient labeled data in supervised learning and proposes a solution leveraging the inherent network structure of text data, such as hyperlinks or citation networks. G2P2 utilizes text-graph interaction strategies with contrastive learning during the pre-training phase, followed by an inventive prompting technique during the classification phase, demonstrating notable effectiveness in zero-and few-shot text classification tasks. Zhao et al. <ref type="bibr" target="#b120">[121]</ref> focused on molecule property prediction, a field grappling with the scarcity of labeled data. Their study introduces an integrated graph-text model to enhance prompt-based molecule task learning in a zero-shot context. This model employs generalized position embedding and decouples encoding of the graph from task prompt, enhancing its generalization capability across novel tasks. Li and Hooi <ref type="bibr" target="#b44">[45]</ref> explored node classification within the framework of multi-modal data (text and graph), particularly focusing on limited-label scenarios. Unlike traditional works that usually feed pre-computed text features into graph neural networks, they incorporate raw texts and graph topology by a handcrafted language prompt template into the model design.</p><p>B. Large Language Models in Graph Data Processing. Chen et al. <ref type="bibr" target="#b4">[5]</ref> explored the potential of Large Language Models (LLMs) in graph node classification tasks. They investigated two pipelines: LLMs-as-Enhancers, which enhances node text attributes using LLMs followed by predictions via Graph Neural Networks (GNNs), and LLMs-as-Predictors, which directly employs LLMs as standalone predictors. Their empirical evaluations revealed that deep sentence embedding models and text-level augmentation through LLMs effectively enhance node attributes, while LLMs also show promise as standalone predictors, albeit with concerns about accuracy and test data leakage. Fatemi et al. <ref type="bibr" target="#b13">[14]</ref> conducted a comprehensive study on encoding graph-structured data for consumption by LLMs. Their findings include the influence of the graph encoding method, the nature of the graph task, and the structure of the graph on LLM performance. They demonstrated that simple prompts are most effective for basic graph tasks and that graph encoding functions significantly impact LLM reasoning. Their experimental setup introduced modifications to the graph encoding function, revealing improvements in performance and demonstrating the effect of model capacity on graph reasoning ability. Jin et al. <ref type="bibr" target="#b35">[36]</ref> introduced an innovative approach to pre-train language models on networks rich in text. Their framework, named PATTON, focuses on integrating the intricacies of textual attributes with the underlying network structure. They developed two novel pretraining strategies: one concentrating on the context within networks for masked language modeling, and the other on predicting masked nodes, thus capturing the interplay between text and network structure. The effectiveness of PATTON was validated through various experiments, showcasing its superior performance over traditional text/graph pretraining methods in diverse tasks such as document classification, retrieval, and link prediction in different domain datasets. This approach signifies a shift in pretraining methodologies, emphasizing the synergy between textual data and network context. Wang et al. <ref type="bibr" target="#b95">[96]</ref> proposed a Knowledge Graph Prompting (KGP) method to enhance LLMs for multi-document question answering (MD-QA). They created a knowledge graph over multiple documents, with nodes representing passages or document structures and edges denoting semantic/lexical similarity. The LM-guided graph traverser in KGP navigates the graph to gather supporting passages, aiding LLMs in MD-QA. Their experiments indicated that the construction of KGs and the design of the LM-guided graph traverser significantly impact MD-QA performance.</p><p>C. Multi-modal Fusion with Graph and Prompting. The integration of multi-modal data using graph and prompting techniques has seen remarkable progress in recent years. For example, Edwards et al. <ref type="bibr" target="#b9">[10]</ref> propose SynerGPT in the field of drug synergy prediction. This model leverages a transformer-based approach, uniquely combining in-context learning with genetic algorithms to predict drug synergies. In the area of vision-language models, Li et al. <ref type="bibr" target="#b43">[44]</ref> develop GraphAdapter, a prompt-based strategy that utilizes an adapter-style tuning mechanism, bringing together textual and visual modalities through a dual knowledge graph. Liu et al. <ref type="bibr" target="#b48">[49]</ref> extend work multi-modal fusion into molecular science with their proposed GIT-Mol. A large language model integrates graph, image, and textual data with the help of prompt, offering substantial improvements in various tasks like molecule generation and property prediction. Although much effort has been made in the past few years, the academic is still trying hard to find better solutions to integrate text and graphs via text-attributed graphs or knowledge graphs. There is still a very large imagination in the fusion of more kinds of modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Graph Domain Adaptation with Prompting</head><p>The field of graph domain adaptation has seen significant advancements, particularly with the integration of prompting techniques. However, graph domain adaptation is still not a well-solved problem because there exist at least two challenges: The first one is how to align semantic spaces from different domains. The second one is how to identify structural differences.</p><p>A. Semantic Alignment. In particular, All in One <ref type="bibr" target="#b79">[80]</ref> extends the "pre-training and fine-tuning" workflow with multi-task prompting for GNNs, unifying prompt formats, and introducing meta-learning for prompt optimization. To make the graph model adaptive to different graph domains, they first reveal that the graph prompt in nature can be seen as graph operation and then they use graph prompt to manipulate different domain graph datasets. GraphControl <ref type="bibr" target="#b124">[125]</ref> introduces a unique deployment module inspired by ControlNet, effectively integrating downstream-specific information as conditional inputs to enhance the adaptability of pre-trained models to target data. This approach aligns input space across various graphs and incorporates unique characteristics of the target data. Zhang et al. <ref type="bibr" target="#b118">[119]</ref> presents a pre-training model for knowledge graph transfer learning. This model uses a general prompt-tuning mechanism, treating task data as a triple prompt, enabling flexible interactions between task KGs and task data. Yi et al. <ref type="bibr" target="#b109">[110]</ref> combines personalized graph prompts with contrastive learning for efficient and effective cross-domain recommendation, particularly in cold-start scenarios. A representative work is proposed by Liu et al. <ref type="bibr" target="#b46">[47]</ref>, in which they describe graph nodes from different domains by language and then use LLM to get a textual embedding. However, this work needs the semantic name of each feature while sometimes graph features are usually latent vectors without clear semantic meaning.</p><p>B. Structural Alignment. Cao et al. <ref type="bibr" target="#b2">[3]</ref> analyze the feasibility between different graph datasets and found that the structural gap holds the upper bound of various graph model transferability. GraphGLOW <ref type="bibr" target="#b121">[122]</ref> addresses the limitations of existing models that operate under a closedworld assumption, where the testing graph is identical to the training graph. This approach often leads to prohibitive computational costs and overfitting risks due to the need for training a structure learning model from scratch for each graph dataset. To this end, it coordinates a single graphshared structure learner with multiple graph-specific GNNs to capture generalizable patterns of optimal message-passing topology across datasets. The structure learner, once trained, can produce adaptive structures for unseen target graphs without fine-tuning, thereby significantly reducing training time and computational resources. AAGOD by Guo et al. <ref type="bibr" target="#b19">[20]</ref> proposes a data-centric framework for OOD detection in GNNs. They use a parameterized amplifier matrix, which is treated as a prompt, to superimpose on the adjacency matrix of input graphs. Inspired by prompt tuning, Shirkavand and Huang <ref type="bibr" target="#b73">[74]</ref> propose DeepGPT for graph transformer models. They add prompt tokens to the input graph and each transformer layer. By updating the prompt tokens, they can efficiently tune a graph model on different graph datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">POTENTIAL APPLICATIONS</head><p>With the widespread utilization of networks as a data modeling structure for representing diverse relational information across social, natural, and academic domains, the graph prompt mechanism exhibits substantial potential for a wide range of real-world applications. In this section, we explore the potential applications of graph prompting in online social networks, recommender systems, knowledge management, and biology.</p><p>Online Social Networks. Online social platforms consist of users who can be represented as nodes, and their social connections form online social networks (OSNs). Previous research has investigated the potential of prompting mechanisms in identifying fake news within OSNs to prevent malicious attacks <ref type="bibr" target="#b100">[101]</ref>. Specifically, they employ textual prompts applied to pre-trained language models (PLMs) to distill general semantic information. By combining this semantic signal with the dynamics of information propagation within social networks, improved classification performance can be achieved. While the use of tailored textual prompts for PLMs has been studied, the application of graph prompting mechanisms within social networks is still under-explored. In the future, it is promising to directly apply prompt tuning techniques to social networks, utilizing few-shot labels for tasks such as fake news detection or anomaly detection <ref type="bibr" target="#b98">[99,</ref><ref type="bibr" target="#b19">20]</ref>, where the labeling process is laborious and requires domain expertise. By incorporating prompts directly within social networks, this approach can address the scarcity of labeled data and enhance the security and trustworthiness of online social networks.</p><p>Recommender Systems. E-commerce platforms provide a valuable opportunity to leverage recommender systems for enhancing online services. While prompt tuning in recommender systems has received limited research attention, it holds significant potential <ref type="bibr" target="#b109">[110,</ref><ref type="bibr" target="#b108">109,</ref><ref type="bibr" target="#b102">103,</ref><ref type="bibr" target="#b21">22]</ref>. In <ref type="bibr" target="#b109">[110]</ref>, the graph prompt tuning technique is applied to crossdomain recommendation scenarios to address the challenges of domain adaptation. Specifically, when applying a pretrained recommendation model to the target domain, extra prompt nodes are introduced to achieve both efficient and effective domain recommendation. Meanwhile, Yang et al. <ref type="bibr" target="#b108">[109]</ref> propose personalized user prompts to bridge the gap between contrastive pretext <ref type="bibr" target="#b99">[100,</ref><ref type="bibr" target="#b111">112]</ref> to downstream recommendation task. They design different kinds of personalized prompts, in combination with pre-trained user embeddings to facilitate dynamic user representations, leading to more accurate and personalized recommending results. In the future, further exploration into the integration of graph prompt tuning within recommender systems can be conducted to enhance recommendation performance, personalization, and adaptability across different domains.</p><p>Knowledge Management. There are two branches of research focused on performing prompting on knowledge graph (KG) for improved knowledge management. The first branch involves direct prompting on knowledge graphs using a pre-trained KG model to facilitate knowledge transfer, enabling better generalization across different KG data and tasks <ref type="bibr" target="#b118">[119]</ref>. The second branch explores the combination of grounded knowledge from KGs with the cognitive abilities of LLMs <ref type="bibr" target="#b95">[96,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b117">118,</ref><ref type="bibr" target="#b58">59]</ref> to improve performance in downstream tasks. In the first research line, Zhang et al. <ref type="bibr" target="#b118">[119]</ref> proposed a structure pre-training and prompt tuning approach to realize knowledge transfer. They designed specific pre-training objectives to obtain a powerful KG model. Subsequently, a general prompt tuning technique was employed to facilitate knowledge transfer between taskspecific KGs and data. In the second research line, prompt tuning techniques are adopted to combine the grounded knowledge inherent in KGs with LLMs for enhancing downstream tasks. For example, in <ref type="bibr" target="#b84">[85]</ref>, a novel graph neural prompting method was introduced to adapt KGs for LLMs by distilling valuable knowledge from KGs in a timeand parameter-efficient manner. Future research can further explore this trend by implementing more efficient graph prompting methods to fully distill beneficial knowledge from KGs to assist LLMs on diverse downstream tasks.</p><p>Biology. Molecules can be represented naturally as graphs, where atoms serve as nodes and chemical bonds act as edges <ref type="bibr" target="#b67">[68]</ref>. Such graph modeling provides a basis for applying graph representation learning methods to perform tasks such as molecular property prediction, thereby benefiting scientific research and discovery <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b120">121,</ref><ref type="bibr" target="#b9">10]</ref>. Previous research on graph representation learning in the molecular domain followed a task-specific approach. It involved training individual models tailored to specific molecule datasets <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b75">76]</ref>, lacking the generalization ability within the domain. Though there exist works that explored the utilization of LLMs (or LMs) as a universal tool for understanding molecules <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b46">47]</ref>, these approaches primarily rely on regular texts to describe molecules, overlooking the inner graph structures within molecules <ref type="bibr" target="#b119">[120]</ref>. Meanwhile, some recent works have focused on investigating the comodeling of graphs and languages to preserve both structural dependencies and achieve generalization abilities across tasks and datasets, even under few-shot or zero-shot settings <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b120">121,</ref><ref type="bibr" target="#b9">10]</ref>. For instance, in <ref type="bibr" target="#b48">[49]</ref>, the authors employed LoRA <ref type="bibr" target="#b28">[29]</ref> to efficiently adapt to downstream tasks. Following this research direction, we believe that graph prompting techniques can also be adopted to achieve more efficient task adaptation within the molecular domain. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-processing</head><note type="other">Feature Engineering Data Prompting Evaluation Data Loader Model Backbone GCN GAT Task Level Prompting Method Graph Transformer Node</note><note type="other">Comprehensive Metrics Batch Evaluator Dynamic Dispatcher</note><p>Fig. <ref type="figure">8</ref>: The architecture of ProG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">PR OG: A UNIFIED LIBRARY FOR GRAPH PROMPT-ING</head><p>An indispensable component for fortifying the graph prompting ecosystem is a well-crafted tool. Despite the plethora of proposed for generalized graph learning, a notable absence persists in the realm of libraries dedicated to graph prompt functionalities. Addressing this gap, we are introducing ProG (Prompt Graph), an open-source, unified library meticulously designed to cater to the specific needs of graph prompting. This initiative promises to significantly enhance the landscape of graph-based applications by providing a versatile and comprehensive resource for researchers and practitioners alike.</p><p>ProG is a PyTorch-based library designed to facilitate single or multi-task prompting for pre-trained GNNs. The architecture is illustrated in Figure <ref type="figure">8</ref>. It seamlessly integrates several widely used datasets in the graph prompt evaluation, including Cora, CiteSeer, Reddit, Amazon, and Pubmed etc. The tool is equipped with essential evaluation metrics such as Accuracy, F1 Score, and AUC score, commonly employed in various graph prompt-related tasks. Notably, ProG incorporates state-of-the-art methods like All in One <ref type="bibr" target="#b79">[80]</ref>, GPPT <ref type="bibr" target="#b77">[78]</ref>, GPF <ref type="bibr" target="#b10">[11]</ref>, and GPF-Plus <ref type="bibr" target="#b11">[12]</ref>, and it continues integrating more graph prompt models. In summary, ProG offers the following key features:</p><p>â€¢ Quick Initiation. All models are implemented within a consistent environment, accompanied by detailed demos, ensuring a swift initiation for newcomers.</p><p>â€¢ Fully Modular. ProG adopts a modular structure, empowering users to customize and construct models as needed.</p><p>â€¢ Easy Extendable. ProG is designed for seamless extension to encompass additional methods and a broader spectrum of downstream tasks, adapting to evolving research needs.</p><p>â€¢ Standardized Evaluation. ProG establishes a uniform set of evaluation processes, promoting equitable performance comparisons across models.</p><p>For additional information and access to the library, please visit the website of our library 5 . Additionally, we have curated a GitHub repository 6 , serving as a centralized re-5. <ref type="url" target="https://github.com/sheldonresearch/ProG">https://github.com/sheldonresearch/ProG</ref> 6. <ref type="url" target="https://github.com/WxxShirley/Awesome-Graph-Prompt">https://github.com/WxxShirley/Awesome-Graph-Prompt</ref> source for the latest advancements in graph prompt learning. This repository includes a list of research papers, benchmark datasets, and available codes, fostering an environment conducive to ongoing research in this dynamic field. Regular real-time updates ensure that the repository remains current with emerging papers and associated codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CHALLENGES AND FUTURE DIRECTIONS 9.1 Current Challenges</head><p>Graph prompt learning has made significant research progress, but it still encounters several challenges. In this subsection, we will discuss current challenges in detail.</p><p>Inherent Limitation within Graph Models Prompts, originated from the NLP field <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b49">50]</ref>, serve as a means to unlock the potential of pre-trained language models for adapting to downstream tasks. This in-context learning ability emerges when the scale of model parameters reaches a certain level. For instance, widely known LLMs typically possess billions of parameters. With such a powerful pretrained model, a simple textual prompt can distill specific knowledge for downstream tasks. However, when it comes to prompting on graph tasks, the conditions become more intractable since pre-trained graph models have significantly fewer parameters, making it challenging to harness their full downstream adaptation potential.</p><p>Intuitive Evaluation of Graph Prompt Learning In the NLP field, prompts are typically in a discrete textual format <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b66">67]</ref>, allowing for intuitive understanding, comparison, and explanation. However, existing graph prompts are represented as learnable tokens <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b16">17]</ref> or augmented graphs <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b31">32]</ref>. Such format poses challenges in intuitively understanding and interpreting graph prompts, as they lack a readable design. As a result, the effectiveness of prompts can only be evaluated based on downstream tasks, limiting efficient and comprehensive performance comparison among different kinds of graph prompts. Therefore, the development of a more intuitive graph prompt design with a readable format remains an open problem.</p><p>More Downstream Applications Currently, graph prompt learning is primarily applied to node or graph classification tasks on open-source benchmarks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b106">107]</ref>. Although potential applications have been discussed in Section 7, the real-world utilization of graph prompt learning remains limited. A notable example is its use in fraud detection within real-world transaction networks, addressing the issue of label scarcity <ref type="bibr" target="#b98">[99]</ref>. However, compared to the widespread application of prompting techniques in the NLP domain <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b117">118]</ref>, the potential of graph prompt learning in diverse real-world applications requires further exploration. Overcoming the main challenges of obtaining powerful domain-specific pre-trained graph models and designing suitable prompts for specific application scenarios that exhibit unique characteristics remains crucial.</p><p>Transferable Prompt Designs Existing studies on graph prompt learning <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17]</ref> typically focus on pre-training and prompt tuning using the same dataset, which limits the exploration of more transferable designs and empirical evaluation. Although PRODIGY <ref type="bibr" target="#b31">[32]</ref> explores transferability within the same domain and All in One <ref type="bibr" target="#b79">[80]</ref> provides empirical results regarding transferability across tasks and domains, the investigation of prompt learning across diverse domains and tasks remains limited. Achieving transferability across diverse tasks and domains requires aligning the task space <ref type="bibr" target="#b79">[80]</ref>, semantic features <ref type="bibr" target="#b124">[125]</ref>, and structural patterns <ref type="bibr" target="#b121">[122]</ref>, which necessitates further theoretical work to provide insights guiding the development of transferable prompt designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Future Directions</head><p>With the above analysis on graph prompt, we summarize future directions as follows:</p><p>Learning Knowledge from Large Graph Models (LGMs) like LLMs. Currently, graph models are typically tailored to specific domains or tasks, limiting generalization abilities across broader domains or tasks <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b124">125]</ref>. Therefore, we are expecting the realization of large graph models (LGMs) as a universal tool to be intelligent enough to handle graph tasks across domains <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b13">14]</ref>. A significant step towards this direction has been taken by Liu et al. <ref type="bibr" target="#b46">[47]</ref>, who proposed a powerful model (OFA) capable of addressing classification tasks for graphs coming from various domains. However, their approach still relies on LLMs as a general template to distill specific domain knowledge, which we believe can be replaced with suitable graph prompting techniques. Just as textual prompt has been widely used to adapt LLM for diverse applications <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b1">2]</ref>, graph prompting techniques are promising to distill LGMs' knowledge specific for concrete downstream tasks. Applying the graph prompts to LGMs has the potential to revolutionize the field of deep graph learning, leveraging LGMs as a universal tool to tackle different tasks on graphs from diverse domains.</p><p>Transferable Learning. As we have discussed in Section 9.1, current graph prompt learning methods <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b73">74]</ref> are primarily limited to intra-dataset/domain settings, lacking the ability to transfer knowledge across different tasks or domains. While there have been efforts in graph transfer learning <ref type="bibr" target="#b124">[125]</ref> to realize domain adaptation, research specifically focused on transferable graph prompting techniques remains limited. To enable the transfer ability, the prompts should be designed to realize the alignment between different task spaces <ref type="bibr" target="#b79">[80]</ref>, reformulating different tasks on graphs into a uniform template. Besides, both structural <ref type="bibr" target="#b121">[122]</ref> and semantics alignment <ref type="bibr" target="#b124">[125]</ref> should be realized via suitable graph prompts to enable domain adaptation. It is promising to implement transferable graph prompts, realizing knowledge transfer across domains to extend the generalization and applicability of graph models.</p><p>More Theoretical Foundation. Despite the great success of graph prompt learning on various downstream tasks, they mostly draw on the successful experience of prompt tuning on NLP domain <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b72">73]</ref>. In other words, most existing graph prompt tuning methods are designed with intuition, and their performance gains are evaluated by empirical experiments. The lack of sufficient theoretical foundations behind the design has led to both performance bottlenecks and poor explainability. Therefore, we believe that building a solid theoretical foundation for graph prompt learning from a graph theory perspective and minimizing the gap between the theoretical foundation and empirical design is also a promising future direction.</p><p>More Explainable and Understandable Design. While existing graph prompt learning methods have demonstrated impressive results on various downstream tasks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b108">109]</ref>, we still lack a clear understanding of what exactly is being learned from the prompts. The black-box learning mode of prompt vectors raises questions about their interpretability and whether we can establish meaningful correspondences between the input data and the prompted graph <ref type="bibr" target="#b79">[80]</ref>. These issues are crucial for understanding and interpreting prompts but are currently missing in most graph prompt research. To work towards trustworthy graph prompt learning, it is promising to explore the self-interpretability <ref type="bibr" target="#b6">[7]</ref> to enable intuitive explanations of graph prompts. By gaining insights into the learned prompt vectors and structures, we can enhance our understanding of the underlying mechanisms and improve the interpretability of graph prompts. This, in turn, can lead to more effective utilization of prompts for security-or privacy-related downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">CONCLUSION</head><p>In this survey, we explore the promising intersection between Artificial General Intelligence and graph data by graph prompt. Our unified framework has unveiled a structured understanding of graph prompts, dissecting them into tokens, token structures, and inserting patterns. This framework is a novel contribution, providing clarity and comprehensiveness for researchers and practitioners. By exploring the interplay between graph prompts and models, we've revealed fresh insights into the essence of graph prompts, highlighting their pivotal role in reshaping AI for graph data. With the development of ProG, a Python library, and a dedicated website, we've expanded the graph prompting ecosystem, enhancing collaboration and access to research, benchmark datasets, and code implementations. Our survey outlines a roadmap for the future. The challenges and future directions we've discussed serve as a beacon for the evolving field of graph prompting. With the above work, we hope our survey can push forward a new era of insights and applications in AGI family.</p><p>collected and analyzed the references, and drew most of the figures.</p><p>â€¢ Hong Cheng, Yun Xiong, and Jia Li: were in charge of proofreading, and discussion, and contributed with many insightful opinions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: The Most Frequently Problems towards Artificial General Intelligence. (a) Cross-modalities, (b) Cross-domains, (c) Cross-tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Statistics of the collected papers</figDesc><graphic coords="6,282.86,53.66,257.99,169.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Our perspective of prompt upon flexibility and expressiveness. (a) Shallow node embedding methods offer flexibility across different downstream tasks but sacrifice expressiveness. (b) GNNs provide expressiveness but require task-specific supervision and constant node features, limiting their flexibility. (c) Prompts enable a balanced approach, achieving both flexibility and expressiveness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Graph pre-training methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>5. 1 Fig. 7 :</head><label>17</label><figDesc>Fig. 7: Prompt Tokens, Structures, and Inserting Patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>5. 3</head><label>3</label><figDesc>Prompt Tuning ........... Question ... 3: ...... How ... do ....... They ....... Learn ... A ........ Graph ........... Prompt? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>5. 4</head><label>4</label><figDesc>Further Discussion ........... Question ... 4: ....... What .... are ....... Their ................ Connections, ...... Pros ..... and . . . . . . . . Cons?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 :</head><label>1</label><figDesc>Research Objectives</figDesc><table><row><cell>Foundamental Problems</cell><cell>P1: How to Make the Model General for Different Modalities? (Section 6.1)</cell></row><row><cell>towards AGI</cell><cell>P2: How to Make the Model General for Different</cell></row><row><cell></cell><cell>Domains? (Section 6.2)</cell></row><row><cell></cell><cell>P3: How to Make the Model General for Different</cell></row><row><cell></cell><cell>Tasks? (Section 5)</cell></row><row><cell></cell><cell>Q1: How to Understand Existing Work with a</cell></row><row><cell>Detailed</cell><cell>Unified Framework? (Section 5.1)</cell></row><row><cell>Questions of</cell><cell>Q2: What's the Nature of Graph Prompt?</cell></row><row><cell>Graph Prompt</cell><cell>(Section 3.5, Section 5.4)</cell></row><row><cell></cell><cell>Q3: How to Design Graph Prompts? (Section 5)</cell></row><row><cell></cell><cell>Q4: How to Deploy Graph Prompts in Real-world</cell></row><row><cell></cell><cell>Applications? (Section 7, Section 8)</cell></row><row><cell></cell><cell>Q5: What Are the Current Challenges and Future</cell></row><row><cell></cell><cell>Directions? (Section 9.1)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>ACM Computing Surveys, 1% Nature, 1% NAACL, 1% TKDE, 2% WSDM, 2% AAAI, 3% ACL, 3% SIGIR, 3% EMNLP, 3% IJCAI, 3% ICML, 3% CIKM, 3% ICLR, 6% The Web Conference, 6% KDD, 10% NeurIPS, 10% arXiv preprint, 31%</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Graph Pre-training Task-specific Fine-tuning Task-agnostic Prompting</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>? ? ?</cell></row><row><cell>Node-level</cell><cell></cell></row><row><cell></cell><cell>Masked Feature Regression</cell></row><row><cell>Edge-level</cell><cell></cell></row><row><cell>? ? ?</cell><cell>? ? ?</cell></row><row><cell>Graph-level</cell><cell></cell></row><row><cell>Contrastive Method</cell><cell>Predictive Method</cell></row></table><note><p><p>ï¼Ÿ</p>Auxiliary Property Prediction</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2 :</head><label>2</label><figDesc>Summary of existing representative works on graph prompt.</figDesc><table><row><cell></cell><cell cols="3">pre-training task</cell><cell></cell><cell>prompt design</cell><cell></cell><cell cols="3">downstream tasks</cell><cell cols="2">answering function</cell></row><row><cell>Paper</cell><cell>node</cell><cell>edge</cell><cell>graph</cell><cell>prompt components</cell><cell>inserting pattern</cell><cell>prompt tuning</cell><cell>node</cell><cell>edge</cell><cell>graph</cell><cell>Preset</cell><cell>Learnable</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>structure token:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GPPT (KDD 2022 [78])</cell><cell>âœ—</cell><cell>âœ“</cell><cell>âœ—</cell><cell>sv âˆˆ R d task token:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>cy</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>S: Subgraph. V (S): Node set within subgraph S. Ï€: Pre-trained parameters. Ï•: Task head parameters. Î¸: Prompt parameters. s: Filled prompt.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Answering Function . . . . . . . . . . . Question . . . 2: . . . . . . . How . . . . do ...... they .............. reformulate ................ downstream ....... tasks ... to ..... the . . . . . . . . .</figDesc><table /><note><p>pretext? A. Handling Different Level Tasks. All in One</p></note></figure>
		</body>
		<back>

			
			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>â€¢ Xiangguo Sun: wrote section 1, section 2, section 3.5 (with Xixi), the main content of section 5, section 6, did proofreading of the whole paper, and proposed most of the original insights.</p><p>â€¢ Jiawen Zhang: wrote most of section 3 (section 3.1-3.4), section 4, section 5(collect, analyze papers and supplement insightful content), section 8, and were in charge of library development.</p><p>â€¢ Xixi Wu: wrote section 3.5, section 5 (collect, analyze papers and supplement insightful content), section 7, most of section 9 (challenges and future directions), </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Transformers and Large Language Models for Chemistry and Drug Discovery</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schwaller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Language Models Are Few-Shot Learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">When to pre-train graph neural networks? An answer from data generation perspective!</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">ULTRA-DP: Unifying Graph Pre-training with Multitask Graph Dual Prompt</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wiener graph deconvolutional network improves graph self-supervised learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tsung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7131" to="7139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards Self-Explainable Graph Neural Network</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="302" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design</title>
		<author>
			<persName><forename type="first">C</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hope</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Prompt tuning for graph neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Universal Prompt Tuning for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Geometry-enhanced molecular representation learning for property prediction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="134" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Talk like a graph: Encoding graphs for large language models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Halcrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Making Pre-Trained Language Models Better Few-Shot Learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3816" to="3830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Enhancing Graph Neural Networks with Structure-Based Prompt</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Prompt Tuning for Multi-View Graph Contrastive Learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Data-Centric Framework to Endow Graph Neural Networks with Out-of-Distribution Detection Ability</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="638" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Motif-Based Prompt Learning for Universal Cross-Domain Recommendation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>WSDM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-Implicit Graph Variational Auto-Encoders</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hasanzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hajiramezanali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duffield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Contrastive Multi-View Representation Learning on Graphs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H K</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="4116" to="4126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BERTese: Learning to Speak to BERT</title>
		<author>
			<persName><forename type="first">A</forename><surname>Haviv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3618" to="3623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BernNet: Learning Arbitrary Graph Spectral Filters via Bernstein Approximation</title>
		<author>
			<persName><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="14" to="239" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">GraphMAE: Self-Supervised Masked Graph Autoencoders</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="594" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">GraphMAE2: A Decoding-Enhanced Masked Self-Supervised Graph Learner</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kharlamov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="737" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">LoRA: Low-Rank Adaptation of Large Language Models</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Strategies for Pre-training Graph Neural Networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">GPT-GNN: Generative Pre-Training of Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1857" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">PRODIGY: Enabling Incontext Learning Over Graphs</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>KrÅ¾manc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pre-training on Large-Scale Heterogeneous Graph</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="756" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Contrastive Pre-Training of GNNs on Heterogeneous Graphs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="803" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">How Can We Know What Language Models Know?</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="423" to="438" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Patton: Language Model Pretraining on Text-Rich Networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-Scale Contrastive Siamese Networks for Self-Supervised Graph Representation Learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1477" to="1483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Node Similarity Preserving Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The Power of Scale for Parameter-Efficient Prompt Tuning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3045" to="3059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">What&apos;s Behind the Mask: Understanding Masked Graph Modeling for Graph Autoencoders</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1268" to="1279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">AdapterGNN: Efficient Delta Tuning Improves Generalization Ability in Graph Neural Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Prefix-Tuning: Optimizing Continuous Prompts for Generation</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4582" to="4597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Prompt-Based Zero-and Few-Shot Node Classification: A Multimodal Approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hooi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A survey of graph meets large language model: Progress and future directions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">One for All: Towards Training One Graph Model for All Classification Tasks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Towards Graph Foundation Models: A Survey and Beyond</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">P-Tuning: Prompt Tuning Can Be Comparable to Fine-Tuning Across Scales and Tasks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="61" to="68" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Graphprompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="417" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pre-training graph neural networks for link prediction in biomedical networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Kwoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2254" to="2262" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mortazavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning Convolutional Neural Networks for Graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Asymmetric Transitivity Preserving Graph Embedding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1105" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Adversarially Regularized Graph Autoencoder for Graph Embedding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2609" to="2615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Unifying Large Language Models and Knowledge Graphs: A Roadmap</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Graph-Guided Reasoning for Multi-Hop Question Answering in Large Language Models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">Z</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-K</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Symmetric Graph Convolutional Autoencoder for Unsupervised Graph Representation Learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6519" to="6528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Graph Representation Learning via Graphical Mutual Information Maximization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="259" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Can Large Language Models Empower Molecular Property Prediction?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning How to Ask: Querying LMs with Mixtures of Soft Prompts</title>
		<author>
			<persName><forename type="first">G</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5203" to="5212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Leveraging Large Language Models for Multiple Choice Question Answering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Rytting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wingate</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Self-Supervised Graph Transformer on Large-Scale Molecular Data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12" to="559" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">To transfer or not to transfer</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Rosenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Marx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">898</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Few-Shot Text Generation with Natural Language Instructions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sch Ãœtze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="390" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">It&apos;s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners</title>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2339" to="2352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Masked Label Prediction: Unified Message Passing Model for Semi-Supervised Classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1548" to="1554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Autoprompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L L</forename><surname>Iv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4222" to="4235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Deep Prompt Tuning for Graph Transformers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shirkavand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">MOTIF-Driven Contrastive Learning of Graph Representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Subramonian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="15" to="980" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">In-foGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization</title>
		<author>
			<persName><forename type="first">F.-Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">MoCL: Data-driven Molecular Fingerprint via Knowledgeaware Contrastive Learning from Molecular Graph</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3585" to="3594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">GPPT: Graph Pre-Training and Prompt Tuning to Generalize Graph Neural Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1717" to="1727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Heterogeneous Hypergraph Embedding for Graph Classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Q</forename><surname>Viet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>WSDM</publisher>
			<biblScope unit="page" from="725" to="733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">All in One: Multi-Task Prompting for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2120" to="2131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Adversarial Graph Augmentation to Improve Graph Contrastive Learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="15" to="920" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">S2GAE: Self-Supervised Graph Autoencoders are Generalizable Learners with Graph Masking</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="787" to="795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Virtual Node Tuning for Few-shot Node Classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2177" to="2188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Bootstrapped representation learning on graphs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thakoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>VeliÄkoviÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Graph Neural Prompting with Large Language Models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Multimodal Few-Shot Learning with Frozen Language Models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tsimpoukelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="200" to="212" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Deep Graph Infomax</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li Ã’</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">MGAE: Marginalized Graph Autoencoder for Graph Clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Scientific discovery in the age of artificial intelligence</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">620</biblScope>
			<biblScope unit="issue">7972</biblScope>
			<biblScope unit="page" from="47" to="60" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Self-Supervised Learning of Contextual Embeddings for Link Prediction in Heterogeneous Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2946" to="2957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Common neighbors matter: fast random walk sampling with common neighbor awareness</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="4570" to="4584" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Hyperbolic heterogeneous information network embedding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5337" to="5344" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Self-supervised Heterogeneous Graph Neural Network with Cocontrastive Learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1726" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Knowledge Graph Prompting for Multi-Document Question Answering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lipka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexa</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Augmenting Low-Resource Text Classification with Graph-Grounded Pre-Training and Prompting</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m">Prompt Tuning on Graph-augmented Lowresource Text Classification</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Voucher Abuse Detection with Prompt-based Fine-tuning on Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Self-supervised Graph Learning for Recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Promptand-Align: Prompt-Based Social Alignment for Few-Shot Fake News Detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">A Survey of Graph Prompting Methods: Techniques, Applications, and Challenges</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Personalized Prompt for Sequential Recommendation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Self-Supervised Representation Learning via Latent Graph Prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="24" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Self-Supervised Learning of Graph Neural Networks: A Unified Review</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAML</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2412" to="2429" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">In-foGCL: Information-Aware Graph Contrastive Learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="30" to="414" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">Data-centric graph learning: A survey</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">An Empirical Study Towards Prompt-Tuning for Graph Contrastive Pre-Training in Recommendations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Contrastive Graph Prompt-tuning for Cross-domain Recommendation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOIS</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="5812" to="5823" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Are Graph Augmentations Necessary? Simple Graph Contrastive Learning for Recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Q V</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Self-Supervised Learning for Recommender Systems: A Survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Graph-Bert: Only Attention is Needed for Learning Graph Representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">ProNE: Fast and Scalable Network Representation Learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">Link Prediction Based on Graph Neural Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">Benchmarking Large Language for News Summarization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ladhak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2581" to="2590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<title level="m" type="main">Large graph models: A perspective</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">GraphGLOW: Universal and Generalizable Structure Learning for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3525" to="3536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Graph Contrastive Learning with Adaptive Augmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2069" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">SGL-PT: A Strong Graph Learner with Graph Prompt Tuning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Graph-Control: Adding Conditional Control to Universal Graph Pre-trained Models for Graph Domain Transfer Learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
