<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts</title>
				<funder ref="#_9ARV7uH">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
				<funder ref="#_vWej6fr">
					<orgName type="full">China National Postdoctoral Program for Innovative Talents</orgName>
				</funder>
				<funder ref="#_ZrnR5vA">
					<orgName type="full">China Postdoctoral Science Foundation</orgName>
				</funder>
				<funder ref="#_x5ZTRXE #_KKGyXWz">
					<orgName type="full">Beijing National Research Center for Information Science and Technology</orgName>
				</funder>
				<funder>
					<orgName type="full">Beijing Key Lab of Networked Multimedia</orgName>
				</funder>
				<funder ref="#_AvwGhvd #_Z6cXBjJ #_f6K9Zyt #_2bdVhuq">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-03-08">8 Mar 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
							<email>zy-zhang20@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution" key="instit1">BNRist</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
							<email>xin_wang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution" key="instit1">BNRist</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
							<email>zwzhang@tsinghua.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Zhou</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution" key="instit1">BNRist</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weigao</forename><surname>Wen</surname></persName>
							<email>weigao.wen@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Xue</surname></persName>
							<email>hui.xueh@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution" key="instit1">BNRist</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
							<email>wwzhu@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution" key="instit1">BNRist</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-03-08">8 Mar 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">7F70BAA41DEBC66ECDC7DC28C8DA8719</idno>
					<idno type="arXiv">arXiv:2403.05026v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-01-20T06:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dynamic graph neural networks (DyGNNs) currently struggle with handling distribution shifts that are inherent in dynamic graphs. Existing work on DyGNNs with out-of-distribution settings only focuses on the time domain, failing to handle cases involving distribution shifts in the spectral domain. In this paper, we discover that there exist cases with distribution shifts unobservable in the time domain while observable in the spectral domain, and propose to study distribution shifts on dynamic graphs in the spectral domain for the first time. However, this investigation poses two key challenges: i) it is non-trivial to capture different graph patterns that are driven by various frequency components entangled in the spectral domain; and ii) it remains unclear how to handle distribution shifts with the discovered spectral patterns. To address these challenges, we propose Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts (SILD), which can handle distribution shifts on dynamic graphs by capturing and utilizing invariant and variant spectral patterns. Specifically, we first design a DyGNN with Fourier transform to obtain the ego-graph trajectory spectrums, allowing the mixed dynamic graph patterns to be transformed into separate frequency components. We then develop a disentangled spectrum mask to filter graph dynamics from various frequency components and discover the invariant and variant spectral patterns. Finally, we propose invariant spectral filtering, which encourages the model to rely on invariant patterns for generalization under distribution shifts. Experimental results on synthetic and real-world dynamic graph datasets demonstrate the superiority of our method for both node classification and link prediction tasks under distribution shifts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dynamic graph neural networks (DyGNNs) have achieved remarkable success in many predictive tasks over dynamic graphs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Existing DyGNNs exhibit limitations in handling distribution shifts, which naturally exist in dynamic graphs due to multiple uncontrollable factors <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. Existing work on out-of-distribution generalized DyGNNs focuses on handling distribution shifts in the time domain. For example, DIDA <ref type="bibr" target="#b6">[7]</ref> utilizes dynamic graph attention to mask the graph trajectories to capture the invariant patterns on dynamic graphs, which assumes that in the time domain, the distribution shift is observable and the invariant and variant patterns can be easily disentangled.</p><p>However, there exist cases that the distribution shift is unobservable in the time domain while observable in the spectral domain, as shown in Figure <ref type="figure" target="#fig_0">1</ref>. The shift in frequency components can In this case, the frequency components in the invariant spectrums determine the node labels, while the relationship between the variant spectrums and labels is not stable under distribution shifts.</p><p>be clearly observed in the spectral domain, while these components are indistinguishable in the time domain. Moreover, in real-world applications, the observed dynamic graphs usually consist of multiple mixed graph structural and featural dynamics from various frequency components <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>To address this problem, in this paper, we study the problem of handling distribution shifts on dynamic graphs in the spectral domain for the first time, which poses the following two key challenges: i) it is non-trivial to capture different graph patterns that are driven by various frequency components entangled in the spectral domain, and ii) it remains unclear how to handle distribution shifts with the discovered spectral patterns.</p><p>To tackle these challenges, we propose Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts (SILD <ref type="foot" target="#foot_0">3</ref> ). Our proposed SILD model can effectively handle distribution shifts on dynamic graphs by discovering and utilizing invariant and variant spectral patterns. Specifically, we first design a DyGNN with Fourier transform to obtain the ego-graph trajectory spectrums so that the mixed graph dynamics can be transformed into separate frequency components. Then we develop a disentangled spectrum mask that leverages the amplitude and phase information of the ego-graph trajectory spectrums to obtain invariant and variant spectrum masks so that graph dynamics from various frequency components can be filtered. Finally, we propose an invariant spectral filtering that discovers the invariant and variant patterns via the disentangled spectrum masks, and minimize the variance of predictions with exposure to various variant patterns. As such, SILD is able to exploit invariant patterns to make predictions under distribution shifts. Experimental results on several synthetic and real-world datasets, including both node classification and link prediction tasks, demonstrate the superior performance of our SILD model compared to state-of-the-art baselines under distribution shifts. To summarize, we make the following contributions:</p><p>• We propose to study distribution shifts on dynamic graphs in the spectral domain, to the best of our knowledge, for the first time.</p><p>• We propose Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts (SILD), which can handle distribution shifts on dynamic graphs in the spectral domain.</p><p>• We employ DyGNN with Fourier transform to obtain the node spectrums, design a disentangled spectrum mask to obtain invariant and variant spectrum masks in the spectral domain, and propose the invariant spectral filtering mechanism so that SILD is able to handle distribution shifts.</p><p>• We conduct extensive experiments on several synthetic and real-world datasets, including both node classification and link prediction tasks, to demonstrate the superior performance of our method compared to state-of-the-art baselines under distribution shifts.</p><p>Following out-of-distribution (OOD) generalization literature <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>, we make the following assumptions of distribution shifts on dynamic graphs: Assumption 1. For a given task, there exists a predictor f (•), for samples (G 1:t v ,y t ) from any distribution, there exists an invariant pattern P t I (v) and a variant pattern P t V (v) such that the following conditions are satisfied: 1) the invariant patterns are sufficient to predict the labels,</p><formula xml:id="formula_0">y t v = f (P t I (v)) + ϵ,</formula><p>where ϵ is a random noise, 2) the observed data is composed of invariant and variant patterns,</p><formula xml:id="formula_1">P t I (v) = G 1:t v \P t V (v), 3</formula><p>) the influence of the variant patterns on labels is shielded by the invariant patterns, y t v ⊥ P t V (v) | P t I (v).</p><p>In the next section, inspired by <ref type="bibr" target="#b15">[16]</ref>, we give a motivation example to provide some high-level intuition before going to our formal method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Motivation Example</head><p>Here we introduce a toy dynamic graph example to motivate learning invariant patterns in the spectral domain. We assume that the invariant and variant patterns lie in the 1-hop neighbors, i.e., each node has an invariant subgraph and a variant subgraph. For simplicity, we focus on the number of neighbors, i.e., each node v has an invariant subgraph related degree d v,1 ∈ R T ×1 and a variant subgraph related degree d v,2 ∈ R T ×1 . Only the former determines the node label, i.e., y v = g ⊤ d v,1 . Note that invariant and variant subgraphs are not observed in the data. We further assume a one-dimensional constant feature for each node, which is set as 1 without loss of generality.</p><p>For the model in the spatial-temporal domain, we adopt sum pooling as one-layer graph convolution, i.e., the message passing for each node and time is</p><formula xml:id="formula_2">h v = u∈Nv 1 = d v,1 + d v,2 .</formula><p>We further adopt a mask m ∈ R T ×1 to filter patterns in the temporal domain and make predictions by a linear classifier, i.e., ŷv = w ⊤ (m ⊙ h v ), where w ∈ R T ×1 denotes the learnable parameters. Then, the empirical risk in the training dataset</p><formula xml:id="formula_3">D tr is R tr (w) = 1 |Dtr| v∈Dtr (ŷ v -y v ) 2 .</formula><p>We have the following proposition. Proposition 1. For any mask m ∈ R T ×1 , for the optimal classifier in the training data w * = arg min w R tr (w) , if ||m ⊙ w * || 2 ̸ = 0, there exist OOD nodes with unbounded error, i.e., ∃v s.t.</p><formula xml:id="formula_4">lim ||dv,2||→∞ (ŷ v -y v ) 2 = ∞.</formula><p>The proposition 1 shows that a classifier trained with masks and empirical risk minimization has unbounded risks in testing data under distribution shifts as the classifier uses variant patterns to make predictions. Next, we show that under mild conditions, an invariant linear classifier in the spectral domain can solve this problem. Denote Φ ∈ C T ×T as the Fourier bases, where</p><formula xml:id="formula_5">Φ k,t = 1 √ T e -j 2πkt</formula><p>T .</p><p>Denote z v = Φ u∈Nv 1 as the spectral representation after a linear message-passing. The prediction is ŷv = w H (m ⊙ z v ), where m ∈ C T ×1 is the mask to filter the spectral patterns, w ∈ C T ×1 is a linear classifier, and (•) H denotes Hermitian transpose. We have the following proposition.</p><formula xml:id="formula_6">Proposition 2. If Φd v,1 ⊙ Φd v,1 ⊙ Φd v,2 ⊙ Φd v,2 = 0, ∀d v,1 , d v,2 , then ∃m ∈ C T ×1</formula><p>such that the optimal spectral classifier in the training data has bounded error, i.e., for w * = arg min w R tr (w), ∃ϵ &gt; 0, ∀v, lim ||dv,2||→∞ (ŷ v -y v ) 2 &lt; ϵ.</p><p>The proposition 2 shows that if the frequency bandwidths of invariant and variant patterns do not have any overlap, there exists a spectral mask such that a linear classifier trained with empirical risk minimization in the spectral domain will have bounded risk in any testing data distribution. This example motivates us to capture invariant and variant patterns in the spectral domain, which is not feasible in the spatial-temporal domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>In this section, we introduce our method named Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts (SILD) to handle distribution shifts in dynamic graphs, including three modules, dynamic graph neural networks with Fourier transform, disentangled spectrum mask, and invariant spectral filtering. The framework of our method is shown in Figure <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dynamic Graph Neural Network with Spectral Transform</head><p>Dynamic Graph Trajectories Modeling Each node on the dynamic graph has its ego-graph trajectory evolving through time that may determine the node properties or the occurrence of future links. Following <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>, we adopt a message-passing network for each graph snapshot to aggregate the neighborhood information at the current time, i.e.,</p><formula xml:id="formula_7">m t u→v ← MSG(h t u , h t v ), h t v ← AGG({m t u→v | u ∈ N t (v)}, h t v }),<label>(1)</label></formula><p>where 'MSG' and 'AGG' denote message and aggregation functions, m t u→v is the message from node u to node v, h t u is the node embedding for node u at time t, N t (v) = {u | (u, v) ∈ E t } is node v's neighborhood at time t. To model the high-order neighborhood information, we can stack multiple message-passing layers. In this way, the node embedding along time {h t u } T t=1 summarizes the evolution of node u's ego-graph trajectories. We denote H ∈ R T ×N ×d as the ego-graph trajectory signals for all nodes on the dynamic graph, where T denotes the total time length, N denotes the number of nodes and d denotes the hidden dimensionality.</p><p>Spectral Transform As some patterns on dynamic graphs are unobservable in the time domain, while observable in the spectral domain, we transform the summarized ego-graph trajectory signals H into the spectral domain via Fourier transform for each node and hidden dimension, i.e.,</p><formula xml:id="formula_8">Φ k,t = 1 √ T e -j 2πkt T , Z = ΦH,<label>(2)</label></formula><p>where Φ ∈ C K×T denotes the Fourier bases, K denotes the number of frequency components, and Z ∈ C K×N ×d denote the node embeddings along frequency components in the spectral domain, and</p><formula xml:id="formula_9">Z k,n,m = T t=1 Φ k,t H t,n,m</formula><p>. By choosing the Fourier bases, our spectral transform has the following advantages: 1) we can use fast Fourier transform (FFT) <ref type="bibr" target="#b26">[27]</ref> to accelerate the computation. The computation complexity of Eq. ( <ref type="formula" target="#formula_8">2</ref>) can be reduced from O(N dT 2 ) to O(N dT logT ). 2) Each basis has clear semantics, e.g., Z k denotes the node embeddings at the k-th frequency component in the spectral domain. In this way, we can observe how the nodes on the dynamic graph evolve in different frequency bandwidths. 3) Fourier transform is able to capture global and periodic patterns <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>, which are common in real-world dynamic graphs, e.g., the interactions on e-commerce networks may result from seasonal sales or product service cycles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Disentangled Spectrum Mask</head><p>To capture invariant patterns in the spectral domain, we propose to explicitly learn spectrum masks to disentangle the invariant and variant patterns. The embeddings in the spectral domain contain the The framework of our proposed method SILD. Given a dynamic graph evolving through time, the dynamic graph neural networks with spectral transform first obtain the ego-graph trajectory spectrums in the spectral domain. Then the disentangled spectrum mask leverages the amplitude and phase information of the ego-graph trajectory spectrums to obtain invariant and variant spectrum masks. Last, invariant spectral filtering discovers the invariant and variant patterns via the disentangled spectrum masks, and minimizes the variance of predictions with exposure to various variant patterns, to help the model exploit invariant patterns to make predictions under distribution shifts. amplitude information as well as the phase information for each node</p><formula xml:id="formula_10">Amp(Z) = Imag 2 (Z) + Real 2 (Z) 1 2 , ϕ(Z) = arctan Imag(Z) Real(Z) ,<label>(3)</label></formula><p>where Real(•) and Imag(•) denote the real and imaginary part of the complex number, i.e., Z = Real(Z) + jImag(Z), j denotes the imaginary unit, Amp(Z) ∈ R K×N ×d and ϕ(Z) ∈ R K×N ×d denote the amplitude and phase information respectively. For brevity, the tensor operators in Eq. ( <ref type="formula" target="#formula_10">3</ref>) are all element-wise, e.g., (Imag 2 (Z)) i,j,k = (Imag(Z i,j,k )) 2 . Then, we obtain the spectrum masks by leveraging both the amplitude and phase information</p><formula xml:id="formula_11">M = MLP(Real(Z)||Imag(Z)), M I = sigmoid(M/τ ), M V = sigmoid(-M/τ ),<label>(4)</label></formula><p>where MLP denotes multi-layer perceptrons, τ is the temporature,</p><formula xml:id="formula_12">M I ∈ [0, 1] K and M V ∈ [0, 1] K</formula><p>denote the spectrum mask for invariant and variant patterns, and || represents the concatenation of the embeddings. In this way, the invariant and variant masks have a negative relationship, and each node can have its own spectrum mask. As the phase information includes high-level semantics in the original signals <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>, we keep the phase information unchanged to reduce harm in the fine-grained semantic information for the graph trajectories, and filter the spectrums by the learned disentangled masks in terms of amplitudes,</p><formula xml:id="formula_13">ZI = MI ⊙ Amp(Z) ⊙ (cos ϕ(Z) + j sin ϕ(Z)), ZV = MV ⊙ Amp(Z) (cos ϕ(Z) + j sin ϕ(Z)),<label>(5)</label></formula><p>where Z I and Z V denote the summarized invariant and variant patterns in the spectral domain. For node classification tasks, we can directly adopt the spectrums for the classifier to predict classes. For link prediction tasks, we can utilize inverse fast Fourier transform (IFFT) to transform the embeddings into the temporal domain for future link prediction</p><formula xml:id="formula_14">H ′ I = Φ H Z I , H ′ V = Φ H Z V ,<label>(6)</label></formula><p>where (•) H is Hermitian transpose, H ′ I and H ′ V denote the filtered invariant and variant patterns that are transformed back into the temporal domain respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Invariant Spectral Filtering</head><p>Under distribution shifts, the variant patterns on dynamic graphs have varying relationships with labels, while the invariant patterns have sufficient predictive abilities with regard to labels. We propose invariant spectral filtering to capture the invariant and variant patterns in the spectral domain, and help the model focus on invariant patterns to make predictions, thus handling distribution shifts. We take node classification tasks for an example as follows.</p><p>Let Z I ∈ C K×N ×d and Z V ∈ C K×N ×d be the filtered invariant and variant spectrums in the spectral domain. Then we can utilize the invariant and variant node spectrums to calculate the task loss</p><formula xml:id="formula_15">L I = l(f I (Z I ), Y), L V = l(f V (Z V ), Y),<label>(7)</label></formula><p>where f I (•) and f V (•) are the classifiers for invariant and variant patterns respectively, Y is the labels, and l is the loss function. The task loss is utilized to capture the patterns with the predictive abilities of labels. Recall in Assumption 1, the influence of variant patterns on labels is shielded given invariant patterns as the invariant patterns have sufficient predictive abilities w.r.t labels, and thus the model's predictions should not change when being exposed to different variant patterns and the original invariant patterns. Inspired by <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b6">7]</ref>, we calculate the invariance loss by</p><formula xml:id="formula_16">L IN V = Var({L m | z : z ∈ S}),<label>(8)</label></formula><p>where L m | z denotes the mixed loss to measure the model's prediction ability with exposure to the specific variant pattern z ∈ C K×d that is sampled from a set of variant patterns S. We adopt all the node embeddings in Z V to construct the set of variant patterns S. Inspired by <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b14">15]</ref>, we calculate the mixed loss as</p><formula xml:id="formula_17">L m | z = l(f I (Z I ) ⊙ σ(f V (z)), Y),<label>(9)</label></formula><p>where σ denotes the sigmoid function. Then, the final training objective is</p><formula xml:id="formula_18">min θ,f I L I + λL IN V + min f V L V ,<label>(10)</label></formula><p>where θ is the parameters that encompass all the model parameters except the classifiers, λ is a hyperparameter to balance the trade-off between the model's predictive ability and invariance properties. Obtain the node embeddings H with snapshot-wise message passing as Eq. (</p><p>Transform the node embeddings into the spectral domain with FFT as Eq. ( <ref type="formula" target="#formula_8">2</ref>)</p><p>4:</p><p>Calculate the disentangled spectrum masks as Eq. ( <ref type="formula" target="#formula_11">4</ref>)</p><p>5:</p><p>Filter spectrums into invariant and variant patterns as Eq. ( <ref type="formula" target="#formula_13">5</ref>)</p><p>6:</p><p>Calculate the task loss as Eq. ( <ref type="formula" target="#formula_15">7</ref>)</p><p>7:</p><p>Sample S variant patterns from collections of Z V and calculate the invariance loss as Eq. ( <ref type="formula" target="#formula_16">8</ref>)</p><p>8:</p><p>Update the model according to Eq. (10) 9: end for <ref type="bibr" target="#b4">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we conduct extensive experiments to verify that our proposed method can handle distribution shifts on dynamic graphs by discovering and utilizing invariant patterns in the spectral domain. More details of the settings and other results can be found in Appendix.</p><p>Baselines. We adopt several representative dynamic GNNs and Out-of-Distribution(OOD) generalization methods as our baselines: • Dynamic GNNs: GCRN <ref type="bibr" target="#b35">[36]</ref> is a representative dynamic GNN that first adopts a GCN <ref type="bibr" target="#b36">[37]</ref> to obtain node embeddings and then a GRU <ref type="bibr" target="#b37">[38]</ref> to model the network evolution. EGCN <ref type="bibr" target="#b25">[26]</ref> adopts an LSTM <ref type="bibr" target="#b38">[39]</ref> or GRU <ref type="bibr" target="#b37">[38]</ref> to flexibly evolve the GCN <ref type="bibr" target="#b36">[37]</ref> parameters through time. DySAT <ref type="bibr" target="#b24">[25]</ref> aggregates neighborhood information at each graph snapshot using structural attention and models network dynamics with temporal self-attention. • OOD generalization methods: IRM <ref type="bibr" target="#b20">[21]</ref> aims at learning an invariant predictor which minimizes the empirical risks for all training domains. GroupDRO <ref type="bibr" target="#b39">[40]</ref> puts more weight on training domains with larger errors to minimize the worst-group risks across training domains. V-REx <ref type="bibr" target="#b40">[41]</ref> reduces the differences in the risks across training domains to reduce the model's sensitivity to distributional shifts. As these methods are not specifically designed for dynamic graphs, we adopt the best-performed dynamic GNNs as their backbones on each dataset. • OOD generalization methods for dynamic graphs: DIDA <ref type="bibr" target="#b6">[7]</ref> utilizes disentangled attention to capture invariant and variant patterns in the spatial-temporal domain, and conducts spatialtemporal intervention mechanism to let the model focus on invariant patterns to make predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Real-world Datasets</head><p>Settings We use 3 real-world dynamic graph datasets, including Collab <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b6">7]</ref>, Yelp <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b6">7]</ref> and Aminer <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>. Following <ref type="bibr" target="#b6">[7]</ref>, we adopt the challenging inductive future link prediction task on Collab and Yelp, where the model should exploit historical graphs to predict the occurrence of links in the next time step. To measure the model's performance under distribution shifts, the model is tested on another dynamic graph with different fields, which is unseen during training. For node classification, we adopt Aminer, a citation network, where nodes represent papers, and edges from u to v with timestamp t denote the paper u published at year t cites the paper v. The task is to predict the venues of the papers. We train on papers published between 2001 -2011, validate on those published in 2012 -2014, and test on those published since 2015. On this dataset, the model is tested to exploit the invariant patterns and make stable predictions under distribution shifts, where the patterns on the dynamic graph may vary in different years.</p><p>Results Based on the results in Table <ref type="table" target="#tab_0">1</ref>, we have the following observations: 1) Under distribution shifts, the general OOD generalization baselines have limited improvements over the dynamic GNNs, e.g., GroupDRO improves over DySAT with 0.9% in Yelp and 0.3% in Aminer15 respectively. A plausible reason is that they are not specially designed to handle distribution shifts on dynamic graphs, and may not consider the graph structural and temporal dynamics to capture invariant patterns. Another reason might be that they strongly rely on high-quality environment labels to capture invariant patterns, which are almost unavailable on real-world dynamic graphs. 2) Our method can better handle distribution shifts than the baselines. The datasets have strong distribution shifts, e.g., COVID-19 happens midway and has considerable influence on the consumer behavior on Yelp, and the citation patterns may shift with the outbreak of deep neural networks on Aminer. Nevertheless, our method SILD has significant improvements over the state-of-the-art OOD generalization baseline for dynamic graphs DIDA on all datasets, e.g., 2% on average for most datasets, which verifies that our method can better capture the invariant and variant patterns in the spectral domain, and thus handling distribution shifts on dynamic graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Synthetic Datasets</head><p>Settings To evaluate the model's generalization ability under distribution shifts, we conduct experiments on synthetic link prediction and node classification datasets, which are constructed by introducing manually-designed distribution shifts. For link prediction datasets, we follow <ref type="bibr" target="#b6">[7]</ref> to generate additional varying features for each node and timestamps on the original dataset Collab, where these additional features are constructed with spurious correlations w.r.t the labels, i.e., the links in the next timestamps. The spurious correlation degree is determined by a shift level parameter. On this dataset, to have better generalization ability, the model should not rely on variant patterns that exploit the additional features with spurious correlations. For node classification, we briefly introduce the construction of the synthetic dataset as follows. We generate the dynamic graph with stochastic block model <ref type="bibr" target="#b44">[45]</ref>, where the link probability between nodes at each graph snapshot is determined by two frequency factors. The correlation of one of the factors with class labels is always 1, while the other factor has a variant relationship with labels, where the relationship is also controlled by a shift level parameter. The model should discover and focus on the invariant frequency factors whose relationship with labels is invariant under distribution shifts. For both datasets, we set the shift level parameters as 0.4, 0.6, 0.8 for training and validation splits, and 0 for test splits.</p><p>Results Based on the results in Table <ref type="table" target="#tab_1">2</ref>, we have the following observations: 1) Our method can better handle distribution shifts than the baselines, especially under stronger distribution shifts. SILD consistently outperforms DyGNN and general OOD generalization baselines by a significantly large margin, which can credit to our special design to handle distribution shifts on dynamic graphs in the spectral domain. Our method also has a significant improvement over the best-performed baseline under the strongest distribution shift, e.g., with absolute improvements of 5% in Link-Synthetic(0.8) and 7% in Node-Synthetic(0.8) respectively. 2) Our method can exploit invariant patterns to consistently alleviate the harmful effects of variant patterns under different distribution shift levels. As the distribution shift level increases, almost all methods decline in performance since the relationship between variant patterns and labels goes stronger, so that the variant patterns are much easier to be exploited by the model, misleading the training process. However, the performance drop of SILD is significantly lower than baselines, which demonstrates that our method can alleviate the harmful effects of variant patterns under distribution shifts by exploiting invariant patterns in the spectral domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Studies</head><p>We conduct ablation studies to verify the effectiveness of the proposed disentangled spectrum mask and invariant spectral filtering in SILD. The ablated version 'SILD w/o I' removes invariant spectral filtering in SILD by setting λ = 0, and 'SILD w/o M' is trained without the disentangled spectrum masks. From Figure <ref type="figure" target="#fig_3">3</ref>, we have the following observations. First, our proposed SILD outperforms all the variants as well as the best-performed baseline on all datasets, demonstrating the effectiveness of each component of our proposed method. Second, 'SILD w/o I' and 'SILD w/o M' drop drastically in performance on all datasets compared to the full version, which verifies that our proposed disentangled spectrum mask and spectral invariant learning can help the model to focus on invariant patterns to make predictions and significantly improve the performance under distribution shifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Works</head><p>Dynamic Graph Neural Networks Dynamic graphs ubiquitously exist in real-world applications <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref> such as event forecasting, recommendation, etc. In comparison with static graphs <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62]</ref>, dynamic graphs contain rich temporal information. Considerable research attention has been devoted to dynamic graph neural networks (DyGNNs) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b62">63]</ref> to model the complex graph dynamics that include structures and features evolving through time. Some works adopt GNN to aggregate neighborhood information for each graph snapshot, and then utilize a sequence module to model the temporal information <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b24">25]</ref>. Some others utilize time-encoding techniques to encode the temporal links into time-aware embeddings and adopt a GNN or memory module <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b69">70]</ref> to process structural information. Some other related works leverage spectral graph neural networks <ref type="bibr" target="#b70">[71]</ref>, global graph framelet convolution <ref type="bibr" target="#b71">[72]</ref>, and graph wavelets <ref type="bibr" target="#b72">[73]</ref> to obtain better dynamic graph representations. However, distribution shifts remain largely unexplored in dynamic graph neural networks literature. The sole prior work DIDA <ref type="bibr" target="#b6">[7]</ref> handles spatial-temporal distribution shifts on dynamic graphs in the temporal domain. To the best of our knowledge, this is the first study of handling distribution shifts on dynamic graphs in the spectral domain.</p><p>Out-of-Distribution Generalization A significant proportion of existing machine learning methodologies operate on the assumption that training and testing data are independent and identically distributed (i.i.d.). However, this assumption may not always hold true, especially in the context of complex real-world scenarios <ref type="bibr" target="#b73">[74]</ref>, and the uncontrollable distribution shifts between training and testing data distribution may lead to a significant decline in the model performance. Out-of-Distribution (OOD) generalization problem has recently drawn great attention in various areas <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b75">76]</ref>. Some works handle structural distribution shifts on static graphs <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b83">84]</ref> and temporal distribution shifts on time-series data <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b84">85]</ref>. However, how to handle distribution shifts on dynamic graphs in the spectral domain remains unexplored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spectral Methods in Neural Networks</head><p>The applications of spectral methods in neural networks have been broadly explored in many areas, including static graph data <ref type="bibr" target="#b85">[86,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b89">90]</ref>, timeseries data <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b90">91,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr" target="#b92">93,</ref><ref type="bibr" target="#b93">94]</ref>, etc., for their advantages of modeling global patterns, powerful expressiveness and interpretability <ref type="bibr" target="#b94">[95,</ref><ref type="bibr" target="#b28">29]</ref>. Some work <ref type="bibr" target="#b95">[96]</ref> proposes to reconstruct the image in the spectral domain to obtain robust image representations. Some work <ref type="bibr" target="#b29">[30]</ref> proposes to augment the image data by perturbing the amplitude information in the spectral domain. Some work <ref type="bibr" target="#b96">[97]</ref> proposes a multiwavelet-based method for compressing operator kernels. However, these methods are not applicable to dynamic graphs, not to mention the more complex scenarios under distribution shifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose a novel model named Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts (SILD), which can handle distribution shifts on dynamic graphs in the spectral domain. We design a DyGNN with Fourier transform to obtain the ego-graph trajectory spectrums.</p><p>Then we propose a disentangled spectrum mask and invariant spectral filtering to discover the invariant and variant patterns in the spectral domain, and help the model rely on invariant spectral patterns to make predictions. Extensive experimental results on several synthetic and real-world datasets, including both node classification and link prediction tasks, demonstrate the superior performance of our method compared to state-of-the-art baselines under distribution shifts. One limitation is that in this paper we mainly focus on dynamic graphs in scenarios of discrete snapshots, and we leave extending our methods to continuous dynamic graphs for further explorations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Notations</head><p>Table <ref type="table">3</ref>: The summary of the notations and their descriptions Notations Descriptions G = (V, E) A graph with the node set and edge set</p><formula xml:id="formula_20">G t = (V t , E t ) Graph slice at time t X t , A t</formula><p>Features and adjacency matrix of a graph at time t G 1:t , Y t , G 1:t , Y t Graph trajectory, label and their corresponding random variable</p><formula xml:id="formula_21">G 1:t v , y t v , G 1:t v , y t v</formula><p>Ego-graph trajectory, the node's label and their corresponding random variable p(•)</p><p>Probability distribution P, P Pattern and its corresponding random variable dv,1, dv,2</p><p>The degrees of node v varying by time g, w</p><p>The parameters of linear classifiers m, M</p><p>The mask to filter node representations ŷv</p><p>The prediction for the node v Rtr(w)</p><p>The risks of the classifier w in training data Φ</p><p>The Fourier bases x</p><p>The conjugate of x MSG(•), AGG(•)</p><p>Message and Aggregation functions h t u Hidden embeddings for node u at time t H, Z Node representations in the temporal domain and spectral domain d</p><p>The dimensionality of node representations Amp(Z), ϕ(Z)</p><p>The amplitudes and phases of the representations Z x H</p><p>The Hermitian transpose of x T, K</p><p>The number of time stamps and the number of frequency components f Proof. For any mask m ∈ R T ×1 , the predictions of the model is</p><formula xml:id="formula_22">ŷv = w ⊤ (m ⊙ (d v,1 + d v,2 )).<label>(11)</label></formula><p>We assume that ||m ⊙ w|| ̸ = 0, otherwise the classifier is a trivial solution and always predicts ŷv = 0 for any node v. The empirical risk in training data is</p><formula xml:id="formula_23">R tr (w) = 1 |D tr | v∈Dtr (ŷ v -y v ) 2 .<label>(12)</label></formula><p>By setting ∂Rtr(w) ∂w = 0, we have the optimal classifier learned from the training data</p><formula xml:id="formula_24">w * = m ⊙ v∈Dtr (d v,1 + d v,2 )g ⊤ d v,1 v∈Dtr (m ⊙ (d v,1 + d v,2 ))⊤(m ⊙ (d v,1 + d v,2 )) .<label>(13)</label></formula><p>Then for a node v that has variant patterns d v,2 = αm ⊙ w * and α ∈ R, the loss of the model's prediction is</p><formula xml:id="formula_25">l v = (ŷ v -y v ) 2 = w * ⊤ (m ⊙ (d v,1 + d v,2 )) -g ⊤ d v,1 2 = α||m ⊙ w * || 2 + (w * ⊤ m ⊙ d v,1 -g ⊤ d v,1 ) 2 = ||d v,2 ||||m ⊙ w * || + (w * ⊤ m ⊙ d v,1 -g ⊤ d v,1 ) 2 . (<label>14</label></formula><formula xml:id="formula_26">)</formula><p>Then ∀ϵ &gt; 0,</p><formula xml:id="formula_27">when ||d v,2 || &gt; √ ϵ-(w * ⊤ m⊙dv,1-g ⊤ dv,1) ||m⊙w * || , i.e., α &gt; √ ϵ-(w * ⊤ m⊙dv,1-g ⊤ dv,1) ||m⊙w * || 2 , (ŷ v - y v ) 2 &gt; ϵ, indicating that lim ||dv,2||→∞ (ŷ v -y v ) 2 = ∞.</formula><p>Thus we conclude the proof.  </p><formula xml:id="formula_28">m i = 0 if ∃v, Φd v,2 ⊙ Φd v,2 i ̸ = 0 1 otherwise . (<label>15</label></formula><formula xml:id="formula_29">)</formula><p>Since the frequency bandwidths of invariant and variants patterns do not have overlap, i.e., Φd v,1 ⊙</p><formula xml:id="formula_30">Φd v,1 ⊙ Φd v,2 ⊙ Φd v,2 = 0, ∀d v,1 , d v,2 , we have m i ⊙ z v,1 = z v,1 and m i ⊙ z v,2</formula><p>for any node v. Let w 1 = Φg, then the prediction for any node v is</p><formula xml:id="formula_31">ŷv = (Φg) H (m ⊙ (z v,1 + z v,2 )) = g ⊤ Φ H (z v,1 ) = g ⊤ Φ H (Φd v,1 ) = g ⊤ d v,1 .<label>(16)</label></formula><p>For any node v, we have (ŷ v -y v ) 2 = 0, so that w 1 = arg min w R tr (w), and ∀v, lim ||dv,2||→∞ (ŷ vy v ) 2 &lt; 1. Thus we conclude the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Experiments and Analyses</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Hyperparameter Sensitivity</head><p>We analyze the sensitivity of hyperparameter λ in SILD for each dataset by altering the hyperparameter on a base ten logarithmic scale. As shown in Figure <ref type="figure" target="#fig_6">4</ref>, when the hyperparameter λ is too small or too large, the performance of the model deteriorates in most datasets, which verifies that the hyperparameter λ is the tradeoff between the sufficiency and invariance conditions of the patterns captured by the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Complexity Analysis</head><p>We analyze the computational complexity of SILD as follows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Reproducibility Details D.1 Training &amp; Evaluation</head><p>Hyperparameters Following <ref type="bibr" target="#b6">[7]</ref>, for all methods, we adopt the hidden dimension as 32 for Aminer and 16 for other datasets. The number of layers is set to 2, and the models are optimized with the Adam optimizer <ref type="bibr" target="#b97">[98]</ref> with a learning rate 1e-2 and weight decay 5e-7. The early stopping strategy on the validation splits is adopted, with 100 epochs for Node-Synthetic datasets and 50 epochs for other datasets. For SILD, we set the sampling number of variant patterns as 1000 for Collab and Yelp, and 100 for other datasets, and λ as 1e-4,1e-3,1e-2,1e-2,1e-2 for Collab, Aminer, Yelp, Link-Synthetic, and Node-Synthetic datasets respectively.</p><p>Evaluation For link prediction tasks, we randomly sample negative links from the nodes that actually do not have links in-between, and the number of negative links is the same as the number of positive links. All the negative and positive samples for validation and testing set are kept the same for all methods. We use the inner product of the two node representations to predict links, use crossentropy as the loss function ℓ, and use Area under the ROC Curve (AUC) as the evaluation metric.</p><p>For node classification tasks, we use a two-layer MLP for the node classifier, use cross-entropy as the loss function ℓ, and use Accuracy (ACC) as the evaluation metric. We randomly run the experiments three times, and report the average results and standard deviations.</p><p>Details of SILD For the node classification dataset Aminer, we conduct the missing graph trajectory complementation as follows. In practice, dynamic graphs usually encounter with the issues of incomplete trajectories, i.e., the nodes have missing historical trajectories for some reasons. For example, on academic citation networks, the papers on dynamic graphs are always different each year and they only have structures (cite other papers) at the published year, which means that they only have a one-year trajectory. In these cases, the modeling of dynamics would be difficult and also inaccurate. To complement the missing historical graph trajectories, we utilize the current structure as the virtual past structure to help model the neighborhood evolution for the node to predict at t ′ , and the message passing is</p><formula xml:id="formula_32">m t u→v ← MSG(h t u , h t v ), h t v ← AGG({m t u→v | u ∈ N t (v) N t ′ (v)}, h t v }).<label>(17)</label></formula><p>In this way, the node embedding h t u for node u which appears at time t ≤ t ′ denotes the neighborhood information it may aggregate if it appears at time t. Note that in Eq. ( <ref type="formula" target="#formula_32">17</ref>), the target is to predict the node labels at time t ′ , where the current neighborhood N t ′ (v) is known to all methods and this method does not exploit extra future information. For the message and aggregation functions, we adopt DIDA <ref type="bibr" target="#b6">[7]</ref> for Yelp dataset and GAT <ref type="bibr" target="#b98">[99]</ref> for other datasets. We adopt two-layer MLPs for both the invariant and variant node classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Dataset Details</head><p>We summarize the dataset statistics in Table <ref type="table" target="#tab_3">4</ref> and describe the dataset details as follows.</p><p>Collab <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b6">7]</ref> <ref type="foot" target="#foot_1">foot_1</ref> is an academic collaboration dataset with papers that were published during 1990-2006, where the nodes and edges represent author and coauthorship respectively. The author features are obtained by averaging the embeddings of the author-related papers, which are extracted by word2vec <ref type="bibr" target="#b99">[100]</ref> from the paper abstracts. The distribution shift comes from different fields, including "Data Mining", "Database", "Medical Informatics", "Theory" and "Visualization". We use 10,1,5 chronological graph slices for training, validation and testing respectively.</p><p>Yelp <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b6">7]</ref> <ref type="foot" target="#foot_2">foot_2</ref> is a business review dataset, where the nodes and edges represent customers or businesses and review behaviors respectively. We utilize the data from January 2019 to December 2020, and select users and reviews with interactions of more than 10. We use word2vec <ref type="bibr" target="#b99">[100]</ref> to extract 32dimensional features from the reviews and average to obtain the user and business features. The distribution shift comes from the out-break of COVID-19 midway as well as the different business categories including "Pizza", "American (New) Food", "Coffee &amp; Tea ", "Sushi Bars" and "Fast Food". We use 15,1,8 chronological graph slices for training, validation and testing respectively.</p><p>Aminer <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b43">44]</ref> is a citation network extracted from DBLP, ACM, MAG, and other sources. We select the top 20 venues, and the task is to predict the venues of the papers. We use word2vec <ref type="bibr" target="#b99">[100]</ref> to extract 128-dimensional features from paper abstracts and average to obtain paper features. The distribution shift may come from the out-break of deep learning. We train on papers published between 2001 -2011, validate on those published in 2012-2014, and test on those published since 2015.</p><p>Link-Synthetic <ref type="bibr" target="#b6">[7]</ref> introduces manual-designed distribution shift on Collab dataset. Denote the original features and structures in Collab as X t 1 and structures as A t . We introduce features X t 2 with a variable correlation with the labels, which are obtained by training the embeddings X t 2 ∈ R N ×d with the reconstruction loss ℓ(X t 2 X t 2 ⊤ , Ãt+1 ), where Ãt+1 refers to the sampled links, and ℓ refers to the cross-entropy loss function. In this way, the generated features can have strong correlations with the sampled links. For each time t, we uniformly sample p(t)|E t+1 | positive links and (1 -p(t))|E t+1 | negative links in A t+1 and the sampling probability p(t) = clip(p + σcos(t), 0, 1) refers to the intensity of shifts. By controlling the parameter p, we can control the correlations of X t and labels A t+1 to vary in training and test stage. Since the model observes the X t = [X t 1 ||X t 2 ] simultaneously and the variant features are not marked, the model should discover and get rid of the variant features to handle distribution shifts. Similar to Collab dataset, we use 10,1,5 chronological graph slices for training, validation and test respectively.</p><p>Node-Synthetic introduces manually designed distribution shifts for node classification tasks, by simulating that some frequency components on dynamic graphs have invariant correlations with labels while some others do not. We adopt a stochastic block model (SBM) <ref type="bibr" target="#b44">[45]</ref> to generate links between nodes. For brevity, we denote the SBM model as SBM(p in , p out ), where p in ∈ [0, 1] C×1 and p out denotes the link probability between the nodes belonging to the same class and the link probability between the nodes from different classes respectively. We adopt C = 5 classes. Based on the class label, each node has two types of parameters f low ∈ {0.02, 0.04, 0.08, 0.10, 0.12} and f high ∈ {0.22, 0.24, 0.28, 0.30, 0.32}. The correlation of f low with labels is set to 0.4, 0.6, 0.8 respectively for training and validation and 0 for testing, and the correlation of f high with labels is set to 1 for all data splits. The dynamic graph G t at time t is constructed by mixing multiple graphs together, including a random graph G t r generated from Gaussian noises, a graph constructed by the invariant parameter G t I = SBM(p high in (t), p out ) and a graph constructed by the variant parameter G t I = SBM(p low in (t), p out ). The relationship between the parameters and the link probability is p low in (t, f ) = S 1 (2 + cos(2πf t)) and p high in (t, f ) = S 2 (2 + cos(2πf t)). We set 1e-3, 1e-2, 5e-3 for p out , S 1 and S 2 respectively. We generate 4-dimensional random features for each node. On this dataset, to have better generalization ability, the model should discover and focus on the dynamic graph constructed with the invariant parameter to make predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Configurations</head><p>All the experiments are conducted with:</p><p>• Operating System: Ubuntu 20.04.5 LTS • CPU: Intel(R) Xeon(R) Gold 6240 CPU @ 2.60GHz • GPU: NVIDIA GeForce RTX 3090 with 24 GB of memory • Software: Python 3.9.12, Cuda 11.3, PyTorch <ref type="bibr" target="#b101">[102]</ref> 1.12.1, PyTorch Geometric <ref type="bibr" target="#b102">[103]</ref> 2.0.4.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: An illustration example: the graph dynamics from different frequency components are entangled in the temporal domain, while it is much easier to distinguish different frequency components by masking the spectrums in the spectral domain. In this case, the frequency components in the invariant spectrums determine the node labels, while the relationship between the variant spectrums and labels is not stable under distribution shifts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure2: The framework of our proposed method SILD. Given a dynamic graph evolving through time, the dynamic graph neural networks with spectral transform first obtain the ego-graph trajectory spectrums in the spectral domain. Then the disentangled spectrum mask leverages the amplitude and phase information of the ego-graph trajectory spectrums to obtain invariant and variant spectrum masks. Last, invariant spectral filtering discovers the invariant and variant patterns via the disentangled spectrum masks, and minimizes the variance of predictions with exposure to various variant patterns, to help the model exploit invariant patterns to make predictions under distribution shifts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>A larger λ encourages the model to capture patterns with better invariance under distribution shifts, with the potential risk of lower predictive ability during training, as the shortcuts brought by the variant patterns might be discarded in the training process. After training, we only adopt invariant patterns to make predictions in the inference stage. The overall algorithm for training on node classification datasets is summarized in Algo. 1. Training pipeline for SILD on node classification datasets Require: Training epochs L, sample number S, hyperparameter λ 1: for l = 1, . . . , L do 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results of ablation studies, where 'w/o I' removes invariant spectral filtering in SILD , 'w/o M' removes disentangled spectrum masks, and 'Best baseline' denotes the best-performed baseline on each dataset. The error bars report the standard deviations. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Sensitivity of hyperparameter λ. The area shows the average AUC and standard deviations in the test stage. The dashed line represents the average AUC of the best performed baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results of different methods on real-world link prediction and node classification datasets. The best results are in bold and the second-best results are underlined. The year in the Aminer dataset denotes the test split, e.g., 'Aminer15' denotes the average test accuracy in 2015.</figDesc><table><row><cell>Task</cell><cell cols="2">Link Prediction (AUC%)</cell><cell>Node Classification (ACC%)</cell></row><row><cell>Dataset</cell><cell>Collab</cell><cell>Yelp</cell><cell>Aminer15 Aminer16 Aminer17</cell></row><row><cell>GCRN</cell><cell>69.72±0.45</cell><cell>54.68±7.59</cell><cell>47.96±1.12 51.33±0.62 42.93±0.71</cell></row><row><cell>EGCN</cell><cell>76.15±0.91</cell><cell>53.82±2.06</cell><cell>44.14±1.12 46.28±1.84 37.71±1.84</cell></row><row><cell>DySAT</cell><cell>76.59±0.20</cell><cell>66.09±1.42</cell><cell>48.41±0.81 49.76±0.96 42.39±0.62</cell></row><row><cell>IRM</cell><cell>75.42±0.87</cell><cell>56.02±16.08</cell><cell>48.44±0.13 50.18±0.73 42.40±0.27</cell></row><row><cell>VREx</cell><cell>76.24±0.77</cell><cell>66.41±1.87</cell><cell>48.70±0.73 49.24±0.27 42.59±0.37</cell></row><row><cell cols="2">GroupDRO 76.33±0.29</cell><cell>66.97±0.61</cell><cell>48.73±0.61 49.74±0.26 42.80±0.36</cell></row><row><cell>DIDA</cell><cell>81.87±0.40</cell><cell>75.92±0.90</cell><cell>50.34±0.81 51.43±0.27 44.69±0.06</cell></row><row><cell>SILD</cell><cell>84.09±0.16</cell><cell>78.65±2.22</cell><cell>52.35±1.04 54.11±0.62 45.54±1.19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results of different methods on synthetic link prediction and node classification datasets. The best results are in bold and the second-best results are underlined. A larger 'shift' denotes a higher distribution shift level. GCRN 72.57±0.72 72.29±0.47 67.26±0.22 27.19±2.18 25.95±0.80 29.26±0.69 EGCN 69.00±0.53 62.70±1.14 60.13±0.89 24.01±2.29 22.75±0.96 24.98±1.32 DySAT 70.24±1.26 64.01±0.19 62.19±0.39 40.95±2.89 37.94±1.01 30.90±1.97 IRM 69.40±0.09 63.97±0.37 62.66±0.33 33.23±4.70 30.29±1.71 29.43±1.38 VREx 70.44±1.08 63.99±0.21 62.21±0.40 41.78±1.30 38.11±2.81 29.56±0.44 GroupDRO 70.30±1.23 64.05±0.21 62.13±0.35 41.35±2.19 35.74±3.93 31.03±1.24 DIDA 85.20±0.84 82.89±0.23 72.59±3.31 43.33±7.74 39.48±7.93 28.14±3.07</figDesc><table><row><cell>Dataset</cell><cell cols="3">Link-Synthetic (AUC%)</cell><cell cols="3">Node-Synthetic (ACC%)</cell></row><row><cell>Shift</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell></row><row><cell>SILD</cell><cell cols="6">85.95±0.18 84.69±1.18 78.01±0.71 43.62±2.74 39.78±3.56 38.64±2.76</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Denote the total number of nodes and edges in the graph as |V| and |E|, and the dimensionality of the hidden representation as d. The snapshot-wise message passing has a time complexity of O(|E|d + |V|d 2 ). The fast Fourier transform has a time complexity of O(|V|d log T ). The disentangled spectrum mask has a time complexity of O(|V|d). Denote |N p | as the number of nodes or edges to predict and S as the sampling number of variant patterns. Our invariant spectral filtering has a time complexity of O(|N p |Sd) in training, and does not put extra time complexity in inference. Therefore, the overall time complexity of SILD is O(|E|d + |V|d 2 + |V|d + |V|d log T + |N p |Sd). In summary, the time complexity of SILD has a linear time complexity with respect to the number of nodes and edges, which is on par with the existing dynamic GNNs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The summary of the dataset statistics.</figDesc><table><row><cell>Dataset</cell><cell cols="2"># Snapshots # Nodes</cell><cell># Links</cell><cell cols="3">Time Granularity # Features Evolving Features</cell></row><row><cell>Collab</cell><cell>16</cell><cell>23,035</cell><cell>151,790</cell><cell>Year</cell><cell>32</cell><cell>No</cell></row><row><cell>Yelp</cell><cell>24</cell><cell>13,095</cell><cell>65,375</cell><cell>Month</cell><cell>32</cell><cell>No</cell></row><row><cell>Link-Synthetic</cell><cell>16</cell><cell>23,035</cell><cell>151,790</cell><cell>-</cell><cell>64</cell><cell>Yes</cell></row><row><cell>Aminer</cell><cell>17</cell><cell>43,141</cell><cell>851,527</cell><cell>Year</cell><cell>128</cell><cell>No</cell></row><row><cell>Node-Synthetic</cell><cell>100</cell><cell>5,000</cell><cell>11,252,385</cell><cell>-</cell><cell>4</cell><cell>No</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>The codes are available at https://github.com/wondergo2017/sild.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>https://www.aminer.cn/collaboration.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>https://www.yelp.com/dataset</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by the <rs type="funder">National Key Research and Development Program of China</rs> No. <rs type="grantNumber">2020AAA0106300</rs>, <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62222209</rs>, <rs type="grantNumber">62250008</rs>, <rs type="grantNumber">62102222</rs>, <rs type="grantNumber">62206149</rs>), <rs type="funder">Beijing National Research Center for Information Science and Technology</rs> under Grant No. <rs type="grantNumber">BNR2023RC01003</rs>, <rs type="grantNumber">BNR2023TD03006</rs>, <rs type="funder">China National Postdoctoral Program for Innovative Talents</rs> No. <rs type="grantNumber">BX20220185</rs>, <rs type="funder">China Postdoctoral Science Foundation</rs> No. <rs type="grantNumber">2022M711813</rs>, and <rs type="funder">Beijing Key Lab of Networked Multimedia</rs>. All opinions, findings, conclusions, and recommendations in this paper are those of the authors and do not necessarily reflect the views of the funding agencies.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_9ARV7uH">
					<idno type="grant-number">2020AAA0106300</idno>
				</org>
				<org type="funding" xml:id="_AvwGhvd">
					<idno type="grant-number">62222209</idno>
				</org>
				<org type="funding" xml:id="_Z6cXBjJ">
					<idno type="grant-number">62250008</idno>
				</org>
				<org type="funding" xml:id="_f6K9Zyt">
					<idno type="grant-number">62102222</idno>
				</org>
				<org type="funding" xml:id="_2bdVhuq">
					<idno type="grant-number">62206149</idno>
				</org>
				<org type="funding" xml:id="_x5ZTRXE">
					<idno type="grant-number">BNR2023RC01003</idno>
				</org>
				<org type="funding" xml:id="_KKGyXWz">
					<idno type="grant-number">BNR2023TD03006</idno>
				</org>
				<org type="funding" xml:id="_vWej6fr">
					<idno type="grant-number">BX20220185</idno>
				</org>
				<org type="funding" xml:id="_ZrnR5vA">
					<idno type="grant-number">2022M711813</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Foundations and modeling of dynamic networks using dynamic graph neural networks: A survey</title>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Skarding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Gabrys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarzyna</forename><surname>Musial</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="79143" to="79168" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learnable encoder-decoder architecture for dynamic graph: A survey</title>
		<author>
			<persName><forename type="first">Yuecai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.10480</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Survivorship bias in performance studies</title>
		<author>
			<persName><forename type="first">William</forename><surname>Stephen J Brown</surname></persName>
		</author>
		<author>
			<persName><surname>Goetzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Roger G Ibbotson</surname></persName>
		</author>
		<author>
			<persName><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Review of Financial Studies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="553" to="580" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An introduction to sample selection bias in sociological data</title>
		<author>
			<persName><forename type="first">Berk</forename><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American sociological review</title>
		<imprint>
			<biblScope unit="page" from="386" to="398" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Shift-robust gnns: Overcoming the limitations of localized graph training data</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Ponomareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reversible instance normalization for accurate time-series forecasting against distribution shift</title>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunwon</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheonbok</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jang-Ho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynamic graph neural networks under spatio-temporal distribution shift</title>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Managing large dynamic graphs efficiently</title>
		<author>
			<persName><forename type="first">Jayanta</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Deshpande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2012 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="145" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Timearcs: Visualizing fluctuations in dynamic networks</title>
		<author>
			<persName><forename type="first">Nick</forename><surname>Tuan Nhon Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angus</forename><forename type="middle">Graeme</forename><surname>Pendar</surname></persName>
		</author>
		<author>
			<persName><surname>Forbes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="61" to="69" />
			<date type="published" when="2016">2016</date>
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Dynamic networks in large financial and economic systems</title>
		<author>
			<persName><forename type="first">Jozef</forename><surname>Barunik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ellington</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07842</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Woods: Benchmarks for out-of-distribution generalization in time series tasks</title>
		<author>
			<persName><forename type="first">Jean-Christophe</forename><surname>Gagnon-Audet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad-Javad</forename><surname>Darvishi-Bayazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Rish</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.09978</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adarnn: Adaptive learning and forecasting of time series</title>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinno</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renjun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongjun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="402" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Environment agnostic invariant risk minimization for classification of sequential datasets</title>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Venkateswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Muthusamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vatche</forename><surname>Isahagian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nalini</forename><surname>Venkatasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1615" to="1624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Diversify to generalize: Learning generalized representations for time series classification</title>
		<author>
			<persName><forename type="first">Wang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinwei</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.07027</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Discovering invariant rationales for graph neural networks</title>
		<author>
			<persName><forename type="first">Ying-Xin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12872</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Handling distribution shifts on graphs: An invariance perspective</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.02466</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A closer look at distribution shifts and out-of-distribution generalization on graphs</title>
		<author>
			<persName><forename type="first">Mucong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kezhi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuhai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Kirchenbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micah</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Community detection and co-author recommendation in co-author networks</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianjun</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Machine Learning and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="597" to="609" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Causal representation learning for out-of-distribution recommendation</title>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022</title>
		<meeting>the ACM Web Conference 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3562" to="3571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Out-of-distribution generalized dynamic graph neural network with disentangled intervention and invariance promotion</title>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.14255</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Invariant risk minimization</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Invariant rationalization</title>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1448" to="1458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Invariant risk minimization games</title>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kush</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Dhurandhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="145" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Roland: graph learning framework for dynamic graphs</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2358" to="2366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dysat: Deep neural representation learning on dynamic graphs via self-attention networks</title>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanhong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Web Search and Data Mining</title>
		<meeting>the 13th International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="519" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Evolvegcn: Evolving graph convolutional networks for dynamic graphs</title>
		<author>
			<persName><forename type="first">Aldo</forename><surname>Pareja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giacomo</forename><surname>Domeniconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toyotaro</forename><surname>Suzumura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroki</forename><surname>Kanezashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Kaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Schardl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Leiserson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5363" to="5370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An algorithm for the machine calculation of complex fourier series</title>
		<author>
			<persName><forename type="first">W</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">W</forename><surname>Cooley</surname></persName>
		</author>
		<author>
			<persName><surname>Tukey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">90</biblScope>
			<biblScope unit="page" from="297" to="301" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Newbold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bracewell</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">N</forename><surname>Bracewell</surname></persName>
		</author>
		<title level="m">The Fourier transform and its applications</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">31999</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Neural time series analysis with fourier transform: A survey</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoujin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Niu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.02173</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A fourier-based framework for domain generalization</title>
		<author>
			<persName><forename type="first">Qinwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14383" to="14392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Structural sparseness and spatial phase alignment in natural scenes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">F</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><surname>Hess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOSA A</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1873" to="1885" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The importance of phase in signals</title>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">V</forename><surname>Oppenheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae</forename><forename type="middle">S</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="529" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A demonstration of the visual importance and flexibility of spatial-frequency amplitude and phase</title>
		<author>
			<persName><forename type="first">N</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fergus</forename><forename type="middle">W</forename><surname>Piotrowski</surname></persName>
		</author>
		<author>
			<persName><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="337" to="346" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Phase in speech and pictures</title>
		<author>
			<persName><forename type="first">Jae</forename><surname>Oppenheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><surname>Kopec</surname></persName>
		</author>
		<author>
			<persName><surname>Pohlig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP&apos;79. IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1979">1979</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="632" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reducing unimodal biases for visual question answering</title>
		<author>
			<persName><forename type="first">Remi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Dancette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Structured sequence modeling with graph convolutional recurrent networks</title>
		<author>
			<persName><forename type="first">Youngjoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="362" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<title level="m">Variational graph auto-encoders</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Çaglar</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization</title>
		<author>
			<persName><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08731</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Out-of-distribution generalization via risk extrapolation (rex)</title>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joern-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Binas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Le Priol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5815" to="5826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cross-domain collaboration recommendation</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;2012</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Arnetminer: extraction and mining of academic social networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="990" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An overview of microsoft academic service (mas) and applications</title>
		<author>
			<persName><forename type="first">Arnab</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darrin</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo-June Paul</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on world wide web</title>
		<meeting>the 24th international conference on world wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="243" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stochastic blockmodels: First steps</title>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">W</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">Blackmond</forename><surname>Laskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Leinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="137" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Structural temporal graph neural networks for anomaly detection in dynamic graphs</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengzhang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaping</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingchao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM international conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM international conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3747" to="3756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dynamic knowledge graph based multi-event forecasting</title>
		<author>
			<persName><forename type="first">Songgaojun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huzefa</forename><surname>Rangwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Ning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1585" to="1595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hierarchical temporal convolutional networks for dynamic recommender systems</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuck</forename><surname>Rosenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The world wide web conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2236" to="2246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Tedic: Neural modeling of behavioral patterns in dynamic social interaction networks</title>
		<author>
			<persName><forename type="first">Yanbang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference</title>
		<meeting>the Web Conference</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="693" to="705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fates of microscopic social ecosystems: Keep alive or dead</title>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxi</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yishi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="668" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Temp: Temporal message passing for temporal knowledge graph completion</title>
		<author>
			<persName><forename type="first">Jiapeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackie</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kit</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03526</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dynamic heterogeneous graph attention neural architecture search</title>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijian</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhou Qin</surname></persName>
		</author>
		<author>
			<persName><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Seventh AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Outof-distribution generalized dynamic graph neural network for human albumin prediction</title>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingwang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueling</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Medical Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Llm4dyg: Can large language models solve problems on dynamic graphs?</title>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijian</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep learning on graphs: A survey</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="249" to="270" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unsupervised graph neural architecture search with disentangled self-supervision</title>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Autogl: A library for automated graph learning</title>
		<author>
			<persName><forename type="first">Chaoyu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijian</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2021 Workshop GTRL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Nas-bench-graph: Benchmarking graph neural architecture search</title>
		<author>
			<persName><forename type="first">Yijian</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Sixth Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Graph differentiable architecture search with structure learning</title>
		<author>
			<persName><forename type="first">Yijian</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijian</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.14522</idno>
		<title level="m">Large graph models: A perspective</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Community preserving network embedding</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention network</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The world wide web conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2022" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Easydgl: Encode, train and interpret for continuous-time dynamic graph learning</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianzu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.12341</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Discrete-time temporal network embedding via implicit hierarchical learning in hyperbolic space</title>
		<author>
			<persName><forename type="first">Menglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Kalander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1975" to="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Hyperbolic variational graph neural network for modeling dynamic graphs</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongbao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4375" to="4383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Variational graph recurrent neural networks</title>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Hajiramezanali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Hasanzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Duffield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoning</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Inductive representation learning in temporal networks via causal anonymous walks</title>
		<author>
			<persName><forename type="first">Yanbang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen-Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.05974</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Dynamic graph representation learning via graph transformer networks</title>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanhong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengting</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinglong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuncheng</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2111.10447</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Inductive representation learning on temporal graphs</title>
		<author>
			<persName><forename type="first">Da</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanwei</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evren</forename><surname>Korpeoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sushant</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kannan</forename><surname>Achan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07962</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Temporal graph networks for deep learning on dynamic graphs</title>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10637</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Spectral temporal graph neural network for multivariate time-series forecasting</title>
		<author>
			<persName><forename type="first">Defu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanyong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congrui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bixiong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17766" to="17778" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Wellconditioned spectral transforms for dynamic graph representation</title>
		<author>
			<persName><forename type="first">Bingxin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuehua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunying</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Guang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning on Graphs Conference</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Learnable spectral wavelets on dynamic graphs to capture global interactions</title>
		<author>
			<persName><forename type="first">Anson</forename><surname>Bastos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Nadgeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuldeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toyotaro</forename><surname>Suzumura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.11979</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Towards out-of-distribution generalization: A survey</title>
		<author>
			<persName><forename type="first">Zheyan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renzhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.13624</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Domain generalization: A survey</title>
		<author>
			<persName><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<idno>arXiv-2103</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Improving out-of-distribution robustness via selective augmentation</title>
		<author>
			<persName><forename type="first">Huaxiu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the Thirty-ninth International Conference on Machine Learning</title>
		<meeting>eeding of the Thirty-ninth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Out-of-distribution generalization on graphs: A survey</title>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07987</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Invariance principle meets out-of-distribution generalization on graphs</title>
		<author>
			<persName><forename type="first">Yongqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaili</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binghui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.05441</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Graph neural architecture search under distribution shifts</title>
		<author>
			<persName><forename type="first">Yijian</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18083" to="18095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Ood-gnn: Out-of-distribution generalized graph neural network</title>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Learning to solve travelling salesman problem with hardness-adaptive curriculum</title>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.03236</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.12345</idno>
		<title level="m">Revisiting transformation invariant geometric deep learning: Are initial representations all you need?</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Generalizing graph neural networks on out-of-distribution graphs</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Shaohua Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.10657</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Learning invariant graph representations for out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Sixth Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Wild-time: A benchmark of in-the-wild distribution shift over time</title>
		<author>
			<persName><forename type="first">Huaxiu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoonho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<meeting>the Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Graph wavelet neural network</title>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunqi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07785</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Interpretable stability bounds for spectral graph filters</title>
		<author>
			<persName><forename type="first">Henry</forename><surname>Kenlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dorina</forename><surname>Thanou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowen</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5388" to="5397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">How powerful are spectral graph neural networks</title>
		<author>
			<persName><forename type="first">Xiyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="23341" to="23362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Specformer: Spectral graph neural networks meet transformers</title>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lele</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.01028</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Spectral subsampling mcmc for stationary time series</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Salomone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matias</forename><surname>Quiroz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Kohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mattias</forename><surname>Villani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Ngoc</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8449" to="8458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">From fourier to koopman: Spectral methods for long-term time series prediction</title>
		<author>
			<persName><forename type="first">Henning</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Brunton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Kutz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1881" to="1918" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingsong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="27268" to="27286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Self-supervised contrastive pre-training for time series via time-frequency consistency</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodoros</forename><surname>Tsiligkaridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3988" to="4003" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">A survey on spectral graph neural networks</title>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yawen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.05631</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Focal frequency loss for image reconstruction and synthesis</title>
		<author>
			<persName><forename type="first">Liming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="13919" to="13929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Multiwavelet-based operator learning for differential equations</title>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiongye</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bogdan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="24048" to="24062" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Arnetminer: Extraction and mining of academic social networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;08</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="990" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
