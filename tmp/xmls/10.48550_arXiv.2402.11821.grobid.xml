<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Microstructures and Accuracy of Graph Recall by Large Language Models</title>
				<funder>
					<orgName type="full">Microsoft AFMR program</orgName>
				</funder>
				<funder ref="#_sX2MDMg">
					<orgName type="full">MacArthur Foundation</orgName>
				</funder>
				<funder ref="#_EbFSegE">
					<orgName type="full">AFOSR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yanbang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Cornell University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
								<orgName type="institution" key="instit3">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hejie</forename><surname>Cui</surname></persName>
							<email>hejie.cui@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Cornell University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
								<orgName type="institution" key="instit3">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
							<email>kleinberg@cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Cornell University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
								<orgName type="institution" key="instit3">Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Microstructures and Accuracy of Graph Recall by Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D5D4AA9F26B2A6B181FBBE1884A8147D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-01-20T06:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graphs data is crucial for many applications, and much of it exists in the relations described in textual format. As a result, being able to accurately recall and encode a graph described in earlier text is a basic yet pivotal ability that large language models (LLMs) need to demonstrate if they are to perform reasoning tasks that involve graph-structured information. Human performance at graph recall by has been studied by cognitive scientists for decades, and has been found to often exhibit certain structural patterns of bias that align with human handling of social relationships. To date, however, we know little about how LLMs behave in analogous graph recall tasks: do their recalled graphs also exhibit certain biased patterns, and if so, how do they compare with humans and affect other graph reasoning tasks? In this work, we perform the first systematical study of graph recall by LLMs, investigating the accuracy and biased microstructures (local subgraph patterns) in their recall. We find that LLMs not only underperform often in graph recall, but also tend to favor more triangles and alternating 2-paths. Moreover, we find that more advanced LLMs have a striking dependence on the domain that a real-world graph comes from -by yielding the best recall accuracy when the graph is narrated in a language style consistent with its original domain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large language models (LLMs) achieve remarkable progress in recent years. In many applications, LLMs' success relies on appropriate handing of graph-structured information embedded in text. For example, to accurately answer questions pertaining to the characters in a story, it is crucial for an LLM to be able to recognize and analyze the social network of relations among these characters. In fact, graph-structured information is ubiquitous across many language-based applications, such as structured commonsense reasoning <ref type="bibr" target="#b31">[32]</ref>, multi-agent communications <ref type="bibr" target="#b2">[3]</ref>, multi-hop question answering <ref type="bibr" target="#b14">[15]</ref>, and more. LLMs' graph reasoning ability has thus become an active research topic.</p><p>Existing works on LLMs' graph reasoning ability have been primarily posited in the context of various graph tasks, from most basic ones such as computing node degree, graph diameter, clustering coefficient, or checking cycles <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b17">18]</ref>, to more challenging ones such as node/graph classification <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b37">38]</ref> and link-based recommendations <ref type="bibr" target="#b54">[55]</ref>. Sec. 7 provides a more comprehensive survey. But all of the tasks above rely on the premise that an LLM is able to start from the graph that is described in the text it is given. Thus, our key starting observation here is that all of these tasks rely on a pivotal (and perhaps seemingly trivial) ability of LLMs -to recall and encode a set of relations described in earlier text. In this paper, we consider a graph recall task which has been extensively studied in cognitive science <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b40">41]</ref>, and which formalizes this basic goal, illustrated in Figure <ref type="figure">1</ref>: a set of pairwise relationships is described in a simple narrative form to the experimental subject (human or LLM); then at a later point in the experiment, the subject is asked to recall and describe these relationships explicitly in the form of a graph.</p><p>38th Conference on Neural Information Processing Systems (NeurIPS 2024).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2402.11821v3 [cs.LG] 31 Oct 2024</head><p>The rationale for studying an LLM's graph recall is simple. If an LLM cannot even accurately recall the graph it is asked to reason upon, it will not be able to do well in any of the more advanced graph tasks surveyed above. Further, structural patterns in recall errors may propagate to (or even serve as the basis for) the behavior of LLMs in these more complex graph tasks. Therefore, we consider it important to investigate LLMs' graph recall ability, a topic absent from existing literature. The edge prediction task is related to but fundamentally different from the graph recall task: the correct answer for graph recall always exists in the prompt and can be directly extracted, which is not true for any prediction tasks.</p><p>Figure <ref type="figure">1</ref>: Graph recall is a simple task but also a crucial pivot for other graph reasoning tasks.</p><p>Meanwhile, the existing two decades of studies on human's graph recall ability provide another fascinating perspective to motivate our study. Cognitive scientists have found through substantial human experiments that, when memorizing a social network, humans extensively employ certain compression heuristics, such as triadic closure, near-clique completion, and certain degree biases <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b34">35]</ref>, due to natural limits on cognitive capacity. Further studies show that a person's ability to accurately recall a social network, along with the microstructures (local subgraph patterns, or network motifs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34]</ref>) in their recalled network, not only has profound influence on their social decisions <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b51">52]</ref>, but also varies on different styles of graph narrative <ref type="bibr" target="#b43">[44]</ref>, and the sex of the person <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22]</ref>. Do LLMs use similar compression heuristics as humans, and how are they affected by different working contexts? As LLMs become increasingly integrated into various social applications, it becomes crucial to understand LLM's behaviors and their associations with human behaviors in these regards.</p><p>The human cognition studies not only motivate our study, but also establish a scientific foundation for our experimental designs. Similar to <ref type="bibr" target="#b41">[42]</ref> which uses political orientation tests designed for humans to assess LLM's political bias, we also find some of the protocols employed by <ref type="bibr" target="#b7">[8]</ref> for testing human's graph recall highly instructional, including: (1) memory clearance, where a classical word span test <ref type="bibr" target="#b16">[17]</ref> is conducted between the presentation of the graph content and the query prompt; this serves as a chat buffer that helps simulate the delayed queries in many real-world applications; <ref type="bibr" target="#b1">(2)</ref> analyzing biased patterns in graph recall via Exponential Random Graph Model (ERGM), a probabilistic generalization to the network motif methods in graph mining; <ref type="bibr" target="#b2">(3)</ref> focusing on the probability of the tokens in subject's response, rather than just their presence; we replicate this through the log_prob parameter in GPT series or conducting Monto Carlo sampling for Gemini. We will elaborate on these in the following sections.</p><p>Our Work In this work, we investigate the several primary questions regarding the capabilities of LLMs in recalling graph structures. First, we examine the accuracy and microstructures in graphs recalled by LLMs through experiments on real-world graph structures, as well as compare these results with human performance. Second, we explore factors affecting LLMs' graph recall abilities, focusing on memory clearance strength, and narrative styles of graph encoding, which are known to affect human's graph recall. Finally, we also consider how LLMs' graph recall influences their performance in downstream tasks like link prediction, and discuss actionable insights that our findings provide for future research. To summarize, our work makes the following contributions:</p><p>1. We propose graph recall as a simple yet fundamental task for understanding LLM's graph reasoning abilities, drawing its connection with the existing cognitive studies on human's graph recall ability. 2. We are the first to design and conduct systematical studies on the accuracy and biased microstructures of LLM's graph recall, and to compare the results with humans. 3. We present many important and interesting findings on LLM's behaviors in graph recall, which significantly helps deepen our understandings about LLM's graph reasoning ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Exponential Random Graph Model (ERGM)</head><p>We use the Exponential Random Graph Model (ERGM) <ref type="bibr" target="#b38">[39]</ref> to characterize the statistical significance of the various microstructural patterns ("network motifs" <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34]</ref>) in the recalled graphs. ERGM is a special case of the exponential family dedicated for modeling graph-structured data.</p><p>Formally, let G be the probability space of all possible graphs over n nodes, and G = (V, E, f ) ∈ G be a random (graph) variable, where V is the set of nodes, E is the set of edges in G, and f : E → [0, 1] is a edge probability function. Assuming independence between edges, the probability of G can be written as:</p><formula xml:id="formula_0">P (G) = e∈E f (e) e / ∈E (1 -f (e))<label>(1)</label></formula><p>A is a list of k predefined microstructural patterns in G. Fig. <ref type="figure" target="#fig_0">2's</ref> Step 6 shows k = 5 such patterns that we primarily investigate in this work, following <ref type="bibr" target="#b7">[8]</ref>. The conditional probability of observing G given the parameter vector θ of length k is defined to be</p><formula xml:id="formula_1">P (G|θ) = exp{ k i=1 θ i s i (G)} c θ = exp{θ T s(G)} c θ<label>(2)</label></formula><p>where s(G) is the sufficient statistics of G, and each s i (G) is a count of the number of occurrences of A[i] in G; c θ is the normalizing constant that only depends on θ. We assume an uninformative prior for θ ∼ [-10, 10]. The posterior P (θ|G) can then be optimized via MAP under Bayesian framework. θ measures the strength of presence of the k microstructural patterns we care about. A large θ i means a strong presence of pattern A[i] in G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Memory Clearance</head><p>The word span test <ref type="bibr" target="#b16">[17]</ref> is a standard method for measuring a human's working memory capacity.</p><p>The test requires the subject to read a series of sentence sets out loud, and then recall the last word in each previous sentence in the current set. The number of sentences in each set gradually increases from two to seven, i.e. from three sets of two sentences to three sets of seven sentences. The test continues until the subject fails to recall the final words for two out of three sets of a given size. See Appendix B.2.1 for the sentence sets we used. Many cognition studies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5]</ref> have adopted this test in their experiment to (1) serve as a chat buffer or spoiler that simulates the delayed query in real-world applications, and (2) clear the shortterm memory of the subject, which allows researchers to better focus on relatively persistent patterns in memory structures. It is important to note that transformer-based LLMs are stateless models that, in strict sense, do not have memory. Nevertheless, we choose to preserve the (slightly misleading) term "memory clearance" to stay aligned with literature and to emphasize the close analogy and behavioral resemblance between LLMs and humans in graph recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Microstructures and Accuracy of Graph Recall by LLMs</head><p>Being able to correctly recall a graph described in earlier text is a fundamental ability for LLMs to perform graph reasoning. While graph recall may seem an easy task for the high-capable LLMs nowadays, our extensive experiment shows that their performance is in fact far from perfect. This section presents our study on the performance and the microstructures of graph recall by LLMs.</p><p>Our experimental protocols are introduced in Sec.3.1, followed by Sec.3.3 which presents head-tohead comparisons of LLMs and humans under <ref type="bibr" target="#b7">[8]</ref>'s framework. Sec.3.2 substantiates the analysis by experiments on more diverse datasets. Our code and data are reported in Appendix A.</p><p>LLMs Tested: GPT-3.5 <ref type="bibr" target="#b9">[10]</ref>, GPT-4 <ref type="bibr" target="#b0">[1]</ref>, Gemini-Pro <ref type="bibr" target="#b45">[46]</ref>. We also examined Llama 2 (13B) <ref type="bibr" target="#b46">[47]</ref>, but they can rarely follow through on our instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Protocols and Datasets</head><p>Our staged protocols are visualized in Fig. <ref type="figure" target="#fig_0">2</ref> and explained below. To recall each graph sample, an LLM needs to separately go through the entire pipeline. The stages proceed in an auto-regressive manner. In other words, the LLM's intermediate response is always appended to the current thread as additional conservation context, before we proceed to the next stage along the pipeline. Step 1: Task Introduction. The LLM is informed that this is a graph recall test, and that the recall task may happen at a later time. It is also incentivized to yield its best performance. These components follow the ones used in <ref type="bibr" target="#b7">[8]</ref> for human studies.</p><p>Step 2: Presenting Graph Vignette. A vignette in sociology is a short, descriptive story that encodes the central piece of information for soliciting subject's response. Here, our vignette encodes the graph structures sampled from a certain application domain using a certain narrative style -see the dataset tab below for details.</p><p>Step 3: Memory Clearance. A standard word span test <ref type="bibr" target="#b16">[17]</ref> is conducted with the LLM. See Sec.2.2 for details. We use the same set of sentences as in <ref type="bibr" target="#b7">[8]</ref>.</p><p>Step 4: Prompting. We use zero-shot prompting with moderate formatting instructions for answers. This follows both <ref type="bibr" target="#b7">[8]</ref>'s protocol and the finding in <ref type="bibr" target="#b17">[18]</ref> that simple prompts are the best for simple tasks, which we also empirically observed.</p><p>Step 5: Retrieving Edge Probabilities. We are interested in both the existence and the probability of each edge (i.e. the f in ERGM) in graphs recalled by LLM -the latter lets us examine LLM's behavior at finer granularities. We use two tricks to retrieve edge probabilities from LLM's answers:</p><p>• For GPT series, we can directly access token probabilities through the ChatCompletion API. We instruct the model to output 1 for each edge it believes to exist, and 0 otherwise. Then, we retrieve and normalize the probabilities for tokens 0 and 1, using the latter as the edge probability.</p><p>• For Gemini-Pro whose token probabilities are not accessible via the API, we conduct Monte Carlo sampling for each potential edge (repeating the query for 100 times), and use the fraction of existence as the edge probability.</p><p>Note that the retrieved edge probabilities essentially constitute a probability graph.</p><p>Step 6: Microstructure Analysis &amp; Performance Measurement. We use the ERGM introduced in Sec. 2.1 to model both the recalled graph and the ground truth, in order to reveal statistically significant structural patterns, or "microstructures" as termed by <ref type="bibr" target="#b7">[8]</ref>, in the recalled graphs. Specifically, for each microstructural pattern, we compute the gap between its estimated coefficient on the recalled graph against its estimated coefficient on the ground-truth graph. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results and Analysis</head><p>Table <ref type="table" target="#tab_0">1</ref> shows the results of microstructural patterns and performance of LLMs in our graph recall test. We primarily investigate the five microstructural patterns as shown in Fig. <ref type="figure" target="#fig_0">2</ref>. This is because these patterns have been observed to be biased patterns in human studies and are substantiated with rich sociological explanations <ref type="bibr" target="#b7">[8]</ref>; other patterns may also be interesting to examine though, which we leave for future work. A positive/negative value means the LLM is biased towards encouraging/depressing the corresponding microstructural pattern in recalled graphs. We have the following findings.</p><p>LLMs underperform in the graph recall test. We start by examining the performance metrics on the right columns. None of the models are able to perform perfectly on any dataset -in fact not even close in most cases. The unsaturated performance shows that the graph recall test is a meaningful task to investigate. The result also helps partially explain the poor performance of LLMs on many other graph tasks including node degree, edge count, and cycle check <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>LLMs may tend to forget, rather than hallucinate edges. The "edge" column shows an interesting result that LLMs generally recall fewer edges than the ground truth. This crucially tells us that LLM's bias in other microstructural patterns may more likely be the consequence of selective forgetting, rather than hallucination.</p><p>An LLM's microstructural bias is relatively robust. For each model, the colors in each column are relatively consistent. This means that an LLM may have have some relatively persistent bias in its microstructual patterns, which does not change significantly across different application domains. This is a positive indicator of the generalizability of our findings above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">LLMs Compared with Humans in Graph Recall</head><p>Brashears et al. reported experimental results of social network recall on a total of 301 humans <ref type="bibr" target="#b7">[8]</ref>.</p><p>Here we report how we build upon this existing result to compare LLMs' and humans' behaviors in the social network recall test.</p><p>To stay aligned with Brashears' settings, we use their dataset which consists of two 15-node social networks: an irreducible one which contains no cliques, and a reducible one which contains multiple cliques. Appendix B introduces more details. Table <ref type="table" target="#tab_1">2</ref> shows the comparison results.</p><p>Regarding the types of bias (forgetting vs. hallucination), LLMs are relatively consistent with humans. Both LLMs and humans tend to forget edges and alt-triangles while "hallucinating" triangles and stars. This interesting finding may reveal that LLMs have some information decoding and recall mechanisms similar to human memory.</p><p>Regarding the strength of the bias, LLMs tend to have weaker forgetting but stronger hallucinations than humans. On microstructures such as edges and alt-triangles, LLMs tend to have a weaker forgetting pattern, while on some prominently increasing microstructures (e.g., triangles), LLMs tend to demonstrate strong hallucinating patterns. These findings suggest that while LLMs may have a distinct advantage than humans in retaining certain graph structures in their memory, they may still struggle with accurately recalling, potentially limiting their ability to perform complex or critical graph tasks. It is a natural question to ask about factors that can affect an LLM's performance in the graph recall task. Many existing works have investigated graph properties and prompting methods as two key variants affecting LLM's performance in graph reasoning tasks <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b13">14]</ref>. Here we focus on several interesting factors that are less explored but still play a crucial role in graph recall: narrative style, strength of memory clearance, and sex priming (Appendix D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Narrative Style</head><p>Motivation. The effect of narrative styles on several graph reasoning abilities has been studied by <ref type="bibr" target="#b17">[18]</ref> with synthetic random graphs. Here we will present an interesting finding on real-world graphs, and by novelly cross-evaluating narrative styles and real-world domains.</p><p>The key idea is the following: it is known that graphs sampled from different domains have different distributions of topology, e.g. road networks are usually star-shaped, whereas social networks usually have more triangles <ref type="bibr" target="#b44">[45]</ref>. Meanwhile, each domain has its own style of narration: for describing road networks geographical locations and names are often used, while for describing social networks names and personal relationships are used more often. Our experiment thus far has always used for each dataset the matched narrative with its domain. Therefore, an interesting question is whether LLM would perform best in graph recall only if the narrative style of the graphs matches the domain.</p><p>To this end, we conduct cross-evaluation over the five different application domains and their corresponding narrative styles as introduced in Sec.3.1. More concretely, for graphs from each domain, we describe it in five different ways, corresponding to the five different domains. The resulting performance is visualized as heatmaps in Fig. <ref type="figure" target="#fig_1">3 (a) -(c)</ref>. Appendix E provides the full table.</p><p>Result Analysis. The heatmaps of GPT-4 and GPT-3.5 support our conjecture: the diagonals (corresponding to a matching between narrative style and the domain of the data) tend to have higher performance. Such an effect seems to be more prominent with better-performing models.</p><p>We find this result striking, that the LLM should do better when the graph is described in the narrative language of the domain that it comes from. While it is an intuitively sensible conjecture that this might help, it is very interesting that this conjecture is borne out so clearly in the results. Gemini-Pro appears more sensitive to small noise in context, while GPT's are more robust. Given this, it is natural to ask whether the result might be coming from the mechanics of the training; in particular, is it possible that the LLM is just reciting text from its training corpus, since the five datasets we use are all public on the Internet? We find this very unlikely, for a simple reason: while the structured graph data comes from the Internet, the narrative descriptions do not; they were generated by us from a simple template for purposes of this experiment, as explained in Sec.3.1.</p><formula xml:id="formula_2">$FF*UDSK5HFDOO $FF/LQN3UHGLFWLRQ )DFHERRNU $FF*UDSK5HFDOO $FF/LQN3UHGLFWLRQ &amp;$5RDGU $FF*UDSK5HFDOO $FF/LQN3UHGLFWLRQ 5HDFWRPHU $FF*UDSK5HFDOO $FF/LQN3UHGLFWLRQ '%/3U $FF*UDSK5HFDOO $FF/LQN3UHGLFWLRQ (UGRV5HQ\LU</formula><p>Since superficial explanations do not seem to explain the strength of the results, it becomes reasonable to suppose that the narrative style is indeed helping with the graph recall task. There are natural, if subtle, reasons why this may indeed be the case: since organically produced text describing road networks, for example, refers a different distribution over graph structures than organically produced text describing social networks, it is a plausible mechanism that recall is helped when the distributional properties in the text align with the distributional properties of the graph that the text describes. An implication is thus that LLMs, especially GPT-4, may have indeed formed a good understanding of the different distributions of graph structures from different domains, as otherwise they wouldn't be able to so consistently perform better when the narrative matches the data domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Strength of Memory Clearance</head><p>Motivation. Our next experiment contains memory clearance (word span test) which is a standard component in previous human graph recall tests. We use memory clearance both to align with these human tests and to simulate the real-world situation where an LLM may not be asked to work on the key data immediately after it receives them. The strength of memory clearance can be naturally measured by the maximum number of sentences of which the subject proceeds to recite the final words. The number in the standard word span test ranges from 2 to 7, and 0 if the test is dropped. Therefore, we vary this number in our experiment and record the performance of each model on each dataset.</p><p>Result Analysis. The results are shown in Figure <ref type="figure" target="#fig_1">3</ref> (d) -(f), and we have the following findings.</p><p>LLMs can significantly differ on their sensitivity to small amount of noise in graph recall. The trends are clear from the three line plots: Gemini-Pro's performance plunges in the first few clearance levels before it touches the bottom. GPT's performance, in contrast, remain more stable, or even increases at initial clearance levels. This indicates that many of our results for GPT models may still likely hold when there is no memory clearance, i.e. prompt is given immediately after relevant context -which is default setting of most previous studies.</p><p>Performance of GPT-3.5's and Gemini-pro's is poor even when the question prompt is provided immediately after the relevant context. This is obvious from the line plots' intersections with y-axis, and perhaps a bit surprising to people who have primarily focused on using LLMs for more challenging graph tasks. This result also helps eliminate the chance that the mediocre performance comes from overly strong memory clearance module that we've installed in place. To boost LLM's ability to reason on graphs, we may need to first figure out how to help them better attend to the correct edge before we seek to improve other more advanced aspects of reasoning. See Sec. 6 for more discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Correlation between LLM's Graph Recall and Link Prediction</head><p>Motivation. The graph recall task should not be confused with How do microstructures and performance of LLM's graph recall affect its behavior in other graph reasoning tasks? In this mini-study, we conduct a correlation analysis of LLM's behavior in the label-free link prediction task, which is an important graph reasoning task for LLMs <ref type="bibr" target="#b23">[24]</ref>. We primarily experiment with GPT-3.5 because its graph recall exhibits more significant microstructural patterns than GPT-4, and meanwhile have larger performance variation than Gemini-Pro.</p><p>Procedures. For each graph in the five datasets, we remove 20% of their edges as missing edges. The LLM is then asked to predict those missing edges. Two types of correlation are studied: (1) accuracy correlation: for each graph in the dataset, we evaluate the LLM's link prediction accuracy (x) and graph recall accuracy (y) then map these results onto a scatter plot; (2) microstructural correlation: similar to Table <ref type="table" target="#tab_0">1</ref> and 2, we evaluate and compare the microstructural coefficients of the predicted graph (in link prediction) and the recalled graph (in graph recall test). Table <ref type="table" target="#tab_2">3</ref> shows the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Result Analysis.</head><p>Figure <ref type="figure" target="#fig_2">4</ref> shows the scatter plots for accuracy correlation; Table <ref type="table" target="#tab_2">3</ref> shows microstructural correlation.</p><p>LLM's link prediction performance correlates well with its graph recall performance on realworld graphs. This is clear from the scatter plot and the r values. For Erdős-Rényi graphs, the correlation is close to zero, which is unsurprising though because the link prediction on random graphs cannot do better than random guess.</p><p>Different tasks may trigger behavior changes of LLMs that can be subtly revealed by their microstructural bias. Table <ref type="table" target="#tab_2">3</ref> shows that LLM's microstructural bias in both tasks tend to be positively correlated on triangles and alt-2-paths, and negatively correlated on alt-triangles. We do not have an intuitive explanation for this result. However, this result does indicate that different tasks can trigger certain interesting behavior changes of LLMs that can be subtly revealed by examining their microstructure patterns, which shows the meaningfulness of our study.</p><p>6 What to Inform about Future Research: an Empirical Perspective</p><p>We consider this study as a step towards the long-term agendas both for improving LLM's graph reasoning ability and for further integrating LLM graph analysis into social-science applications. Here we discuss how our findings may translate into actionable bias mitigation strategies and architectural improvements. Appendix F further discusses limitations of the work.</p><p>• The graph recall test is one of the simplest and most fundamental graph reasoning tasks. Since advanced LLMs yield unsatisfactory performance in this test, we need to re-examine the recent development of approaches that attempt to directly solve more challenging graph reasoning tasks. Meanwhile, notice the association between graph recall test and the "Needle in a Haystack" test (i.e. random fact retrieval from long context) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b18">19]</ref>: in some sense, the former can viewed as a "graph-customized" version of the latter. Therefore, existing techniques for boosting LLM's performance in the "Needle in a Haystack" test, e.g. recurrent memory augmentations <ref type="bibr" target="#b25">[26]</ref>, may likely benefit LLM's graph recall ability as well.</p><p>• Our experiment shows that LLMs tend to favor more triangles and alternating 2-paths. Researchers of Graph Foundation Models (GFM) <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b28">29]</ref> should be alerted of such systematic bias. Because the procedures for training GFMs are largely inspired by those for LLMs, similar retrieval bias could emerge. A potential mitigation strategy worthy of exploration is to consider balancing/compensating the frequencies of different graph motifs that occur in the training data.</p><p>• We have also found that more advanced LLM's performance have a striking dependence on the compatibility between the application domain of a real-world graph and its narrative style. This hints a potential direction for designing more powerful graph encoding for LLMs via certain domain adaptation strategies -by combining the adaptation strategy with either the graph-to-text methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref> or the graph-to-embedding methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>7 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Humans in the Graph Recall Task</head><p>There have been decades of work studying human's graph recall ability, primarily focused on their memory and recall of social networks. This line of work originates from sociologist's need to collect real-world social networks by asking people to recall their social relationships. <ref type="bibr" target="#b39">[40]</ref> noticed that people forget a significant portion of their social networks, so they built a to predict missing links from recall. <ref type="bibr" target="#b8">[9]</ref> more closely study the mechanism of the forgetting of social networks. <ref type="bibr" target="#b4">[5]</ref> found that humans can more accurately recall social networks that contain more certain microstructural patterns such as triangles or cliques.</p><p>Following the previous works, <ref type="bibr" target="#b7">[8]</ref> establishes experimental protocols for examining the performance and microstructural patterns of human's recall of social networks. In their study, each subject reads a short, artificial description of the relationships among a number of 15 people: "Henry is a member of the same club as Elizabeth. James sings in a choir with Anne ...". The subject is then asked to name who has interacted with whom in the experiment. The authors are also among the first to verify that humans' graph recall tend to exhibit patterns of triadic closure. <ref type="bibr" target="#b6">[7]</ref> further investigates how sex affects human's recall. <ref type="bibr" target="#b5">[6]</ref> uses a signed network model to show that human's graph recall may repel certain unstable patterns that involve unbalanced relationships between enemies and friends. Since LLMs are trained on human-readable corpus, many ideas and findings on human's graph recall are instructional to our exploration of LLM's graph reasoning ability. Further studies show that a person's ability to accurately recall a social network has profound influence on their social decisions <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref>.</p><p>The graph recall test is a meaningful test for both humans and LLMs, though their experiment outcomes may need to be interpreted in a subtly different way. The graph recall test is meaningful for LLMs because we observe their performance to be far from perfect in the test. Compared with humans, however, LLMs may face different challenges. Human's bottleneck in this test is their limited brain capacity, while LLMs may have difficulty in always attending to the correct positions in distant earlier context (or in precisely encoding context into hidden states for RNN-based models).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Graph Reasoning with Large Language Models</head><p>Graph reasoning with LLMs is an active research area in recent years. On the benchmark and methodology level, datasets and frameworks are developed to integrate graphs with LLMs <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>In addition, researchers are exploring ways to solve question answers over structured data with LLMs in a unified way <ref type="bibr" target="#b22">[23]</ref>. Others are also trying to advance the prompting capabilities of LLMs by mimicking the connective manner of human thinking or elaboration <ref type="bibr" target="#b3">[4]</ref>. There has also been work on developing general graph models to handle various graph tasks with a unified framework <ref type="bibr" target="#b29">[30]</ref>. More recently, <ref type="bibr" target="#b42">[43]</ref> provides theoretical analysis on the limit of transformer's reasoning capability on graphs, by relating transformer's capabilities on graph reasoning to the computational complexity of related tasks. On the application level, LLMs have been adopted in graph reasoning applications such as node classification <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b13">14]</ref>, graph classification <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b59">60]</ref>, knowledge graphs <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b55">56]</ref>, and recommendation systems <ref type="bibr" target="#b54">[55]</ref>. We are still lacking a data perspective to address the basic question of whether LLMs can accurately remember the graph that they are supposed to reason upon, which is a prerequisite for any advanced graph tasks. Inspired and supported by cognitive studies, we conduct the first comprehensive investigation of graph recall by LLMs, filling a gap in the existing literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>This work proposes and studies graph recall as a simple yet fundamental task for understanding LLM's graph reasoning abilities. We design and conduct systematical studies on the accuracy and biased microstructures of LLM's graph recall, by creatively drawing its connection with the existing cognitive studies on humans. Future work may examine more varieties of microstructural patterns including higher-order structures <ref type="bibr" target="#b50">[51]</ref> and "sense of distance", both of which which are crucial for understanding graph structures <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b56">57]</ref>. Another direction is to study how to improve LLM's graph recall by prompting or other methods.     "Maternity, or additional offspring, may force upon the woman a distressful life and future."</p><p>"By Wednesday night's vote meeting, Sabrina was thoroughly disgusted by the superficiality of the week."</p><p>"I ask him to please stop lying about trying to take my place in the war."</p><p>"Since the 1950's, freeways have been built through every large and medium-sized city."</p><p>"Instead, he took a job in Washington, analyzing weapons expenditures for the U.S. Navy."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Full Comparison of True Graph and LLM Recall Graph</head><p>We provide a full comparison of the true graph and the recall graph from different LLMs. The results are shown in Table <ref type="table" target="#tab_4">4</ref>, Table <ref type="table" target="#tab_5">5</ref>, and Table <ref type="table" target="#tab_6">6</ref>, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Sex Priming as a Mini-study</head><p>Motivation. <ref type="bibr" target="#b6">[7]</ref> interestingly found that females can more accurately recall their social networks than males. The underlying explanation is that underrepresented groups tend to be more aware of their surroundings. We conjecture that this trend might also exist in the corpus on which LLMs are trained, and therefore wonder whether LLMs perform better at graph recall when asked to play the role of a female.</p><p>Procedures. Aligned with <ref type="bibr" target="#b6">[7]</ref>'s design, we include at the beginning of the test (i.e. prior to all steps) a sex prime, which piece of text designed to elicit the test subject's awareness of their sex. <ref type="bibr" target="#b6">[7]</ref>'s sex prime is a short question that asks the subject's opinion on same-sex versus mixed-sex housing. However, this method fails with LLMs because they refuse to identify as having any pre-given sex. Instead, we send role-playing instructions to LLMs by stating their "sex" in the system message at the beginning of the chat. We further confirm the successful elicitation by asking what their sex is afterwards. We compare the LLM's graph recall performance under male and female roles.</p><p>Result Analysis. The results are shown in Fig. <ref type="figure" target="#fig_8">9</ref> (g) -(i). The error bars are 95% confidence intervals.</p><p>The effect of sex roles appear to be insignificant in most cases, which negates our initial conjecture and interestingly opposes the existing findings on humans. In fact, LLMs underperform in both roles when compared with the control group, i.e. cross-referencing Table <ref type="table" target="#tab_0">1</ref> "Accuracy" column.</p><p>Figure <ref type="figure" target="#fig_8">9</ref>: Different factors that influence LLM's graph recall. (g) -(i): sex. Following <ref type="bibr" target="#b6">[7]</ref>'s procedures, we test and find that the effect of sex roles that an LLM is asked to play is insignificant on their capability of graph recall -a result different from that on humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Full Performance Metrics with Different Narrative Styles</head><p>The full performance comparison of different narrative styles on five datasets for each LLM is present in Table <ref type="table" target="#tab_7">7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Limitations</head><p>Our evaluation of LLM's graph recall performance is not exhaustive. Due to budget constraint, we only test on datasets from five domains and on a limited number of graph samples from each domain. Also, while we have experimented with several main-stream LLMs including GPT-3.5, GPT-4, Gemini-Pro, and Llama-2, there are other popular LLMs such as Claude 3 and Llama-3 which our experiment have not covered.</p><p>Another limitation of our work, which we also consider to be an important direction for future work, lies in the distinction between graph recall and graph retrieval. The latter requires a more variational setting on the amount of contextual noise in graph narratives. In our work, we have intentionally designed the graph narratives to be simple and fixed, because we already observed considerable errors in LLM's graph recall at this starting point (and we know little about LLM's behaviors even in this simplest setting). However, the real-world structure-rich text that LLMs process may contain significantly more contextual noise, where some of the findings in this work may not be easily generalizable (although we conjecture that many systematic bias we have observed will persist). How to both realistically and comprehensively vary the amount of noise in graphs narratives remains to be a challenging topic to explore.</p><p>Finally, despite being small, the gap between our result and the recent "Needles in a Haystack" by Greg Kamradt <ref type="bibr" target="#b18">[19]</ref> on GPT-4 needs more investigation. In <ref type="bibr" target="#b18">[19]</ref>, it is reported that GPT-4 makes little errors in long-context recall over Paul Graham essays, up to the context length of 73k. While our study also shows (in Table <ref type="table" target="#tab_0">1</ref>) that GPT-4 performs well on three out of the five datasets used, its performance on the rest two is mediocre. There can be many possible explanations to reconcile the gap, but the lack of robustness that we observe in tests of this kind is an important starting point for further investigation.</p><p>G Relationship with the "Edge Existence" Task in (Fatemi et al.) <ref type="bibr" target="#b17">[18]</ref> The edge existence task is one of the six exemplary tasks proposed in <ref type="bibr" target="#b17">[18]</ref> for measuring LLM's graph reasoning ability. Both our graph recall test and the edge existence task require the LLM to decide the existence of graph edges described in earlier text. However, our work has the following main differences and novelty.</p><p>• The main topics of investigation are different. The central topics in this paper are 1) to unveil biased subgraph patterns (motifs) in LLM's graph recall, (2) to rigorously compare LLM's performance with humans, and (3) to investigate how results of (1) are affected by various factors. These topics are not studied in <ref type="bibr" target="#b17">[18]</ref>. • The empirical findings are completely non-overlapped. Our work presents many findings about LLM's biased graph recall patterns and their rigorous comparison with humans, which are novel to <ref type="bibr" target="#b17">[18]</ref>. • Our experimental pipelines are different. Our paper uses a more rigorous evaluation pipeline inspired by classical human cognition studies. In addition to synthetic data, we also extensively use real-world datasets for experiment, which was not reported <ref type="bibr" target="#b17">[18]</ref>.</p><p>• The answer NA means that the paper does not include experiments.</p><p>• If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways.</p><p>For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. , with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.</p><p>In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Open access to data and code</head><p>Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?</p><p>Answer: [Yes] Justification: We've provided original code and data to reproduce the main experimental results.</p><p>Guidelines:</p><p>• The answer NA means that paper does not include experiments requiring code. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Setting/Details</head><p>Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?</p><p>Answer: [Yes]</p><p>Justification: The paper has provided the main hyperparameters for the evaluation test, such as the prior for the ERGM, and the specifications for generating the synthetic datasets. More nuanced details have been included in the code.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not include experiments.</p><p>• The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiment Statistical Significance</head><p>Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?</p><p>Answer: [Yes]</p><p>Justification: We provided 95% conficence internvals for all of the main results in the paper.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not include experiments.</p><p>• The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Experiments Compute Resources</head><p>Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?</p><p>Answer: [Yes]</p><p>Justification: We have provided our cloud providers for all the APIs used in this paper in Appendix.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not include experiments.</p><p>• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). • The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.</p><p>• If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Broader Impacts</head><p>Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?</p><p>Answer: [Yes]</p><p>Justification: We have a section for this.</p><p>Guidelines:</p><p>• The answer NA means that there is no societal impact of the work performed.</p><p>• If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Safeguards</head><p>Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?</p><p>Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper is not directly involved with research with human subjects. It is, however, inspired by and adopts some of the results in a (referenced) previous paper that involves human subject. All details of instructions can be found in that paper. Guidelines:</p><p>• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Experimental protocols for analyzing microstructures and accuracy of LLM's graph recall. See Sec.3.1 for detailed explanations.</figDesc><graphic coords="4,108.00,72.00,396.00,130.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Different factors that influence LLM's graph recall. (a) -(c): narrative styles. The heatmaps show that more advanced LLMs like GPT-4 yield best recall accuracy when the graph is narrated in a language style consistent with its original domain. (d) -(f): memory clearance.Gemini-Pro appears more sensitive to small noise in context, while GPT's are more robust.</figDesc><graphic coords="7,108.00,72.00,396.01,242.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Correlation between GPT-3.5's performance at graph recall (y) and link prediction (x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Graph samples of the Facebook dataset.</figDesc><graphic coords="15,168.98,86.05,274.04,274.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Graph samples from the Reactome (protein-protein interaction) dataset.</figDesc><graphic coords="15,168.98,414.03,274.04,274.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Graph samples of the Erdos-Renyi dataset.</figDesc><graphic coords="16,170.56,111.61,270.87,270.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The two 15-node social networks with different connectivity patterns used in [8] and in our Sec. 3.3 experiment.</figDesc><graphic coords="16,151.26,487.57,309.47,164.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>9 .</head><label>9</label><figDesc>Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have strictly conformed to the NeurIPS Code of Ethics Guidelines:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Microstructural patterns and performance of graph recall by LLMs on graphs sampled from various application domains; mean ± ci 95% reported. The numbers reported for microstructural patterns are signficance parameters θ computed by the ERGM model introduced in Sec. 2.1. A positive (negative) number means the LLM is biased towards encouraging (depressing) the corresponding microstructural pattern in recalled graphs. The patterns are visualized in Fig.2Step 6. Full table available in Appendix C.</figDesc><table><row><cell>Steps 1 -6 are repeated for 100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>LLMs vs. Humans: microstructural patterns and performance of graph recall, conducted on the two social networks used in<ref type="bibr" target="#b7">[8]</ref>. Numbers on the "humans" row were taken from Brashears' paper and postprocessed by us. * not reported in Brashears' paper.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Microstructure</cell><cell></cell><cell></cell><cell>Performance</cell></row><row><cell>Model</cell><cell>Edge</cell><cell>Triangle</cell><cell>Star</cell><cell>Alt-Triangle</cell><cell>Alt-2-Path</cell><cell>Accuracy (%)</cell></row><row><cell cols="2">"Irreducible" social network</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Humans</cell><cell>-3.19+-5.97</cell><cell>9.52+-1.23 ↑</cell><cell cols="2">2.31+-0.11 ↑ -3.39+-1.22 ↓</cell><cell>-1.71+-3.32</cell><cell>29.72</cell></row><row><cell>GPT-3.5</cell><cell>-2.23+-2.79</cell><cell>5.40+-0.51 ↑</cell><cell cols="2">4.69+-2.09 ↑ -1.74+-0.87 ↓</cell><cell>-1.26+-1.39</cell><cell>31.51</cell></row><row><cell>GPT-4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>95.71</cell></row><row><cell cols="2">Gemini-Pro 9.82+-0.28 ↑</cell><cell>7.63+-0.45 ↑</cell><cell cols="2">-0.46+-1.12 ↑ -2.88+-0.98 ↓</cell><cell>-0.47+-0.99</cell><cell>24.99</cell></row><row><cell cols="2">"Reducible" social network</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Humans</cell><cell cols="3">-9.41+-2.21 ↓ 1.71+-0.51 ↑ -1.67+-0.03 ↓</cell><cell>- *</cell><cell>4.34+-1.72 ↑</cell><cell>17.80</cell></row><row><cell>GPT-3.5</cell><cell>-5.64+-1.56 ↓</cell><cell>-0.13+-5.71</cell><cell>-2.52+-16.43</cell><cell>-4.79+-14.96</cell><cell>1.91+-2.97</cell><cell>51.11</cell></row><row><cell>GPT-4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>-5.36+-1.59 ↓ 11.40+-3.07 ↑ 3.40+-1.45 ↑ -2.05+-1.22 ↓ -0.96+-0.48 ↓ -2.82+-2.51 ↓ -1.10+-0.86 ↓ -1.70+-0.97 ↓ -0.93+-0.64 ↓ 0.47+-0.27 95.74 Gemini-Pro -3.25+-2.09 ↓ 10.92+3.52 ↑ -1.99+-8.35 -0.44+-4.37 0.03+-0.39 38.82 4 What Affects LLM's Graph Recall?</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Correlation between LLM's graph recall and link prediction on different microstructures.</figDesc><table><row><cell>Microstructure</cell></row></table><note><p>Since both GPT and Gemini are able to perform 100% correctly in the word span test, by design principle we always need to progress to the maximum set of seven sentences. This makes the memory clearance a significant source of noisy context between LLM's first sight of the graph and the final prompt of the recall question. Therefore, it is natural to wonder if the relatively poor performance of LLMs in this graph recall test could have resulted from too much noisy context. In this mini-study, we investigate how different strengths of memory clearance measured would affect the performance.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>B.1 Samples of the Graph Narratives for All Datasets **Facebook** "The following people have friendship relations: Harper, James, Abigail, Noah, Alexander, Oliver, William, Charlotte, Sophia, Benjamin, Emily, Daniel, Ethan, Henry. Their have friendships: Harper and Daniel are friends. Harper and Sophia are friends. James and William are friends. James and Daniel are friends. James and Abigail are friends. James and Sophia are friends. James and Noah are friends. Abigail and Benjamin are friends. Abigail and Noah are friends. Abigail and Ethan are friends. Abigail and William are friends. Noah and Charlotte are friends. Noah and Benjamin are friends. Noah and Emily are friends. " **Traffic Network** We have a traffic network that involves the following destinations: Bank, Town Hall, Grocery Store, High School. The traffic network is the following. The traffic can directly flow from the Bank to the Grocery Store. The traffic can directly flow from the Town Hall to the Grocery Store. The traffic can directly flow from the Grocery Store to the High School. Node 0 is connected with Node 3. Node 0 is connected with Node 4. Node 0 is connected with Node 6. Node 0 is connected with Node 7. Node 0 is connected with Node 8. Node 0 is connected with Node 10. Node 0 is connected with Node 11. Node 0 is connected with Node 12. Node 0 is connected with Node 13. Node 0 is connected with Node 14. Node 1 is connected with Node 4. Node 1 is connected with Node 5. Node 1 is connected with Node 7. Node 1 is connected with Node 8. Node 1 is connected with Node 9. Node 1 is connected with Node 10. Node 1 is connected with Node 11. Node 1 is connected with Node 14. Node 1 is connected with Node 16. Node 2 is connected with Node 4. Node 2 is connected with Node 5. Node 2 is connected with Node 6. Node 2 is connected with Node 7. Node 2 is connected with Node 9. Node 2 is connected with Node 10. Node 2 is connected with Node 11. Node 2 is connected with Node 13. Node 2 is connected with Node 14. Node 2 is connected with Node 16.</figDesc><table><row><cell>B.2 Textual Samples Used in Experiment</cell></row><row><cell>B.2.1 Sentence Sets for Memory Clearance</cell></row><row><cell>**Two sentence sets**</cell></row><row><cell>"The ghillie suit, the modern sniper's principle camouflage uniform, derives its name from</cell></row><row><cell>Scottish game hunters."</cell></row><row><cell>"Industrial accidents-explosions of stored oil and gas-are bad enough on land."</cell></row><row><cell>**Three sentence sets**</cell></row><row><cell>"But overall, the teenage share of the population wasn't getting much bigger."</cell></row><row><cell>"This is a system steeped in tradition, and I think that's part of the problem."</cell></row><row><cell>"We're unable to move, our legs stuck beneath us as under a great weight."</cell></row><row><cell>**Protein Interaction** ...</cell></row><row><cell>The following proteins have mutual interactions: Q13563, Q16280, P61006, O75385, Q9ULV0,</cell></row><row><cell>O75154, Q7L804, Q15907, Q96QF0, Q8IV77, P08100, P18085, Q14028, P98161, Q9ULH1,</cell></row><row><cell>Q9P2M4. Their interactions are: Protein Q13563 interacts with Protein P98161. Protein Q13563</cell></row><row><cell>interacts with Protein P08100. Protein Q13563 interacts with Protein Q8IV77. Protein Q13563</cell></row><row><cell>interacts with Protein Q16280. Protein Q13563 interacts with Protein Q14028. Protein Q13563</cell></row><row><cell>interacts with Protein P18085. Protein Q13563 interacts with Protein Q9ULH1. Protein Q13563</cell></row><row><cell>interacts with Protein O75154. Protein Q13563 interacts with Protein P61006. Protein Q13563</cell></row><row><cell>interacts with Protein Q96QF0. Protein Q16280 interacts with Protein P98161. Protein Q16280</cell></row><row><cell>interacts with Protein P08100. Protein Q16280 interacts with Protein Q8IV77. Protein Q16280</cell></row><row><cell>interacts with Protein Q14028. Protein Q16280 interacts with Protein P18085. Protein Q16280</cell></row><row><cell>interacts with Protein Q9ULH1. Protein Q16280 interacts with Protein O75154. Protein Q16280</cell></row><row><cell>interacts with Protein P61006. Protein Q16280 interacts with Protein Q96QF0. Protein P61006</cell></row><row><cell>interacts with Protein Q96QF0. Protein P61006 interacts with Protein P98161. Protein P61006</cell></row><row><cell>interacts with Protein P08100. Protein P61006 interacts with Protein Q8IV77. Protein P61006</cell></row><row><cell>interacts with Protein Q14028. Protein P61006 interacts with Protein Q9ULH1. Protein P61006</cell></row><row><cell>interacts with Protein O75154. Protein O75385 interacts with Protein Q15907. Protein O75385</cell></row><row><cell>interacts with Protein Q9P2M4.</cell></row></table><note><p>**Erdos-Renyi Graph** A graph has the following nodes: Node 0, Node 1, Node 2, Node 3, Node 4, Node 5, Node 6, Node 7, Node 8, Node 9, Node 10, Node 11, Node 12, Node 13, Node 14, Node 15, Node 16, and the following edges: Node 0 is connected with Node 1. Node 0 is connected with Node 2. **Seven Sentence Set** "One afternoon I open a letter from my younger sister, the photo chronicler of family events." "Although they have enough food to sustain your group for years, supermarkets are also dangerous."</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Full comparison of true graph and GPT-3.5 recall graph on different microstructures.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Microstructure</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Edge</cell><cell>Triangle</cell><cell>Star</cell><cell>Alt-Triangle</cell><cell>Alt-2-Path</cell></row><row><cell>Facebook</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True Graph</cell><cell>0.88+-1.46</cell><cell>0.61+-0.57</cell><cell>-1.75+-0.45</cell><cell>-0.02+-0.24</cell><cell>1.07+-0.21</cell></row><row><cell>Recall Graph</cell><cell>-2.82+-5.47</cell><cell>2.33+-1.33</cell><cell>-2.46+-3.00</cell><cell>-0.93+-2.23</cell><cell>4.38+-1.72</cell></row><row><cell>Diff</cell><cell>-3.70+-5.80</cell><cell>1.72+-1.34</cell><cell>-0.70+-3.00</cell><cell>-0.91+-2.25</cell><cell>3.31+-1.77</cell></row><row><cell>CA Road</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True Graph</cell><cell>0.86+-2.50</cell><cell>2.90+-1.79</cell><cell>-4.83+-1.53</cell><cell>-2.07+-0.37</cell><cell>4.75+-0.52</cell></row><row><cell>Recall Graph</cell><cell>1.51+-1.73</cell><cell cols="2">10.21+-1.74 -8.29+-2.10</cell><cell>-3.54+-1.05</cell><cell>7.10+-0.91</cell></row><row><cell>Diff</cell><cell>0.64+-0.91</cell><cell>7.31+-3.49</cell><cell>-3.46+-1.77</cell><cell>-1.47+-0.92</cell><cell>2.35+-1.29</cell></row><row><cell>Reactome</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True Graph</cell><cell>18.19+-5.96</cell><cell>1.23+-4.31</cell><cell>-9.46+-2.50</cell><cell>1.15+-2.61</cell><cell>1.40+-3.28</cell></row><row><cell>Recall Graph</cell><cell>0.18+-5.92</cell><cell>0.52+-2.78</cell><cell>-4.49+-5.15</cell><cell>-5.17+-3.49</cell><cell>5.84+-4.11</cell></row><row><cell>Diff</cell><cell cols="2">-18.01+-6.22 -0.71+-4.62</cell><cell>4.96+-3.81</cell><cell>-6.32+-4.23</cell><cell>4.43+-4.93</cell></row><row><cell>DBLP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True Graph</cell><cell>7.63+-3.24</cell><cell cols="2">-1.68+-2.02 -5.30+-2.41</cell><cell>2.69+-2.14</cell><cell>-1.17+-2.30</cell></row><row><cell>Recall Graph</cell><cell>-0.49+-2.24</cell><cell>-0.51+-3.86</cell><cell>1.88+-1.79</cell><cell>-8.47+-4.16</cell><cell>4.60+-3.58</cell></row><row><cell>Diff</cell><cell>-8.12+-3.25</cell><cell>1.17+-4.43</cell><cell cols="3">7.17+-2.44 -11.16+-4.35 5.77+-3.76</cell></row><row><cell>Erdős-Rényi</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True Graph</cell><cell>-0.19+-3.44</cell><cell cols="2">-1.96+-1.59 -0.75+-3.28</cell><cell>0.86+-1.66</cell><cell>1.38+-0.89</cell></row><row><cell>Recall Graph</cell><cell>-1.60+-5.10</cell><cell>7.61+-4.75</cell><cell>0.64+-1.66</cell><cell>0.45+-3.50</cell><cell>4.74+-2.53</cell></row><row><cell>Diff</cell><cell>-1.40+-5.01</cell><cell>9.57+-4.89</cell><cell>1.40+-2.71</cell><cell>-0.41+-4.19</cell><cell>3.36+-2.76</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Full comparison of true graph and GPT-4 recall graph on different microstructures.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Microstructure</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Edge</cell><cell>Triangle</cell><cell>Star</cell><cell>Alt-Triangle</cell><cell>Alt-2-Path</cell></row><row><cell>Facebook</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True Graph</cell><cell>0.88+-1.46</cell><cell cols="3">0.61+-0.57 -1.75+-0.45 -0.02+-0.24</cell><cell>1.07+-0.21</cell></row><row><cell>Recall Graph</cell><cell>0.71+-1.51</cell><cell cols="3">0.66+-0.61 -1.69+-0.45 -0.03+-0.26</cell><cell>1.08+-0.21</cell></row><row><cell>Diff</cell><cell>-0.17+-0.50</cell><cell>0.05+-0.78</cell><cell>0.06+-0.21</cell><cell>-0.01+-0.26</cell><cell>0.01+-0.06</cell></row><row><cell>CA Road</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True Graph</cell><cell>0.86+-2.50</cell><cell cols="3">2.90+-1.79 -4.83+-1.53 -2.07+-0.37</cell><cell>4.75+-0.52</cell></row><row><cell>Recall Graph</cell><cell>2.20+-1.55</cell><cell cols="3">8.97+-2.43 -7.93+-1.52 -3.34+-1.15</cell><cell>6.60+-0.78</cell></row><row><cell>Diff</cell><cell>1.34+-1.62</cell><cell cols="3">6.07+-3.93 -3.09+-2.08 -1.27+-0.96</cell><cell>1.85+-0.91</cell></row><row><cell>Reactome</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True Graph</cell><cell>18.19+-5.96</cell><cell>1.23+-4.31</cell><cell>-9.46+-2.50</cell><cell>1.15+-2.61</cell><cell>1.40+-3.28</cell></row><row><cell>Recall Graph</cell><cell>6.64+-5.56</cell><cell cols="3">2.05+-2.31 -6.77+-3.21 -0.84+-1.24</cell><cell>0.95+-0.97</cell></row><row><cell>Diff</cell><cell cols="2">-11.54+-5.82 0.82+-4.28</cell><cell>2.69+-2.87</cell><cell cols="2">-1.99+-2.03 -0.46+-2.14</cell></row><row><cell>DBLP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True Graph</cell><cell>7.63+-3.24</cell><cell cols="2">-1.68+-2.02 -5.30+-2.41</cell><cell>2.69+-2.14</cell><cell>-1.17+-2.30</cell></row><row><cell>Recall Graph</cell><cell>6.37+-3.08</cell><cell cols="2">-3.09+-3.33 -5.99+-3.24</cell><cell>4.40+-3.10</cell><cell>-2.76+-2.51</cell></row><row><cell>Diff</cell><cell>-1.26+-2.56</cell><cell cols="2">-1.41+-3.26 -0.69+-3.74</cell><cell>1.71+-2.96</cell><cell>-1.59+-2.24</cell></row><row><cell>Erdős-Rényi</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True Graph</cell><cell>-0.19+-3.44</cell><cell cols="2">-1.96+-1.59 -0.75+-3.28</cell><cell>0.86+-1.66</cell><cell>1.38+-0.89</cell></row><row><cell>Recall Graph</cell><cell>-1.69+-3.21</cell><cell cols="3">-0.01+-1.09 -0.23+-2.13 -0.40+-1.49</cell><cell>0.93+-1.76</cell></row><row><cell>Diff</cell><cell>-1.49+-4.15</cell><cell>1.95+-1.54</cell><cell>0.52+-2.79</cell><cell cols="2">-1.26+-1.66 -0.45+-0.76</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Full comparison of true graph and Gemini-Pro recall graph on different microstructures.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Microstructure</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Edge</cell><cell>Triangle</cell><cell>Star</cell><cell>Alt-Triangle</cell><cell>Alt-2-Path</cell></row><row><cell>Facebook</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True Graph</cell><cell>0.88+-1.46</cell><cell cols="2">0.61+-0.57 -1.75+-0.45</cell><cell>-0.02+-0.24</cell><cell>1.07+-0.21</cell></row><row><cell cols="4">Recall Graph -1.44+-0.90 -1.68+-3.00 -0.25+-2.35</cell><cell>2.38+-1.35</cell><cell>1.74+-1.06</cell></row><row><cell>Diff</cell><cell cols="3">-2.31+-1.26 -2.29+-2.99 1.50+-2.37</cell><cell>2.40+-1.37</cell><cell>0.67+-1.10</cell></row><row><cell>CA Road</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True Graph</cell><cell>0.86+-2.50</cell><cell cols="2">2.90+-1.79 -4.83+-1.53</cell><cell>-2.07+-0.37</cell><cell>4.75+-0.52</cell></row><row><cell>Recall Graph</cell><cell>1.70+-1.15</cell><cell>4.93+-0.84</cell><cell>0.41+-2.95</cell><cell>-1.18+-0.44</cell><cell>-2.07+-2.28</cell></row><row><cell>Diff</cell><cell>0.84+-1.68</cell><cell>2.02+-0.29</cell><cell>5.24+-0.27</cell><cell>0.89+-0.28</cell><cell>-6.82+-0.62</cell></row><row><cell>Reactome</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True Graph</cell><cell>18.19+-5.96</cell><cell cols="2">1.23+-4.31 -9.46+-2.50</cell><cell>1.15+-2.61</cell><cell>1.40+-3.28</cell></row><row><cell>Recall Graph</cell><cell>6.25+-5.81</cell><cell>2.50+-4.04</cell><cell>4.02+-5.87</cell><cell>4.47+-5.71</cell><cell>5.56+-5.01</cell></row><row><cell>Diff</cell><cell cols="3">-11.94+-4.65 1.27+-5.22 13.47+-4.70</cell><cell>3.32+-4.57</cell><cell>4.15+-4.01</cell></row><row><cell>DBLP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True Graph</cell><cell>7.63+-3.24</cell><cell cols="2">-1.68+-2.02 -5.30+-2.41</cell><cell>2.69+-2.14</cell><cell>-1.17+-2.30</cell></row><row><cell cols="4">Recall Graph -11.84+-4.72 -3.40+-3.83 5.94+-2.13</cell><cell>-8.39+-3.34</cell><cell>9.66+-3.99</cell></row><row><cell>Diff</cell><cell cols="5">-19.47+-3.30 -1.72+-3.45 11.24+-2.45 -11.08+-3.90 10.83+-4.53</cell></row><row><cell>Erdős-Rényi</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True Graph</cell><cell cols="3">-0.19+-3.44 -1.96+-1.59 -0.75+-3.28</cell><cell>0.86+-1.66</cell><cell>1.38+-0.89</cell></row><row><cell cols="4">Recall Graph -3.06+-1.67 -0.45+-4.00 -1.41+-1.20</cell><cell>1.45+-2.17</cell><cell>0.69+-0.46</cell></row><row><cell>Diff</cell><cell>-2.86+-5.16</cell><cell cols="2">1.51+-5.04 -0.66+-3.95</cell><cell>0.59+-2.81</cell><cell>-0.70+-1.15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Full graph recall performance metrics with different narrative styles on five datasets.</figDesc><table><row><cell>Dataset</cell><cell>Narrative Style</cell><cell>F1</cell><cell>GPT-3.5 (%) Accuracy Precision Recall</cell><cell>F1</cell><cell>GPT-4 (%) Accuracy Precision Recall</cell><cell>F1</cell><cell>Gemini-Pro (%) Accuracy Precision Recall</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>• Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>• According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper is not directly involved with research with human subjects. It is, however, inspired by and adopts some of the results in a (referenced) previous paper that involves human subject. All details of instructions can be found in that paper. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>We thank <rs type="person">Matt Brashears</rs>, <rs type="person">Eric Quintane</rs> for their help in discussing their earlier work on network recall and its relation to the current project, as well as their help with data relevant to their work. We also thank <rs type="person">Tianxiang Zhao</rs> for his insights into the intersection of graphs and LLMs. Their generous assistance was a great benefit to our project. Our work here has been supported in part by a <rs type="grantName">Vannevar Bush Faculty Fellowship</rs>, <rs type="funder">AFOSR</rs> grant <rs type="grantNumber">FA9550-19-1-0183</rs>, a <rs type="grantName">Simons Collaboration grant</rs>, a grant from the <rs type="funder">MacArthur Foundation</rs>, and a grant from the <rs type="funder">Microsoft AFMR program</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_EbFSegE">
					<idno type="grant-number">FA9550-19-1-0183</idno>
					<orgName type="grant-name">Vannevar Bush Faculty Fellowship</orgName>
				</org>
				<org type="funding" xml:id="_sX2MDMg">
					<orgName type="grant-name">Simons Collaboration grant</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Code and data</head><p>Our code and data can be downloaded at: <ref type="url" target="https://github.com/Abel0828/llm-graph-recall">https://github.com/Abel0828/llm-graph-recall</ref>. For the experiment in Sec.3.3, we utilize the two social networks in <ref type="bibr" target="#b7">[8]</ref>'s human experiment. Each social network was presented to the test taker in two different narrative styles: one is friendship-based, and the other is kinship-based e.g. "James is the brother of Anne...". In Brashears' experiment, a test taker has equal chance to see one of the two narrative style (but never both). The final result in their paper, however, was reported in an aggregated form that mixes up response to both narrative styles, as the authors reported to see no difference between caused by these two. To stay aligned we choose do the same with LLMs, but make a note that LLMs may behave differently to different graph narrative styles, which we have further investigated in a separate study, Sec.4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More Experimental Details</head><p>We need to be careful with the generation of the node names, because it affects how much parametric knowledge in the LLM gets elicited to aid the graph recall process. We general guideline is that, although the parametric knowledge can potentially create a shortcut for LLM's handling of the graph recall task, this should not be forcefully forbidden as it is also seen in real-world applications. Therefore, we have two cases when creating our datasets, namely <ref type="bibr" target="#b0">(1)</ref> where parametric knowledge is potentially useful, and (2) where parametric knowledge is less useful.</p><p>• For protein networks, the protein names are not random. They are unique protein identifiers (known as the "UniProtKB/Swiss-Prot accession number" or NCBI index). Each node is assigned its real name. We also confirmed that LLMs know and precisely understand those protein identifiers. The same is true for DBLP coauthorship networks. • For all other networks, the node names are generated and randomly assigned. For example, in traffic (geographical) networks, the node names are "bank", "townhall", "high school", etc.</p><p>Computing Resources. We use the Azure OpenAI service to test GPT-based models. For Gemini-Pro, we use the gemini-pro model API provided by Google's Generative AI. For Llama Family models, we use the open-sourced models meta-llama/Llama-2-7b-hf and meta-llama/Llama-2-13b-hf on Hugging Face, tuned on two Quadro RTX 8000 GPUs with 48 GB of RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NeurIPS Paper Checklist</head><p>The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit.</p><p>Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:</p><p>• You should answer [Yes] , [No] , or [NA] .</p><p>• [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.</p><p>• Please provide a short (1-2 sentence) justification right after your answer (even for NA).</p><p>The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.</p><p>The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.</p><p>While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.</p><p>IMPORTANT, please:</p><p>• Delete this instruction block, but keep the section heading "NeurIPS paper checklist",</p><p>• Keep the checklist subsection headings, questions/answers and guidelines below.</p><p>• Do not modify the questions and only use the provided macros for your answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Claims</head><p>Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?</p><p>Answer: [Yes]</p><p>Justification: We've made claims about this work's originality, novelty, and significance. They are accurately described, with the central task and scope well defined.</p><p>Guidelines:</p><p>• The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Limitations</head><p>Question: Does the paper discuss the limitations of the work performed by the authors?</p><p>Answer: [Yes]</p><p>Justification: We have Appendix F for this. Guidelines:</p><p>• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate "Limitations" section in their paper.</p><p>• The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach.</p><p>For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Theory Assumptions and Proofs</head><p>Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: This paper does not include theoretical results. Guidelines:</p><p>• The answer NA means that the paper does not include theoretical results.</p><p>• All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. • All assumptions should be clearly stated or referenced in the statement of any theorems.</p><p>• The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Result Reproducibility</head><p>Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We've provided original code and data to be downloaded at: <ref type="url" target="https://anonymous.4open.science/r/llm-graph-recall-8513">https: //anonymous.4open.science/r/llm-graph-recall-8513</ref>. They can be used to reproduce the main experimental results of the paper. Guidelines:</p><p>Answer: [NA]</p><p>Justification: We believe that this paper poses no such risks.</p><p>Guidelines:</p><p>• The answer NA means that the paper poses no such risks.</p><p>• Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.</p><p>12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?</p><p>Answer: [Yes]</p><p>Justification: We have properly cited all the dataset being used in this paper.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not use existing assets.</p><p>• The authors should cite the original paper that produced the code package or dataset.</p><p>• The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset.</p><p>• For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset's creators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.">New Assets</head><p>Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?</p><p>Answer: [NA]</p><p>Justification: The paper does not release new assets.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not release new assets.</p><p>• Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.">Crowdsourcing and Research with Human Subjects</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lama</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilge</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florencia</forename><surname>Leoni Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janko</forename><surname>Altenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyamal</forename><surname>Anadkat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Network motifs: theory and experimental approaches</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Genetics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="450" to="461" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models as agent models</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5769" to="5779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph of thoughts: Solving elaborate problems with large language models</title>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Besta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Blach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ales</forename><surname>Kubicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Gerstenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Gianinazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><surname>Gajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Podstawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Niewiadomski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Nyczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Humans use compression heuristics to improve the recall of social networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><surname>Brashears</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1513</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The enemy of my friend is easy to remember: Balance as a compression heuristic</title>
		<author>
			<persName><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><forename type="middle">Aufderheide</forename><surname>Brashears</surname></persName>
		</author>
		<author>
			<persName><surname>Brashears</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in group processes</title>
		<imprint>
			<publisher>Emerald Group Publishing Limited</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sex and network recall accuracy</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Matthew E Brashears</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hoagland</surname></persName>
		</author>
		<author>
			<persName><surname>Quintane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Networks</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="74" to="84" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The microstructures of network recall: How social networks are encoded and represented in human memory</title>
		<author>
			<persName><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brashears</surname></persName>
		</author>
		<author>
			<persName><surname>Quintane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Networks</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="113" to="126" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Forgetting in the recall-based elicitation of personal and social networks</title>
		<author>
			<persName><forename type="first">Devon D</forename><surname>Brewer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="43" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Personality correlates of structural holes</title>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Ronald S Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">T</forename><surname>Jannotta</surname></persName>
		</author>
		<author>
			<persName><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="87" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Graphllm: Boosting graph reasoning ability of large language model</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiqiao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohai</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanwen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.05845</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Llaga: Large language and graph assistant</title>
		<author>
			<persName><forename type="first">Runjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Kumar Jaiswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Label-free node classification on graphs with large language models (llms)</title>
		<author>
			<persName><forename type="first">Zhikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Selection-inference: Exploiting large language models for interpretable logical reasoning</title>
		<author>
			<persName><forename type="first">Antonia</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murray</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reactome: a database of reactions, pathways and biological processes</title>
		<author>
			<persName><forename type="first">David</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Gavin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanming</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Haw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Gillespie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phani</forename><surname>Caudy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gopal</forename><surname>Garapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bijay</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName><surname>Jassal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">suppl_1</biblScope>
			<biblScope unit="page" from="D691" to="D697" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Individual differences in working memory and reading</title>
		<author>
			<persName><forename type="first">Meredyth</forename><surname>Daneman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><forename type="middle">A</forename><surname>Carpenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of verbal learning and verbal behavior</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="450" to="466" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Talk like a graph: Encoding graphs for large language models</title>
		<author>
			<persName><forename type="first">Bahare</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Halcrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.04560</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<ptr target="https://github.com/gkamradt/LLMTest_NeedleInAHaystack" />
		<title level="m">Llmtest needle in a haystack -pressure testing llms</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>gkamradt</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking</title>
		<author>
			<persName><forename type="first">Jiayan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengyu</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.15066</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Explanations as features: Llm-based features for text-attributed graphs</title>
		<author>
			<persName><forename type="first">Xiaoxin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Homophily and differential returns: Sex differences in network structure and access in an advertising firm</title>
		<author>
			<persName><forename type="first">Herminia</forename><surname>Ibarra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Administrative science quarterly</title>
		<imprint>
			<biblScope unit="page" from="422" to="447" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Structgpt: A general framework for large language model to reason over structured data</title>
		<author>
			<persName><forename type="first">Jinhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zican</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<editor>
			<persName><forename type="first">Houda</forename><surname>Bouamor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kalika</forename><surname>Bali</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Large language models on graphs: A comprehensive survey</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Bowen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.02783</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Interpersonal networks in organizations: Cognition, personality, dynamics, and culture</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Kilduff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Krackhardt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Kuratov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aydar</forename><surname>Bulatov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Anokhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artyom</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Burtsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.10790</idno>
		<title level="m">In search of needles in a 10m haystack: Recurrent memory finds what llms miss</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to discover social circles in ego networks</title>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distance encoding: Design provably more powerful neural networks for graph representation learning</title>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4465" to="4478" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">One for all: Towards training one graph model for all classification tasks</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiarui</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lecheng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyue</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.00149</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">One for all: Towards training one graph model for all classification tasks</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiarui</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lecheng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyue</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Towards graph foundation models: A survey and beyond</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengmei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.11829</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Language models of code are few-shot commonsense learners</title>
		<author>
			<persName><forename type="first">Aman</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1384" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Graph foundation models</title>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzhuo</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.02216</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Network motifs: simple building blocks of complex networks</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shen-Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shalev</forename><surname>Itzkovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Kashtan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Chklovskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">298</biblScope>
			<biblScope unit="issue">5594</biblScope>
			<biblScope unit="page" from="824" to="827" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A mechanistic model of human recall of social network structure and relationship affect</title>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Omodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Brashears</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Arenas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">17133</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Integrating graphs with large language models: Methods and prospects</title>
		<author>
			<persName><forename type="first">Yizhen</forename><surname>Shirui Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Let your graph do the talking: Encoding structured data for llms</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bahare</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Tsitsulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehran</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Halcrow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.05862</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huayi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhirui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.07443</idno>
		<title level="m">Can large language models empower molecular property prediction?</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An introduction to exponential random graph (p*) models for social networks</title>
		<author>
			<persName><forename type="first">Garry</forename><surname>Robins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pip</forename><surname>Pattison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Kalish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><surname>Lusher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social networks</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="191" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Predicting the structure of a communications network from recalled data</title>
		<author>
			<persName><forename type="first">Romney</forename><surname>Kimball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Faust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="285" to="304" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Network recall among older adults with cognitive impairments</title>
		<author>
			<persName><forename type="first">Siyun</forename><surname>Adam R Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><forename type="middle">E</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brea</forename><forename type="middle">L</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName><surname>Perry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Networks</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="99" to="108" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Rozado</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.01789</idno>
		<title level="m">The political preferences of llms</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Understanding transformer reasoning capabilities via graph algorithms</title>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Sanford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bahare</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Tsitsulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehran</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Halcrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vahab</forename><surname>Mirrokni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.18512</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Power and the perception of social networks</title>
		<author>
			<persName><forename type="first">Brent</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Markovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Steketee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Networks</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="166" to="171" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Using strong triadic closure to characterize ties in social networks</title>
		<author>
			<persName><forename type="first">Stavros</forename><surname>Sintos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panayiotis</forename><surname>Tsaparas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1466" to="1475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Gemini: a family of highly capable multimodal models</title>
		<author>
			<persName><forename type="first">Gemini</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Schalkwyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anja</forename><surname>Hauth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.11805</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<title level="m">Llama 2: Open foundation and fine-tuned chat models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Can language models solve graph problems in natural language</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangbin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoxuan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochuang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 Conference on Neural Information Processing Systems</title>
		<meeting>the 2023 Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Can language models solve graph problems in natural language? In NeurIPS</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangbin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoxuan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochuang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Inductive representation learning in temporal networks via causal anonymous walks</title>
		<author>
			<persName><forename type="first">Yanbang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen-Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.05974</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">From graphs to hypergraphs: Hypergraph projection and its reconstruction</title>
		<author>
			<persName><forename type="first">Yanbang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">On the relationship between relevance and conflict in online social link recommendations</title>
		<author>
			<persName><forename type="first">Yanbang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Tedic: Neural modeling of behavioral patterns in dynamic social interaction networks</title>
		<author>
			<persName><forename type="first">Yanbang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference</title>
		<meeting>the Web Conference</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="693" to="705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Generic representation learning for dynamic social interaction</title>
		<author>
			<persName><forename type="first">Yanbang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Subrahmanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining Workshop</title>
		<meeting>26th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining Workshop</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Llmrec: Large language models with graph augmentation for recommendation</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xubin</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiabin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lixin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suqi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Schema-aware reference as prompt improves data-efficient knowledge graph construction</title>
		<author>
			<persName><forename type="first">Yunzhi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Revisiting graph neural networks and distance encoding from a practical view</title>
		<author>
			<persName><forename type="first">Yanbang</forename><surname>Haoteng Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12228</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Autoalign: Fully automatic and effective knowledge graph alignment enabled by large language models</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bayu</forename><surname>Distiawan Trisedya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianzhong</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijian</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.14522</idno>
		<title level="m">Large graph models: A perspective</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning</title>
		<author>
			<persName><forename type="first">Haiteng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<idno>34 51.58 47.48 52.57 Geological 57.47 56.18 51.41 68.15 50.09 54.05 50.41 50.06 48.72 51.28 44.97 63</idno>
	</analytic>
	<monogr>
		<title level="j">Facebook Social</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">34 71</biblScope>
			<biblScope unit="page" from="60" to="66" />
			<date type="published" when="2023">2023. 99 49.54 71.59 50.68 53.69 50.45 50.91 43</date>
		</imprint>
	</monogr>
	<note>bioRxiv. Reactome Social 56.32 54</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
