<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Machine Learning in the Era of Large Language Models (LLMs)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-06-04">4 Jun 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shijie</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiani</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhikai</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wenzhuo</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Haitao</forename><surname>Mao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">Graph Machine Learning in the Era of Large Language Models (LLMs)</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-06-04">4 Jun 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">5943A0D87A61E157ABCA5BB6847834B8</idno>
					<idno type="arXiv">arXiv:2404.14928v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-01-20T06:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Machine Learning</term>
					<term>Graph Foundation Models</term>
					<term>Graph Learning</term>
					<term>Large Language Models (LLMs)</term>
					<term>Pre-training and Fine-tuning</term>
					<term>Prompting</term>
					<term>Representation Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graphs play an important role in representing complex relationships in various domains like social networks, knowledge graphs, and molecular discovery. With the advent of deep learning, Graph Neural Networks (GNNs) have emerged as a cornerstone in Graph Machine Learning (Graph ML), facilitating the representation and processing of graph structures. Recently, LLMs have demonstrated unprecedented capabilities in language tasks and are widely adopted in a variety of applications such as computer vision and recommender systems. This remarkable success has also attracted interest in applying LLMs to the graph domain. Increasing efforts have been made to explore the potential of LLMs in advancing Graph ML's generalization, transferability, and few-shot learning ability. Meanwhile, graphs, especially knowledge graphs, are rich in reliable factual knowledge, which can be utilized to enhance the reasoning capabilities of LLMs and potentially alleviate their limitations such as hallucinations and the lack of explainability. Given the rapid progress of this research direction, a systematic review summarizing the latest advancements for Graph ML in the era of LLMs is necessary to provide an in-depth understanding to researchers and practitioners. Therefore, in this survey, we first review the recent developments in Graph ML. We then explore how LLMs can be utilized to enhance the quality of graph features, alleviate the reliance on labeled data, and address challenges such as graph heterogeneity and out-of-distribution (OOD) generalization. Afterward, we delve into how graphs can enhance LLMs, highlighting their abilities to enhance LLM pre-training and inference. Furthermore, we investigate various applications and discuss the potential future directions in this promising field.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>G Raph data are widespread in many real-world appli- cations <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, including social graphs, knowledge graphs, and recommender systems <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref>. Typically, graphs consist of nodes and edges, e.g., in a social graph, nodes represent users and edges represent relationships <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. In addition to the topological structure, graphs tend to possess various features of nodes, such as textual description, which provide valuable context and semantic information about nodes. To effectively model the graph, Graph Machine Learning (Graph ML) has garnered significant interest. With the advent of deep learning (DL), Graph Neural Networks (GNNs) have become a critical technique in Graph ML due to their message-passing mechanism. This mechanism allows each node to obtain its representation by recursively receiving and aggregating messages from neighboring nodes <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, thereby capturing the high-order relationships and dependencies within the graph structure. To mitigate the Figure <ref type="figure">1</ref>: Illustration of the application of Large Language Models (LLMs) in graph machine learning. The integration of LLMs with Graph Neural Networks (GNNs) is utilized to model an extensive range of graph data across various downstream tasks. reliance on supervised data, many research focused on developing self-supervised Graph ML methods to advance GNNs to capture transferable graph patterns, enhancing their generalization capabilities across various tasks <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b12">[13]</ref>. Given the exponential growth of applications of graph data, researchers are actively working to develop more powerful Graph ML methods.</p><p>Recently, Large Language Models (LLMs) have started a new trend of AI and have shown remarkable capabilities in natural language processing (NLP) <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. With the evolution of these models, LLMs are not only being applied   to language tasks but also showcasing great potentials in various applications such as CV <ref type="bibr" target="#b15">[16]</ref>, and Recommender System <ref type="bibr" target="#b16">[17]</ref>. The effectiveness of LLMs in complex tasks is attributed to their extensive scale in both architecture and dataset size. For example, GPT-3 with 175 billion parameters demonstrates exciting capabilities by generating human-like text, answering complex questions, and coding. Furthermore, LLMs are able to grasp extensive general knowledge and sophisticated reasoning due to their vast training datasets. Therefore, their abilities in linguistic semantics and knowledge reasoning enable them to learn semantic information. Additionally, LLMs exhibit emergence abilities, excelling in new tasks and domains with limited or no specific training. This attribute is expected to provide high generalisability across different downstream datasets and tasks even in few-shot or zero-shot situations. Therefore, leveraging the capabilities of LLMs in Graph Machine Learning (Graph ML) has gained increasing interest and is expected to enhance Graph ML towards Graph Foundation Models (GFMs) <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>.</p><p>GFMs are generally trained on extensive data and can be adapted for a wide range of downstream tasks <ref type="bibr" target="#b19">[20]</ref>. By exploiting the ability of LLMs, it is expected to enhance the ability of Graph ML to generalize a variety of tasks, thus facilitating GFMs. Currently, researchers have made several initial efforts to explore the potential of LLMs in advancing Graph ML towards GFMs. Figure <ref type="figure">1</ref> demonstrates an example of integrating LLMs and GNNs for various graph tasks. Firstly, some methods leverage LLMs to alleviate the reliance of vanilla Graph ML on labeled data, where they make inferences based on implicit and explicit graph structure information <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b22">[23]</ref>. For instance, InstructGLM <ref type="bibr" target="#b20">[21]</ref> finetunes models like LlaMA <ref type="bibr" target="#b23">[24]</ref> and T5 <ref type="bibr" target="#b24">[25]</ref> by serializing graph data as tokens and encoding structural information about the graph to solve graph tasks. Secondly, to overcome the challenge of feature quality, some methods further employ LLMs to enhance the quality of graph features <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b27">[28]</ref>. For example, SimTeG <ref type="bibr" target="#b25">[26]</ref> fine-tunes LLMs on textual graphs datasets to obtain textual attribute embeddings, which are then utilized to augment the GNN for various downstream tasks. Additionally, some studies explore using LLMs to address challenges such as heterogeneity <ref type="bibr" target="#b28">[29]</ref> and OOD <ref type="bibr" target="#b26">[27]</ref> of graphs.</p><p>On the other hand, although LLM achieves great success in various fields, it still faces several challenges, including hallucinations, actuality awareness, and lacking explainability <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b32">[33]</ref>. Graphs, especially knowledge graphs, capture extensive high-quality and reliable factual knowledge in a structured format <ref type="bibr" target="#b4">[5]</ref>. Therefore, incorporating graph structure into LLMs could improve the reasoning ability of LLMs and mitigate these limitations <ref type="bibr" target="#b33">[34]</ref>. To this end, efforts have been made to explore the potential of graphs in augmenting LLMs' explainability <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref> and mitigating hallucination <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>. Given the rapid evolution and significant potential of this field, a thorough review of recent advancements in graph applications and Graph ML in the era of LLMs is imperative.</p><p>Therefore, in this survey, we aim to provide a com-prehensive review of Graph Machine Learning in the era of LLMs. The outline of the survey is shown in Figure <ref type="figure" target="#fig_2">2</ref>: Section 2 reviews work related to graph machine learning and foundation models. Section 3 introduces the deep learning methods on graphs, which focus on various GNN models and self-supervised methods. Subsequently, the survey delves into how LLMs can be used to enhance Graph ML in Section 4 and how graphs can be adopted for augmenting LLMs in Section 5. Finally, some applications and potential future directions for Graph ML in the era of LLMs are discussed in Section 6 and Section 7, respectively. Our main contributions can be summarized as follows:</p><p>• We detail the evolution from early graph learning methods to the latest GFMs in the era of LLMs; • We provide a comprehensive analysis of current LLMs enhanced Graph ML methods, highlighting their advantages and limitations, and offering a systematic categorization;</p><p>• We thoroughly investigate the potential of graph structures to address the limitations of LLMs; • We explore the applications and prospective future directions of Graph ML in the era of LLMs, and discuss both research and practical applications in various fields. Concurrent to our survey, Wei et al. <ref type="bibr" target="#b38">[39]</ref> review the development of graph learning. Zhang et al. <ref type="bibr" target="#b39">[40]</ref> provide a prospective review of large graph models. Jin et al. <ref type="bibr" target="#b40">[41]</ref> and Li et al. <ref type="bibr" target="#b41">[42]</ref> review different techniques for pretraining language models (in particular LLMs) on graphs and applications to different types of graphs, respectively. Liu et al. <ref type="bibr" target="#b42">[43]</ref> review the Graph Foundation Models according to the pipelines. Mao et al. <ref type="bibr" target="#b18">[19]</ref> focus on the fundamental principles and discuss the potential of GFMs. Different from these concurrent surveys, our survey provides a more comprehensive review with the following differences: <ref type="bibr" target="#b0">(1)</ref> we present a more systematic review of the development of Graph Machine Learning and further exploration of LLMs for Graph ML towards GFMs; (2) we present a more comprehensive and fine-grained taxonomy of recent advancements of Graph ML in the era of LLMs; (3) we delve into the limitations of recent Graph ML, and provide insights into how to overcome these limitations from LLM's perspective; (4) we further explore how graphs can be used to augment LLMs; and (5) we thoroughly summarize a broad range of applications and present a more forward-looking discussion on the challenges and future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we briefly review some related works in the fields of graph machine learning and foundation model techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Machine Learning</head><p>As one of the most active fields in artificial intelligence, graph learning has attracted considerable attention to its capability to model complex relationships and structures in data represented as graphs <ref type="bibr" target="#b43">[44]</ref>. Nowadays, it has been widely adopted in various applications, including social network analysis <ref type="bibr" target="#b44">[45]</ref>, protein detection <ref type="bibr" target="#b45">[46]</ref>, recommender systems <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, etc.</p><p>The initial phases of graph learning typically use Random Walks, which is a foundational method for exploring graph structures. This technique involves a stochastic process of moving from one node to another within a graph, which is instrumental in understanding node connectivity and influence within networks. Building upon Random Walks, Graph Embedding methods aim to represent nodes (or edges) as low-dimensional vectors while preserving graph topology and node relationships. Representative methods such as LINE <ref type="bibr" target="#b48">[49]</ref>, DeepWalk <ref type="bibr" target="#b49">[50]</ref>, and Node2Vec <ref type="bibr" target="#b50">[51]</ref> leverage Random Walks to learn node representations, capturing local structures and community information effectively.</p><p>Due to the exceptional representation learning and modeling capabilities, GNNs bolstered by deep learning have brought significant advances in graph learning. For example, GCNs <ref type="bibr" target="#b51">[52]</ref> introduce convolutional operations to graph data, enabling effective aggregation of neighborhood information for each node, thus enhancing node representation learning. GraphSAGE <ref type="bibr" target="#b52">[53]</ref> learns a function to aggregate information from a node's local neighborhood in an inductive setting, allowing efficient embedding generation for unseen nodes. GAT <ref type="bibr" target="#b53">[54]</ref> further advances GNNs by integrating attention mechanisms, assigning varying weights to nodes in a neighborhood, thereby sharpening the model's ability to focus on significant nodes. Inspired by the success of transformers <ref type="bibr" target="#b54">[55]</ref> in NLP and CV, several studies <ref type="bibr" target="#b55">[56]</ref>- <ref type="bibr" target="#b59">[60]</ref> adopt self-attention mechanisms to graph data, providing a more global perspective of graph structures and interactions. Recent works <ref type="bibr" target="#b60">[61]</ref>- <ref type="bibr" target="#b64">[65]</ref> further leverage transformer architectures to enhance graph data modeling. For example, GraphFormer <ref type="bibr" target="#b60">[61]</ref> integrates GNN within each layer in the transformer, enabling simultaneous consideration of textual and graph information.</p><p>The advancements in LLMs have given rise to graph learning. Recent works <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b66">[67]</ref> apply techniques from these advanced language models like LLaMA <ref type="bibr" target="#b23">[24]</ref> or ChatGPT to graph data, resulting in models capable of understanding and handling graph structures in a manner similar to natural language processing. A typical approach, GraphGPT <ref type="bibr" target="#b22">[23]</ref>, tokenizes graph data for insertion into LLMs (i.e. Vicuna <ref type="bibr" target="#b67">[68]</ref> and LLaMA <ref type="bibr" target="#b23">[24]</ref>) thus providing a powerful generalization capability. GLEM <ref type="bibr" target="#b68">[69]</ref> further integrates the graph models and LLMs, specifically DeBERTa <ref type="bibr" target="#b69">[70]</ref>, within a variational Expectation-Maximization (EM) framework. It alternates between updating LLM and GNN in the E-step and M-step, thereby scaling efficiently and improving effectiveness in downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Foundation Models (FMs)</head><p>Foundation Models (FMs) represent a significant breakthrough in the field of artificial intelligence, characterized by their ability to be extensively pre-trained on large-scale datasets and adapted to a variety of downstream tasks. These models are distinguished by their extensive pre-training on large-scale datasets and their adaptability to a wide range of downstream tasks. It is worth noting that FMs are not limited to a single field and can be found in natural language <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, vision <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b71">[72]</ref>, and graph domains <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b42">[43]</ref>, serving as a promising research direction.</p><p>In the realm of vision, Visual Foundation Models (VFMs) have gained significant success, making substantial impacts on areas such as image recognition, object detection, and scene understanding. Specifically, VFMs benefit from pretraining on extensive and diverse image datasets, allowing them to learn intricate patterns and features. For instance, models such as DALL-E <ref type="bibr" target="#b72">[73]</ref>, and CLIP <ref type="bibr" target="#b70">[71]</ref> leverage selfsupervised learning to understand and generate images based on textual descriptions, demonstrating remarkable cross-modal understanding capabilities. Recent Visual Chat-GPT <ref type="bibr" target="#b71">[72]</ref> integrates ChatGPT with a series of Visual Foundation Models (VFMs), making it perform a variety of complex visual tasks. These VFMs allow models to learn from a broader range of visual data, thereby improving their generalizability and robustness.</p><p>In the sphere of Natural Language Processing (NLP), Large Language Models (LLMs) such as ChatGPT and LLaMA have also revolutionized the field <ref type="bibr" target="#b73">[74]</ref>. Characterized by their extensive scale, LLMs are trained on billions of parameters using extensive textual datasets, which enable them to excel in comprehending and generating natural language. The landscape of pre-trained language models is diverse, such as GPT (Generative Pre-trained Transformer) <ref type="bibr" target="#b13">[14]</ref>, BERT (Bidirectional Encoder Representations from Transformers) <ref type="bibr" target="#b14">[15]</ref> and T5 (Text-To-Text Transfer Transformer) <ref type="bibr" target="#b24">[25]</ref>. These models can broadly fall into three categories: encoder-only, decoder-only, and encoder-decoder models. Encoder-only models, such as BERT, specialize in understanding and interpreting language. In contrast, decoder-only models like GPT excel in generating coherent and contextually relevant text. Encoder-decoder models, like T5, combine both abilities, efficiently performing various NLP tasks from translation to summarization.</p><p>As an encoder-only model, BERT introduces a paradigm in NLP with its innovative bi-directional attention mechanism, which analyzes text from both directions simultaneously, unlike its predecessors like transformer which processed text in a single direction (either left-to-right or right-to-left). This feature allows BERT to attain a comprehensive context understanding, significantly improving its language nuance comprehension. On the other hand, decoder-only models such as GPT, including variants like ChatGPT, utilize a unidirectional self-attention mechanism. This design makes them particularly effective in predicting subsequent words in a sequence, thus excelling in tasks like text completion, creative writing, language translation, and code generation <ref type="bibr" target="#b74">[75]</ref>. Additionally, as an encoder-decoder model, T5 uniquely transforms a variety of NLP tasks as text generation problems. For example, it reframes sentiment analysis from a classification task to a text generation task, where input like "Sentiment: Today is sunny" would prompt T5 to generate an output such as "Positive". This text-to-text approach underscores T5's versatility and adaptability across diverse language tasks.</p><p>The evolution of LLMs has seen the emergence of advanced models like GPT-3 <ref type="bibr" target="#b96">[97]</ref>, LaMDA <ref type="bibr" target="#b97">[98]</ref>, PaLM <ref type="bibr" target="#b98">[99]</ref>, and Vicuna <ref type="bibr" target="#b67">[68]</ref>. These models represent significant advances in NLP, distinguished by their enhanced capabilities in comprehending and generating complex, fine-grained language. Their training methods are usually more sophisticated, involving larger datasets and more powerful computational resources. This scaling up has led to unprecedented language understanding and generation capabilities, exhibiting emergent properties such as in-context learning (ICL), adaptability, and flexibility. Furthermore, recent advancements demonstrate the successful integration of LLMs with other models, like recommender system <ref type="bibr" target="#b16">[17]</ref>, reinforcement learning (RL) <ref type="bibr" target="#b99">[100]</ref>, GNNs <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b100">[101]</ref>- <ref type="bibr" target="#b102">[103]</ref>. This integration enables LLMs to tackle both traditional and novel challenges, proposing prospective avenues for applications.</p><p>LLMs have found applications in diverse sectors like chemistry <ref type="bibr" target="#b103">[104]</ref>, <ref type="bibr" target="#b104">[105]</ref>, education <ref type="bibr" target="#b105">[106]</ref>, <ref type="bibr" target="#b106">[107]</ref>, and finance <ref type="bibr" target="#b107">[108]</ref>, <ref type="bibr" target="#b108">[109]</ref>. In these fields, they contribute to various tasks from data analysis to personalized learning. Particularly, LLMs exhibit great potential in graph tasks such as graph classification and link prediction, demonstrating their versatility and broad applicability. Specifically, several studies like Simteg <ref type="bibr" target="#b25">[26]</ref>, GraD <ref type="bibr" target="#b101">[102]</ref>, Graph-Toolformer <ref type="bibr" target="#b100">[101]</ref>, Graph CoT <ref type="bibr" target="#b109">[110]</ref>, and Graphologue <ref type="bibr" target="#b102">[103]</ref> have notably advanced graph learning. These models utilize LLMs for textual graph learning, graph-aware distillation, and graph reasoning, illustrating the potential of LLMs in enhancing the understanding of and interaction with complex graph structures.</p><p>Although FMs have revolutionized Vision and NLP domains, the development of Graph Foundation Models (GFMs) is still in the nascent stages. With the rapid evolution and significant potential of this field, it is imperative to continue exploring and developing advanced techniques that can further enhance Graph ML towards GFMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DEEP LEARNING ON GRAPHS</head><p>With the rapid development of deep neural networks (DNNs), GNN techniques modeling graph structure and node attributes for representation learning have been widely explored and have become one key technology in Graph ML. While vanilla GNNs demonstrate proficiency in various graph tasks, they still encounter several challenges such as scalability, generalization to unseen data, and limited capability in capturing complex graph structures. To overcome these limitations, many efforts have been made to improve GNN with the self-supervised paradigm. Therefore, to provide a comprehensive review of these methods, in this section, we first introduce the backbone architecture, including GNN-based models and graph transformer-based models. After that, we explore two important aspects of self-supervised graph ML models: graph pretext tasks and downstream adaptation. Note that a comprehensive summary of these methods is presented in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Backbone Architecture</head><p>As one of the most active fields in the artificial intelligence (AI) community, various GNN methods have been proposed to solve various tasks. The powerful capability of these models is largely dependent on the development of their backbone architectures. Therefore, in this subsection, we focus on two broadly used architectures: neighborhood aggregation-based models and graph transformer-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Neighborhood Aggregation-based Model</head><p>Neighborhood aggregation-based models are the most popular graph learning architectures that have been extensively studied and applied in various downstream  <ref type="bibr" target="#b10">[11]</ref> GNN Contrastive Learning Fine-tuning Graph GCC <ref type="bibr" target="#b12">[13]</ref> GNN Contrastive Learning URL&amp;Fine-tuning Node, Graph G-BERT <ref type="bibr" target="#b79">[80]</ref> BERT Graph Generation Fine-tuning Recommendation AdapterGNN <ref type="bibr" target="#b80">[81]</ref> GNN Multi-task Fine-tuning Graph GROVER <ref type="bibr" target="#b81">[82]</ref> Graph Transformer Property Prediction Fine-tuning Graph Graph-Bert <ref type="bibr" target="#b59">[60]</ref> Graph Transformer Graph Generation URL&amp;Fine-tuning Node G-Adapter <ref type="bibr" target="#b82">[83]</ref> Graph Transformer Multi-task Fine-tuning Graph GraphGPT <ref type="bibr" target="#b83">[84]</ref> Graph Transformer Graph Generation Fine-tuning Node, Edge, Graph MoMu <ref type="bibr" target="#b84">[85]</ref> BERT, GNN Contrastive Learning Fine-tuning Graph TOUCHUP-G <ref type="bibr" target="#b85">[86]</ref> BERT, ViT, GNN tasks. These models operate based on the message-passing mechanism <ref type="bibr" target="#b110">[111]</ref>, which updates a node's representation by aggregating the features of its neighboring nodes along with its own features. Formally, this process can be represented as:</p><formula xml:id="formula_0">m u = Aggregate(f v , v ∈ N u ),<label>(1)</label></formula><formula xml:id="formula_1">f ′ u = U pdate(m u , f u ),<label>(2)</label></formula><p>where, for each node u, a message m u is generated through the aggregation function from its neighboring nodes. Subsequently, the graph signal f is updated with the message. GCN is a typical method designed to leverage both the graph structure and the node attributes. This architecture updates node representations by aggregating neighboring features with the node's own. As the number of network layers increases, each captures an increasingly larger neighborhood. Owing to the efficiency and performance, GCN <ref type="bibr" target="#b51">[52]</ref> has been widely applied by several methods such as CSSL <ref type="bibr" target="#b10">[11]</ref> and PRODIGY <ref type="bibr" target="#b93">[94]</ref>. GraphSAGE <ref type="bibr" target="#b52">[53]</ref> is another notable neighborhood aggregation-based model. Due to its inductive paradigms, GraphSAGE can easily generalize to unseen nodes or graphs, making it widely employed by many studies such as PinSage <ref type="bibr" target="#b111">[112]</ref> for inductive learning. Additionally, several studies <ref type="bibr" target="#b77">[78]</ref>, <ref type="bibr" target="#b90">[91]</ref>, <ref type="bibr" target="#b93">[94]</ref> incorporate Graph Attention Networks (GATs) <ref type="bibr" target="#b53">[54]</ref> as the backbone architecture. GATs integrate attention mechanisms into GNNs, assigning variable weights to neighboring nodes, thereby focusing on the most relevant parts of the input graph for improved node representations. As another important model in the family of GNNs, Graph Isomorphism Network (GIN) <ref type="bibr" target="#b112">[113]</ref> has also been widely used <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b86">[87]</ref>, <ref type="bibr" target="#b94">[95]</ref>, due to its powerful representation ability. Its unique architecture guarantees the expressiveness equivalent to the Weisfeiler-Lehman isomorphism test, making it widely chosen as the backbone model for a lot of structure-intensive tasks.</p><p>Although these models are widely adopted to solve graph tasks, they still suffer from some inherent limitations, such as over-smoothing and lack of generalization. In addition, the lower amount of parameters also limits the modeling capacity as the backbone model to serve multiple datasets and tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Graph Transformer-based Model</head><p>While neighborhood aggregation-based GNN models have shown remarkable performance in processing graphstructured data, they suffer from some limitations. A significant challenge for these models is their difficulty in handling large graphs due to their reliance on local neighborhood information and their limited capacity in capturing long-range dependencies within the graph <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b113">[114]</ref>, <ref type="bibr" target="#b114">[115]</ref>. To overcome these problems, inspired by the success of the transformer model in various NLP tasks, graph transformer-based models have been proposed <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b63">[64]</ref>. These models leverage the self-attention mechanism to adaptly capture both local and global graph structures, which allows the model to stack multiple layers without over-smoothing. Due to the lower inductive bias, graph transformer-based models can learn the structural patterns from data rather than solely relying on the graph structure. Additionally, transformers have demonstrated great scaling behavior in CV and NLP, suggesting that their performance can keep improving with more data and parameters.</p><p>Graph transformer-based models have been widely applied as a backbone architecture in various tasks <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b81">[82]</ref>, <ref type="bibr" target="#b82">[83]</ref>, <ref type="bibr" target="#b95">[96]</ref>, <ref type="bibr" target="#b115">[116]</ref>. For example, Heterformer <ref type="bibr" target="#b116">[117]</ref> introduces a graph-empowered Transformers architecture by adding neighbor tokens into each language Transformer layer. Edgeformers <ref type="bibr" target="#b117">[118]</ref> propose to encode text and structure inside each Transformer layer jointly. Graph-Bert <ref type="bibr" target="#b59">[60]</ref> employs a transformer to pre-train on the graph dataset with feature and edge reconstruction tasks and then finetunes for various downstream tasks. Similarly, GROVER <ref type="bibr" target="#b81">[82]</ref> introduces a self-supervised graph transformer-based model designed specifically for large-scale molecular data. It pretrains on extensive molecular datasets and then fine-tunes for specific downstream tasks. GraphGPT <ref type="bibr" target="#b83">[84]</ref> employs a (semi-)Eulerian path to transform the graph into a sequence of tokens, and then feeds the sequence into the transformer. Specifically, it constructs a dataset-specific vocabulary such that each node can correspond to a unique node ID.</p><p>Despite graph transformer-based models that can somehow address the limitations of traditional GNNs, they also face several challenges. One of the challenges is the quadratic complexity caused by self-attention, which becomes particularly problematic for large-scale graphs. In addition, there is a risk of losing some information about the original graph structure when serializing the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Self-Supervised Learning on Graphs</head><p>To adapt GNNs to various graph tasks, many self-supervised learning methods have been proposed and extensively studied. These approaches enable GNNs to learn graph representations from the pre-training task and transfer them to various downstream tasks, such as node classification, graph classification, and link prediction. Therefore, in this subsection, we will introduce graph self-supervised learning methods from pretext tasks and downstream adaptation, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Graph Pretext Tasks</head><p>Graph Contrastive Learning aims to learn augmentation representations by contrasting similar and dissimilar graph data pairs, effectively identifying nuanced relationships and structural patterns. We can review graph contrastive learning from two perspectives: graph augmentations and the scale of contrast.</p><p>Generally, graph augmentations can be broadly categorized into two types: 1) feature perturbation and 2) topology perturbation. They assume that tiny changes in the feature or structural space do not change the semantics of the Node/Edge/(sub)graph. Feature perturbation involves perturbing the features of the nodes in the graph. For example, GRACE <ref type="bibr" target="#b76">[77]</ref> randomly masks the node features to learn more robust representations. On the other hand, topology perturbation mainly involves modifying the structure of the graph. A typical example is CSSL <ref type="bibr" target="#b10">[11]</ref> which employs strategies like edge perturbation or node dropping to adopt graph-graph level contrast, thereby enhancing the robustness of representations.</p><p>Regarding the scale of contrast, the approaches can be divided into node-level and graph-level. For example, GRACE <ref type="bibr" target="#b76">[77]</ref> computes the similarities between node-level embeddings to learn discriminative node representations. GCC <ref type="bibr" target="#b12">[13]</ref> also works at the node level but learns local structural patterns by sampling a node's neighbors to obtain subgraphs (positive pairs) and contrasting them with randomly selected non-contextual subgraphs (negative pairs). In contrast, DGI <ref type="bibr" target="#b75">[76]</ref> contrasts node-level embeddings with graph-level embedding to capture global graph structures. GraphCL <ref type="bibr" target="#b9">[10]</ref> takes a different approach by implementing graph-to-graph level contrast, thereby learning robust representations. The scale used for pre-training has a huge impact on the downstream performance. When adopting contrastive learning as the pre-training task, one key challenge is how to design the objective such that the embeddings learned can account for downstream tasks of different scales. Graph Generation methods aim to learn the distribution of graph data to enable graph generation or reconstruction. In contrast to models in CV that predict masked image patches, or in NLP that predict the next token in a sequence, graph data presents a unique challenge due to its interconnected nature. Consequently, graph generation methods typically work on the feature or structural space. Feature generation methods focus on masking the features of one or a subset of nodes and then training the model to recover the masked features. For instance, GraphMAE <ref type="bibr" target="#b77">[78]</ref> utilizes a masked autoencoder framework to reconstruct masked graph portions based on their context, effectively capturing the underlying node semantics and their connection patterns. Alternatively, structure generation methods concentrate on training the model to recover the graph structure. The method GraphGPT <ref type="bibr" target="#b83">[84]</ref> encodes the graph into sequences of tokens and then employs a transformer decoder to predict the next token of the sequence to recover the connectivity of the graph. In addition, Graph-Bert <ref type="bibr" target="#b59">[60]</ref> is trained on both node attribute recovery and graph structure recovery tasks to ensure that the model captures local node attribute information while maintaining a global view of the graph structure. Graph Property Prediction methods gain guidance from the node-, edge-, and graph-level properties, which are inherently present in the graph data. These methods follow a training approach similar to supervised learning, as both utilize "sample-label" pairs for training. The key distinction lies in the origin of the labels: in supervised learning, labels are manually annotated by human experts which can be costly in real scenarios, whereas in property-based learning, the labels are automatically generated from the graph using some heuristics or algorithms. For example, GROVER <ref type="bibr" target="#b81">[82]</ref> utilizes professional software to extract the information on graph motifs as labels for classification. Similarly, <ref type="bibr" target="#b118">[119]</ref> leverage statistical properties of the graph for graph selfsupervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Downstream Adaptation</head><p>Unsupervised Representation Learning (URL) is a common method due to the scarcity of labeled data in the real world <ref type="bibr" target="#b75">[76]</ref>- <ref type="bibr" target="#b78">[79]</ref>. In URL, the pre-trained graph encoder is frozen and only a task-specific layer is learned during downstream tuning. The learned representations are then directly fed into decoders. This pattern allows URLs to be efficiently applied to downstream tasks. For example, DGI <ref type="bibr" target="#b75">[76]</ref> trains an encoder model to learn node representations within graph-structured. Node representations can then be used for downstream tasks. However, due to the gap between the pretext task and downstream tasks, URL can also lead to suboptimal performance. Fine-tuning is the default method to adapt a pre-trained model to a certain downstream task. As shown in Figure <ref type="figure" target="#fig_3">3</ref>, it adds a randomly initialized task header (e.g., a classifier) on top of the pre-trained model, and during fine-tuning, both the backbone model and the header are jointly trained <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b59">[60]</ref>. Compared with URL, fine-tuning provides more flexibility as it allows changes in the backbone parameters, and one can choose the layers to be tuned while keeping others fixed. Additionally, recent studies <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b80">[81]</ref>, <ref type="bibr" target="#b82">[83]</ref> further explore advanced graph fine-tuning methods that go beyond naive fine-tuning. For instance, AdapterGNN <ref type="bibr" target="#b80">[81]</ref> introduces two trainable adapters in parallel before and after the message passing. It freezes the GNN model during fine-tuning while only tuning the adapters, enabling parameter-efficient fine-tuning with minimal influence on the downstream performance. Prompt-tuning: "Pre-training &amp; fine-tuning" is prevalent in adapting pre-trained models to specific downstream tasks, but it overlooks the gap between pre-training and downstream tasks, potentially limiting generalization capabilities. Moreover, fine-tuning for different tasks also leads to significant time and computational costs. Inspired by recent advancements in NLP, several methods <ref type="bibr" target="#b86">[87]</ref>- <ref type="bibr" target="#b92">[93]</ref>, <ref type="bibr" target="#b94">[95]</ref>, <ref type="bibr" target="#b95">[96]</ref> have presented the potential of introducing prompts to adapt pre-trained models to specific tasks as illustrated in Figure <ref type="figure" target="#fig_3">3</ref>. Specifically, Prompt-tuning first unifies the downstream task with the pre-trained task into the same paradigm, followed by the introduction of learnable prompts for tuning. For example, GPPT <ref type="bibr" target="#b87">[88]</ref> first reframe node classification as link predictions. GraphPrompt <ref type="bibr" target="#b86">[87]</ref> further extends graph classification into link prediction. On the other hand, Prog <ref type="bibr" target="#b90">[91]</ref> unifies all the downstream tasks into subgraph classification. The inserting prompt including vectors <ref type="bibr" target="#b86">[87]</ref>, <ref type="bibr" target="#b87">[88]</ref>, <ref type="bibr" target="#b89">[90]</ref>, node <ref type="bibr" target="#b94">[95]</ref> and sub-graph <ref type="bibr" target="#b90">[91]</ref>. By inserting these prompts, the pre-trained parameters can be utilized in a way that aligns more closely with the requirements of the downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LLMS FOR GRAPH MODELS</head><p>Despite great potential, Graph ML based on the GNNs has its inherent limitations. Firstly, vanilla GNN models commonly demand labeled data for supervision, and obtaining these annotations can be resource-intensive in terms of time and cost. Secondly, real-world graphs often contain abundant textual information, which is crucial for downstream tasks. However, GNNs typically rely on shallow text embeddings for semantic extraction, thereby limiting their capacity to capture intricate semantics and text features. Moreover, the diversity of graphs presents challenges for GNN models in terms of generalization across diverse domains and tasks.</p><p>Recently, LLMs have achieved remarkable success in handling natural language, with exciting features like (1) conducting zero/few-shot predictions and (2) providing a unified feature space. These capabilities present a potential solution to address the above challenges faced by Graph ML and GFMs. Therefore, this section aims to investigate the contributions that current LLMs can make to enhance Graph ML's progress towards GFMs, while also examining their current limitations, as Figure <ref type="figure" target="#fig_5">4</ref> shows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Enhancing Feature Quality</head><p>Graphs encompass diverse attribute information, spanning text, images, audio, and other multi-modal modes. The semantics of these attributes play a crucial role in a range of downstream tasks. In comparison with earlier pre-trained models, LLMs stand out due to their substantial parameter volume and training on extensive datasets, endowing them with rich open-world knowledge. Consequently, researchers are exploring the potential of LLMs to improve feature quality and align feature space. This section delves into research endeavors aimed at leveraging LLMs to accomplish these goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Enhancing Feature Representation</head><p>Researchers utilize the powerful language understanding capabilities of LLMs to generate better representations for text attributes compared to traditional shallow text embeddings <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b119">[120]</ref>, <ref type="bibr" target="#b120">[121]</ref>. For example, Patton <ref type="bibr" target="#b153">[154]</ref> proposes to pre-train a language model on the target graph to obtain high-quality feature representation. METERN <ref type="bibr" target="#b154">[155]</ref> introduces a soft prompt-based method to learn node multiplex embeddings for different edge types with one language model encoder. Chen et al. <ref type="bibr" target="#b26">[27]</ref> utilize LLMs as text encoders and the GNN model as a predictor, validating the effectiveness of LLMs as an enhancer in node classification tasks. In LKPNR <ref type="bibr" target="#b119">[120]</ref>, an LK-Aug news encoder enhances the news recommender system by concatenating LLM embeddings with entity embeddings within the news text to obtain an enriched news representation. Several researchers explore fine-tuning LLMs to obtain text representations better suited for downstream graph tasks. SimTeG <ref type="bibr" target="#b25">[26]</ref> treats node classification and link prediction tasks as text classification and text similarity tasks, fine-tuning PLMs using LoRA <ref type="bibr" target="#b155">[156]</ref> on the TAG dataset. The fine-tuned PLMs are then used to generate embeddings for text attributes, followed by GNN training for downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Generating Augmented Information</head><p>Several studies investigate leveraging the generation capabilities and general knowledge of LLMs to generate augmented information from original textual attributes. TAPE <ref type="bibr" target="#b121">[122]</ref> Heterophily and Generalization Instruction Who is the key influencer in the network?</p><p>The key influencer in the network is Annie.  first leverages LLM to generate potential node labels and explanations, utilizing text attributes (such as title and abstract) as input. These labels and explanations generated by the LLM are regarded as augmented attributes. Subsequently, these augmented attributes are encoded by a fine-tuned language model (LM) and processed by a GNN model, which integrates the graph structure for making final predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Solving Vanilla GNN Training Limitations</head><note type="other">Enhancing Feature Quality Graph Attributes Social Network Answer Ignoring Structure</note><p>In contrast to TAPE, KEA <ref type="bibr" target="#b26">[27]</ref> does not directly predict node labels with LLM. Instead, LLM extracts terms mentioned in textual attributes and provides detailed descriptions of these terms.</p><p>In the domain of molecular property prediction, both LLM4Mol <ref type="bibr" target="#b65">[66]</ref> and GPT-MolBERTa <ref type="bibr" target="#b125">[126]</ref> adopt a similar approach, where LLMs generate interpretations for input Simplified Molecular-Input Line-Entry System (SMILES) notations as augmented attributes.</p><p>In the realm of recommender systems, several methods leverage LLMs to enhance the textual attributes of both users and items. LLM-Rec [125] enables LLMs to produce more detailed item descriptions by explicitly stating the recommendation intent within the prompt. RLMRec <ref type="bibr" target="#b122">[123]</ref> explores using LLM to enhance user preference. Specifically, the LLM receives user and item information as input, generates user preferences, potential types of users that the item may attract, and the reasoning process. LLMRec <ref type="bibr" target="#b123">[124]</ref> employs a similar approach to enhance item and user attributes in recommender systems. For instance, based on historical behavior information, LLM outputs user profiles like age, gender, country, language, and preferred or disliked genres. For item attributes, taking movie information such as title as input, LLM generates outputs such as movie director, country, and language.</p><p>In addition to generating augmented text attributes, researchers also employ LLMs to enhance graph topological structures by generating or refining nodes and edges. In ENG <ref type="bibr" target="#b126">[127]</ref>, LLM is employed to generate new nodes and their corresponding text attributes for each node category. To integrate the generated nodes into the original graph, the authors train an edge predictor using relations in the original dataset as supervised signals. Sun et al. <ref type="bibr" target="#b127">[128]</ref> leverage LLMs to refine graph structures. Specifically, they let LLMs remove unreliable edges by predicting the semantic similarity between node attributes. Additionally, they utilize pseudolabels generated by LLMs to aid the GNN in learning proper edge weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Aligning Feature Space</head><p>In real-world scenarios, the text attributes of graphs across different domains exhibit considerable diversity. Additionally, beyond text modal attributes, the graph may Table <ref type="table">2</ref>: A summary of LLM for Graph ML research. We present the GNN model, LLM model, predictor, domain, task, datasets, and project link. FT is Fine-tuning, refers to whether modifications are made to the parameters of the LLM model while PR is Prompting, involves inputting textual prompts to the LLM to obtain responses. In the context of task, "node" denotes node-level tasks such as node classification, "edge" signifies edge-level tasks like link prediction, "graph" represents graph-level tasks such as graph classification, and "structure" pertains to structure understanding tasks, such as node degree counting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Role</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Solving Vanilla GNN Training Limitations</head><p>The training of vanilla GNNs relies on labeled data. However, obtaining high-quality labeled data has long been associated with substantial time and costs. In contrast to GNNs, LLMs showcase robust zero/few-shot capabilities and possess expansive open-world knowledge. This unique characteristic empowers LLMs to directly leverage node information for prediction, without relying on extensive annotated data. Therefore, researchers have explored employing LLMs to generate annotations or predictions, alleviating dependence on human supervision signals in Graph ML. According to how structural information in graph data is processed, we categorize the methods into the following three categories:</p><p>• Ignoring structural information: utilize node attributes exclusively for constructing textual prompts, disregarding neighboring labels and relations.</p><p>• Implicit Structural information: describe neighbor information and graph topology structure in natural language; • Explicit Structural information: employ GNN models to encode graph structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Ignoring Structural Information</head><p>The fundamental distinction between graphs and text lies in the structural information inherent in graphs. Given that the LLM processes text as its input, an intuitive approach involves leveraging the textual attributes of the target node, disregarding the structural information within the graph, and making predictions directly. For instance, the work of <ref type="bibr" target="#b129">[130]</ref> explores the effectiveness of LLMs in solving graph tasks without using structure information. In the citation network, they employ the article's title and abstract to construct a prompt and instruct the LLM to predict the article's category. Since this kind of paradigm does not incorporate the structural information of the graph, the actual task performed by the LLM is text classification rather than a graph-related task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Implicit Structural Information</head><p>Researchers implicitly leverage structural information to solve graph tasks by describing graph structure in natural language. For example, Hu et al. <ref type="bibr" target="#b129">[130]</ref> propose two kinds of methods for utilizing structural information. The first method involves directly inputting the data of all neighboring nodes into LLM, while the second method employs a retrievalbased prompt to guide the LLM to focus solely on relevant neighbor data. Similarly, Huang et al. <ref type="bibr" target="#b136">[137]</ref> employ an LLM to assign scores to neighboring nodes and subsequently choose high-scoring nodes as structural information. NLGraph <ref type="bibr" target="#b130">[131]</ref> introduces a Build-a-Graph prompting strategy to improve the LLM's understanding of graph structure. This strategy entails appending "Let's construct a graph with the nodes and edges first." after providing the graph data description. The work of <ref type="bibr" target="#b20">[21]</ref> introduces InstructGLM, which utilizes natural language for graph description and fine-tunes Flan-T5 through instruction tuning. They generate a set of 31 prompts by combining four configuration parameters: task type, inclusion of node features, maximum hop order, and utilization of node connections. Notably, maximum hop order and node connections implicitly convey graph structure information to the LLM. GraphEdit <ref type="bibr" target="#b140">[141]</ref> leverages LLMs to understand graph structure and refine it by removing noisy edges and uncovering implicit node connections. Specifically, it employs an edge predictor to identify the top k candidate edges for each node, and these candidate edges, along with the original edges of the graph, are then fed into the LLM. The LLM is prompted to determine which edges should be integrated into the final graph structure.</p><p>In addition to employing natural language expression, several researchers leverage structured languages for graph description. GPT4Graph <ref type="bibr" target="#b21">[22]</ref>, for instance, utilizes Graph Modelling Language <ref type="bibr" target="#b156">[157]</ref> and Graph Markup Language <ref type="bibr" target="#b157">[158]</ref> to represent graph structure in XML format. GraphText <ref type="bibr" target="#b28">[29]</ref> constructs a graph syntax tree for each graph, containing node attributes and relations information. By traversing this tree, structural graph-text sequences can be generated. The advantage of GraphText lies in the ability to integrate the typical inductive bias of GNNs through the construction of various graph syntax trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Explicit Structural Information</head><p>While implicitly describing structure in natural language has achieved preliminary success, these methods still face certain limitations. Firstly, due to the constraint of input length, LLMs can only get local structural information, and lengthy contexts might diminish their reasoning <ref type="bibr" target="#b158">[159]</ref> and instruction-following abilities <ref type="bibr" target="#b26">[27]</ref>. Secondly, for different tasks and datasets, substantial effort is often required for prompt engineering. A prompt that performs well on one dataset may not generalize effectively to others, resulting in a lack of robustness. Consequently, researchers investigate representing graph structure explicitly, typically comprising three essential modules: encoding module, fusion module, and LLM module. More specifically, the encoding module aims to process the graph-structured and textual information, generating graph embeddings and text embeddings, respectively. Afterward, the fusion module takes these two embeddings as input, producing a modality fusion embedding. At last, the modality fusion embedding, which contains both graph information and instruction information, is fed into the LLM to obtain the final answer. Given the research focus is on how LLMs explicitly utilize graph structure information, we will delve into the encoding and fusion modules of various studies in detail, without primarily focusing on the LLM model itself. Encoding Module. The encoding module is responsible for both graph and text encoding, and we will provide separate summaries for each.</p><p>• Graph Encoding. Pre-trained GNN models are commonly used for graph encoding. For instance, GIT-Mol <ref type="bibr" target="#b146">[147]</ref> employs the GIN model from the pre-trained MoMu model <ref type="bibr" target="#b84">[85]</ref> to encode molecular graphs. KoPA <ref type="bibr" target="#b144">[145]</ref> utilizes the pre-trained RotateE model to obtain embeddings for entities and relations in the knowledge graph. In addition, GIMLET <ref type="bibr" target="#b145">[146]</ref> presents a unified graph-text model without the need for additional graph encoding modules. Particularly, GIMLET proposes a distance-based joint position embedding method, where the shortest graph distance is utilized to represent the relative positions between graph nodes, enabling the Transformer encoder to encode both graph and text. GraphToken <ref type="bibr" target="#b151">[152]</ref> evaluates a series of GNN models as graph encoders, including GCN, MPNN <ref type="bibr" target="#b110">[111]</ref>, GIN, Graph Transformer, HGT <ref type="bibr" target="#b58">[59]</ref>, etc. • Text Encoding. Due to the tremendous capability of LLMs in understanding textual information, most existing methods, such as ProteinChat <ref type="bibr" target="#b148">[149]</ref> and DrugChat <ref type="bibr" target="#b143">[144]</ref>, directly employ LLMs as text encoders. In GraphLLM <ref type="bibr" target="#b141">[142]</ref>, the tokenizer and frozen embedding table of LLM are leveraged to obtain the representation of node text attributes, aligning with the downstream frozen LLM.</p><p>Fusion Module. The goal of the fusion module is to align the graph and text modalities, generating a fusion embedding as input for the LLM. To achieve the goal, a straightforward solution is to design a linear projection layer to directly transform the graph representation generated by GNN into an LLM-compatible soft prompt vector <ref type="bibr" target="#b143">[144]</ref>, <ref type="bibr" target="#b144">[145]</ref>, <ref type="bibr" target="#b147">[148]</ref>.</p><p>Additionally, inspired by BLIP2's Q-Former <ref type="bibr" target="#b159">[160]</ref>, <ref type="bibr" target="#b146">[147]</ref>  propose a GIT-Former, which aligns graph, image, and text with the target text modality using self-attention and crossattention mechanisms.</p><p>In addition to the above methods, G-Retriever is proposed to integrate both explicit and implicit structural information <ref type="bibr" target="#b150">[151]</ref>. To be specific, GAT is employed to encode the graph structure, while representing node and relationship details through textual prompts. To accommodate real-world graphs with larger scales, G-Retriever introduces a RAG module specifically designed for retrieving subgraphs relevant to user queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Heterophily and Generalization</head><p>Despite achieving promising performance in graph tasks, GNNs exhibit several shortcomings. A notable drawback involves the inadequacy of the neighbor information aggregation mechanism, especially when dealing with heterogeneous graphs. GNN performance notably diminishes when faced with instances where adjacent nodes lack similarity. Additionally, GNN encounters challenges in out-of-distribution (OOD) generalization, leading to a degradation in model performance on distributions beyond the training data. This challenge is particularly prevalent in practical applications, primarily due to the inherent difficulty of encompassing all possible graph structures within limited training data. Consequently, when GNNs infer on unseen graph structures, their performance may experience a substantial decline. This reduced generalization capability renders GNNs relatively fragile when confronted with evolving graph data in real-world scenarios. For example, GNNs may encounter difficulties handling newly emergent social relationships in social networks.</p><p>LLMs have been utilized to mitigate the above limitations. In particular, GraphText <ref type="bibr" target="#b28">[29]</ref> effectively decouples depth and scope by encapsulating node attributes and relationships in the graph syntax tree. This approach yields superior results compared to the GNN baseline, particularly on heterogeneous graphs. Chen et al. <ref type="bibr" target="#b26">[27]</ref> investigate the LLM's ability to handle OOD generalization scenarios. They utilize the GOOD <ref type="bibr" target="#b160">[161]</ref> benchmark as the criterion and results demonstrate that LLMs exhibit promising performances in addressing OOD generalization issues. OpenGraph <ref type="bibr" target="#b152">[153]</ref> aims at solving zero-shot graph tasks across different domains. In this model, LLMs are leveraged to generate synthetic graphs of data scarcity scenarios, thereby enhancing the pre-training process of OpenGraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">GRAPHS FOR LLMS</head><p>LLMs have demonstrated impressive language generation and understanding capabilities across various domains. Nevertheless, they still face several pressing challenges, including factuality awareness, hallucinations, limited explainability in the reasoning process, and beyond. To alleviate these issues, one potential approach is to take advantage of the Knowledge Graphs (KGs), which store high-quality, humancurated factual knowledge in a structured format <ref type="bibr" target="#b4">[5]</ref>. Recent reviews <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b161">[162]</ref>- <ref type="bibr" target="#b163">[164]</ref> have summarized the research on using KGs to enhance LMs. Hu et al. <ref type="bibr" target="#b161">[162]</ref> present a review on knowledge-enhanced pre-training language models for natural language understanding and natural language generation. Agrawal et al. <ref type="bibr" target="#b162">[163]</ref> systematically review research on mitigating hallucination in LLMs by leveraging KGs across three dimensions: inference process, learning algorithm, and answer validation. Pan et al. <ref type="bibr" target="#b163">[164]</ref> provides a comprehensive summary of the integration of KGs and LLMs from three distinct perspectives: KG-enhanced LLMs, LLM-augmented KGs, and the synergized LLMs and KGs, where LLMs and KGs mutually reinforce each other. In this section, we will delve into relevant research that explores the usage of KGs to achieve knowledge-enhanced language model pre-training, mitigate hallucinations, and improve inference explainability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">KG-enhanced LLM Pre-training</head><p>While LLMs excel in text understanding and generation, they may still produce grammatically accurate yet factually incorrect information. Explicitly incorporating knowledge from KGs during LLM pre-training holds promise for augmenting LLM's learning capacity and factual awareness <ref type="bibr" target="#b164">[165]</ref>- <ref type="bibr" target="#b166">[167]</ref>.</p><p>In this subsection, we will outline the research advancements in KG-enhanced pre-trained language models (PLMs). While there is limited work on KG-enhanced pre-training for LLMs, research on KG-enhanced PLMs can offer insights for LLM pretraining. Existing KG-enhanced pre-training methods can be classified into three main categories: modifying input data, modifying model structures, and modifying pre-training tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Modifying Input Data</head><p>Several researchers investigate integrating KG knowledge by modifying input data while keeping the model architecture unchanged. For instance, Moiseev et al. <ref type="bibr" target="#b167">[168]</ref> directly train PLMs on mixed corpora consisting of factual triples from KGs and natural language texts. E-BERT <ref type="bibr" target="#b168">[169]</ref> aligns entity vectors with BERT's wordpiece vector space, preserving the structure and refraining from additional pre-training tasks. KALM <ref type="bibr" target="#b169">[170]</ref> utilizes an entity-name dictionary to identify entities within sentences and employs an entity tokenizer to tokenize them. The input of the Transformer consists of the original word embeddings and entity embeddings. Moreover, K-BERT <ref type="bibr" target="#b170">[171]</ref> integrates the original sentence with relevant triples by constructing a sentence tree, where the trunk represents the original sentence and the branches represent the triples. To convert the sentence tree into model input, K-BERT introduces both a hard-position index and a softposition index within the embedding layer to differentiate between original tokens and triple tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Modifying Model Structures</head><p>Some research designs knowledge-specific encoders or fusion modules to better inject knowledge into PLMs. ERNIE <ref type="bibr" target="#b171">[172]</ref> introduces a K-Encoder to inject knowledge into representations. This involves feeding token embeddings and the concatenation of token embeddings and entity embeddings into a fusion layer for generating new token embeddings and entity embeddings. In contrast, CokeBERT <ref type="bibr" target="#b172">[173]</ref> extends this approach by incorporating relation information from KGs during pre-training. It introduces a semantic-driven GNN model to assign relevant scores to relations and entities based on the given text. Finally, it fuses the selected relations and entities with text using a K-Encoder similar to ERNIE. KLMO <ref type="bibr" target="#b173">[174]</ref> propose Knowledge Aggregator to fuse text modality and KG modality during pre-training. To incorporate the structural information in KG embeddings, KLMO utilizes KG attention, which integrates a visibility matrix with a conventional attention mechanism, facilitating interaction among adjacent entities and relations within the KG. Subsequently, the token embeddings and contextual KG embeddings are aggregated with entity-level cross-KG attention.</p><p>Several studies refrain from modifying the overall structures of the language model but introduce additional adapters to inject knowledge. To preserve the original knowledge within PLMs, Wang et al. <ref type="bibr" target="#b174">[175]</ref> propose K-Adapter as a pluggable module to leverage KG knowledge. During pre-training, the parameters of the K-Adapter are updated while the parameters of the PLMs remain frozen. KALA <ref type="bibr" target="#b175">[176]</ref> introduces a Knowledge-conditioned Feature Modulation layer, which functions similarly to an adapter module, by scaling and shifting the intermediate hidden representations of PLMs with retrieved knowledge representations. To further control the activation levels of adapters, DAKI <ref type="bibr" target="#b176">[177]</ref> incorporates an attention-based knowledge controller module, which is an adapter module with additional linear layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Modifying Pre-training Tasks</head><p>To explicitly model the interactions between text and KG knowledge, various pre-training tasks are proposed. Three major lines of work in this direction include entity-centric tasks <ref type="bibr" target="#b171">[172]</ref>, <ref type="bibr" target="#b177">[178]</ref>- <ref type="bibr" target="#b180">[181]</ref>, relation-centric tasks <ref type="bibr" target="#b164">[165]</ref>, and beyond.</p><p>For entity-centric tasks, ERNIE <ref type="bibr" target="#b171">[172]</ref> randomly masks some token-entity alignments and then requires the model to predict all corresponding entities based on aligned tokens. LUKE <ref type="bibr" target="#b177">[178]</ref> uses Wikipedia articles as training corpora and treats hyperlinks within them as entity annotations, training the model to predict randomly masked entities. KILM <ref type="bibr" target="#b178">[179]</ref> also utilizes hyperlinks in Wikipedia articles as entities. However, it inserts entity descriptions after corresponding entities, tasking the model with reconstructing the masked description tokens rather than directly masking entities. In addition to predicting masked entities, GLM <ref type="bibr" target="#b179">[180]</ref> further introduces a distractor-suppressed ranking task. This task leverages negative entity samples from KGs as distractors, enhancing the model's ability to distinguish various entities.</p><p>Relation-centric tasks are also commonly utilized in KGenhanced PLMs. For instance, JAKET <ref type="bibr" target="#b181">[182]</ref> proposes relation prediction and entity category prediction tasks for enhancing knowledge modeling. Dragon <ref type="bibr" target="#b182">[183]</ref> is pre-trained in a KG link prediction task. Given a text-KG pair, the model needs to predict the masked relations in KG and the masked tokens in the sentence. ERICA <ref type="bibr" target="#b183">[184]</ref> introduces a relation discrimination task aiming at semantically distinguishing the proximity between two relations. Specifically, it adopts a contrastive learning manner, wherein the relation representations of entity pairs belonging to the same relations are encouraged to be closer.</p><p>Additionally, there are several innovative pre-training tasks for KG-enhanced pre-training. KEPLER <ref type="bibr" target="#b184">[185]</ref> proposes a knowledge embedding task to enhance knowledgeawareness of PLMs. Specifically, it uses PLMs to encode entity descriptions as entity embeddings and jointly train the knowledge embedding and masked language modeling tasks on the same PLM. ERNIE 2.0 <ref type="bibr" target="#b185">[186]</ref> constructs a series of continuous pre-training tasks from word, structure, and semantic perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">KG-enhanced LLM Inference</head><p>Knowledge within KGs can be dynamically updated, whereas updating the knowledge in LLMs often necessitates adjustment of model parameters, which demands substantial computational resources and time. Therefore, many studies opt to utilize KGs during LLMs inference stage. The "black-box" nature of LLMs poses a significant challenge in understanding how the model made a specific prediction or generated a specific text. Additionally, LLMs have often been criticized for generating false, erroneous, or misleading content, typically referred to as hallucination <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b186">[187]</ref>. Given the structured and fact-based nature of KGs, integrating them during the inference stage can enhance the explainability of LLM answers and consequently mitigate hallucinations.</p><p>While several methods extract relevant triples from KGs based on user queries and describe these triples in natural language within prompts <ref type="bibr" target="#b187">[188]</ref>, <ref type="bibr" target="#b188">[189]</ref>, these approaches overlook the structured information inherent in KGs, and still fail to elucidate how LLMs arrive at their answers. Consequently, extensive studies utilize KGs to aid LLMs in reasoning and generate intermediary information like relation paths, evidence subgraphs, and rationales, forming the basis for explaining the LLM's decision-making process and checking for hallucinations <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b189">[190]</ref>- <ref type="bibr" target="#b191">[192]</ref>.</p><p>Several researchers investigate enabling LLMs to directly reason on KGs and generate relation paths to interpret LLM's answers. The reasoning path at each step helps to enhance the explainability of the answer and the transparency of the reasoning process. Through observing the reasoning decisions made at each step, it becomes possible to identify and address hallucinations arising from LLMs' reasoning. Both RoG <ref type="bibr" target="#b34">[35]</ref>, Knowledge Solver <ref type="bibr" target="#b190">[191]</ref>, and Keqing <ref type="bibr" target="#b35">[36]</ref> employ relation paths as explanations for LLM's responses. Specifically, given the KG schema and user query, RoG <ref type="bibr" target="#b34">[35]</ref> guides LLMs to predict multiple relation paths using textual prompts like "Please generate helpful relation paths for answering the question". Subsequently, LLMs generate the final answer based on the retrieving results of the valid relation path. Conversely, the Knowledge Solver method <ref type="bibr" target="#b190">[191]</ref> differs in that it enables LLMs to generate the relation path step by step. Keqing <ref type="bibr" target="#b35">[36]</ref> initially decomposes complex questions into several sub-questions, each of which can be addressed by pre-defined logical chains on KGs, and then LLMs will generate final answers with relation paths based on the answers of sub-questions. Mindmap <ref type="bibr" target="#b189">[190]</ref> uses evident subgraphs to explain the answers generated by LLMs, where path-based and neighbor-based methods are introduced to obtain several evident subgraphs. The LLM in Mindmap is prompted to merge these evident subgraphs, utilizing the merged graph to generate the final answer. In contrast to previous methods which involve gradually retrieving knowledge and obtaining answers, KGR <ref type="bibr" target="#b36">[37]</ref> takes a different approach. Initially, the LLM directly generates a draft answer. Subsequently, it extracts the claims requiring verification from this answer and retrieves KG's information to correct claims with hallucinations. Based on the corrected claims, the LLM adjusts the draft answer to get the final answer.</p><p>The above research employs relation paths or evident graphs as the basis for explaining the LLM's decision-making process and checking hallucinations. In contrast, several research explore using inherently interpretable models rather than LLMs to make final predictions. ChatGraph <ref type="bibr" target="#b192">[193]</ref> presents an innovative approach to enhance both the text classification capabilities and explainability of ChatGPT. It utilizes ChatGPT to extract triples from unstructured text and subsequently constructs KGs based on these triples. To ensure the explainability of the classified results, ChatGraph avoids employing LLMs directly for predictions. Instead, it leverages a graph model without non-linear activation functions and trains the model on text graphs to get predictions. Given a question and a list of possible answers, XplainLLM <ref type="bibr" target="#b193">[194]</ref> propose an explainer model to explain why LLMs choose a particular answer while rejecting others. Specifically, the approach involves constructing an element graph based on the entities present in the question and the candidate answers. Subsequently, a GCN model is employed to assign attention scores to each node within the element graph. Nodes exhibiting high attention scores are identified as reason elements, and LLMs are then prompted to provide explanations based on these selected reason elements.</p><p>To assess the transparency and interpretability of LLMs, various benchmarks have been proposed. For example, Li et al. <ref type="bibr" target="#b37">[38]</ref> introduce a novel task named Knowledgeaware Language Model Attribution (KaLMA) and develop a corresponding benchmark dataset. This benchmark evaluates the LLM's capability to derive citation information from KG to support its answers. KaLMA also provides an automatic evaluation covering aspects such as text quality, citation quality, and text-citation alignment of the answers. In addition, XplainLLM <ref type="bibr" target="#b193">[194]</ref> introduces a dataset for better understanding LLMs' decision-making from the perspectives of "why-choose" and "why-not-choose".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">APPLICATIONS</head><p>In this section, we will present practical applications that demonstrate the potential and value of GFMs and LLMs. As shown in Table <ref type="table">2</ref>, recommender systems, knowledge graphs, AI for science, and robot task planning emerge as the most prevalent domains. We will provide a comprehensive summary of each of these applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Recommender Systems</head><p>Recommender systems leverage user historical behaviors to predict items that users are likely to appreciate <ref type="bibr" target="#b194">[195]</ref>- <ref type="bibr" target="#b196">[197]</ref>. Graphs play a crucial role in recommender systems, wherein items can be regarded as nodes and collaborative behaviors such as clicks and purchases can be viewed as edges. Recently, an increasing amount of research is exploring the use of LLMs for direct recommendation <ref type="bibr" target="#b197">[198]</ref>- <ref type="bibr" target="#b200">[201]</ref> or leveraging LLMs to enhance graph models or datasets for recommendation tasks <ref type="bibr" target="#b119">[120]</ref>, <ref type="bibr" target="#b122">[123]</ref>, <ref type="bibr" target="#b123">[124]</ref>, <ref type="bibr" target="#b201">[202]</ref>, <ref type="bibr" target="#b202">[203]</ref>.</p><p>For directly using LLMs as recommendation models, liu et al. <ref type="bibr" target="#b203">[204]</ref> construct task-specific prompts to evaluate ChatGPT's performance on five common recommendation tasks, encompassing rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization. Bao et al. <ref type="bibr" target="#b204">[205]</ref> employ prompt templates to guide LLM to decide whether the user will like the target item based on their historical interactions and perform instruction tuning on the LLM to improve its recommendation capability.</p><p>For using LLMs to enhance traditional recommendation methods or datasets, KAR <ref type="bibr" target="#b201">[202]</ref> leverages LLMs to generate factual knowledge of items and reasoning basis of user preferences; these knowledge texts are then encoded into vectors and integrated into existing recommendation models. Methods like LLM-Rec [125], RLMRec <ref type="bibr" target="#b122">[123]</ref>, and LLM-Rec <ref type="bibr" target="#b123">[124]</ref> enrich recommendation datasets by incorporating LLM-generated descriptions. In contrast, Wu et al. <ref type="bibr" target="#b202">[203]</ref> utilize LLMs to condense recommendation datasets, in which LLMs are employed to synthesize a condensed dataset for the content-based recommendation, aiming at addressing the challenge of resource-intensive training on large datasets.</p><p>While the previously discussed methods have explored utilizing LLMs for certain recommendation tasks or domains, an emerging research direction aims to develop foundation models for recommendation. Tang et al. <ref type="bibr" target="#b198">[199]</ref> propose an LLM-based domain-agnostic framework for sequential recommendation. Their approach integrates user behavior across domains, and leverages LLMs to model user behaviors based on multi-domain historical interactions and item titles. Hua et al. <ref type="bibr" target="#b205">[206]</ref> attempt to address the potential unfairness of recommender systems introduced by LLM bias. They propose a Counterfactually Fair Prompting method to develop an unbiased foundation model for recommendation. To summarize the progress in the area of recommendation foundation model, Huang et al. <ref type="bibr" target="#b206">[207]</ref> provide a systematic overview of the existing approaches, categorizing them into three main types: language foundation models, personalized agent foundation models, and multi-modal foundation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Knowledge Graphs</head><p>LLMs with robust text generation and language understanding capabilities have found extensive applications in KGrelated tasks, including KG completion <ref type="bibr" target="#b144">[145]</ref>, <ref type="bibr" target="#b207">[208]</ref>, <ref type="bibr" target="#b208">[209]</ref>, KG question answering <ref type="bibr" target="#b188">[189]</ref>, <ref type="bibr" target="#b190">[191]</ref>, <ref type="bibr" target="#b209">[210]</ref>- <ref type="bibr" target="#b211">[212]</ref>, KG reasoning <ref type="bibr" target="#b212">[213]</ref> and beyond. Meyer et al. <ref type="bibr" target="#b213">[214]</ref> introduce LLM-KG-Bench, a framework that automatically evaluates the model's proficiency in KG engineering tasks such as fixing errors in Turtle files, facts extraction, and dataset generation. KG-LLM <ref type="bibr" target="#b208">[209]</ref> is proposed to evaluate LLMs' performance on KG completion, including triple classification, relation prediction, and link prediction tasks. Kim et al. <ref type="bibr" target="#b209">[210]</ref> propose KG-GPT, using LLMs for complex reasoning tasks on knowledge graphs. ChatKBQA <ref type="bibr" target="#b210">[211]</ref> introduces a generate-then-retrieve framework for LLMs on knowledge base question answering. Wu et al. <ref type="bibr" target="#b188">[189]</ref> present a KG-enhanced LLM framework for KG question answering, which involves fine-tuning an LLM to convert structured triples into free-form text, enhancing LLMs' understanding of KG data. The successful application of LLMs in tasks such as KG construction, completion, and question answering offers robust support for advancing the understanding and exploration of KGs.</p><p>Drawing inspiration from foundation models in language and vision, researchers are delving into the development of foundation models tailored for KGs. These GFMs aim to generalize to any unseen relations and entities within KGs. Galkin et al. <ref type="bibr" target="#b214">[215]</ref> propose Ultra, which learns universal graph representations by leveraging interactions between relations. This study is based on the insight that those interactions remain similar and transferable across different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">AI for Science</head><p>The rapid advancement of AI has led to an increasing number of studies leveraging AI to assist scientific research <ref type="bibr" target="#b215">[216]</ref>, <ref type="bibr" target="#b216">[217]</ref>. Recent research has applied LLMs and GFMs for scientific purposes, such as drug discovery, molecular property prediction, and material design. Notably, these applications encompass scenarios involving graph-structured data.</p><p>The molecular graph is a way of representing molecules, where the nodes represent atoms, and the edges represent the bonds between the atoms. With the emergence of LLMs, researchers have explored their performance in tasks related to molecular graphs. Methods like MolReGPT <ref type="bibr" target="#b138">[139]</ref> and GPT-MolBERTa <ref type="bibr" target="#b125">[126]</ref> adopt a similar approach, converting molecular graphs into textual descriptions using SMILES language. They create prompts based on SMILES data, asking the LLM to provide detailed information about functional groups, shapes, chemical properties, etc. This information is then used to train a smaller LM for molecular property prediction. In contrast to methods directly using LLMs for prediction, ReLM <ref type="bibr" target="#b135">[136]</ref> first uses GNNs to predict highprobability candidate products, and then leverages LLMs to make the final selection from these candidates.</p><p>In addition to the above research, LLMs are further utilized in drug discovery and materials design. Bran et al. <ref type="bibr" target="#b104">[105]</ref> present ChemCrow, a chemistry agent integrating LLMs and 18 specialized tools for diverse tasks across drug discovery, materials design, and organic synthesis. InstructMol <ref type="bibr" target="#b217">[218]</ref> presents a two-stage framework for aligning language and molecule graph modalities in drug discovery. Initially, the framework maintains the LLM and graph encoder parameters fixed, focusing on training the projector to align molecule-graph representations. Subsequently, instruction tuning is conducted on the LLM to address drug discovery tasks. Zhao et al. <ref type="bibr" target="#b218">[219]</ref> propose ChemDFM, the first dialogue foundation model for chemistry. Trained on extensive chemistry literature and general data, ChemDFM exhibits proficiency in various chemistry tasks such as molecular recognition, molecular design, and beyond.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Robot Task Planning</head><p>Robot task planning aims to decompose tasks into a series of high-level operations for the step-by-step completion by a robot <ref type="bibr" target="#b219">[220]</ref>. During task execution, the robot needs to perceive information about the surrounding environment, typically represented using scene graphs. In a scene graph, nodes represent scene objects like people and tables, while edges describe the spatial or functional relationships between objects. Enabling LLMs for robot task planning crucially depends on how to represent the environmental information in the scene graph.</p><p>Many studies have explored using textual descriptions of scene information and constructing prompts for LLMs to generate task plans. Chalvatzaki et al. <ref type="bibr" target="#b220">[221]</ref> introduce the Graph2NL mapping table, representing attributes with different numerical ranges using corresponding textual expressions. For instance, a distance value greater than 5 is represented as "distant", and smaller than 3 is represented as "reachable". SayPlan <ref type="bibr" target="#b221">[222]</ref> describes the scene graph in JSON as a text sequence, iteratively invoking LLM to generate plans and allowing for self-correction. Zhen et al. <ref type="bibr" target="#b222">[223]</ref> propose an effective prompt template, Think Net Prompt, to enhance LLM performance in task planning. In contrast to methods that rely on language to describe scene graph information, GRID <ref type="bibr" target="#b120">[121]</ref> employs the graph transformer to encode scene graphs. It utilizes cross-modal attention to align the graph modality with user instruction, ultimately outputting action tokens through a decoder layer. The powerful understanding and reasoning capabilities of LLMs showcase significant potential in robot task planning. However, as task complexity increases, the search space explosively expands, posing a challenge in efficiently generating viable task plans with LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">FUTURE DIRECTIONS</head><p>In this survey, we have thoroughly reviewed the latest developments of Graph Machine Learning in the era of LLMs, revealing significant advancements and potential in this field. By harnessing the power of LLMs, it is potential to enhance Graph ML to enable GFMs. As this research direction is still in the exploratory stage, future directions in this field can be diverse and innovative. Therefore, in this section, we delve into several potential future directions of this promising field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Generalization and Transferability</head><p>While Graph ML has deployed for various graph tasks, a notable problem is their limited capacity for generalization and transferability across different graph domains <ref type="bibr" target="#b39">[40]</ref>. Different from fields such as NLP and CV, where data often adhere to a uniform format (e.g., a sequence of tokens or a grid of pixels), graphs can be highly heterogeneous in nature. This heterogeneity manifests in varying graph sizes, densities, and types of nodes and edges, which presents a significant challenge in developing a universal model capable of performing optimally across various graph structure data. Currently, LLMs have demonstrated great potentials in improving the generalization ability of the graph model. For example, OFA <ref type="bibr" target="#b128">[129]</ref> provides a solution for classification tasks across several certain domains. Nevertheless, there is still scarce exploration of the generalizability of GFMs compared to LLMs. Therefore, future research should aim to develop more adaptable and flexible models that can effectively apply learned patterns from one graph type, such as social networks, to another, like molecular structures, without extensive retraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Multi-modal Graph Learning</head><p>Recent LLMs have shown significant potential in advancing GFMs. Many efforts have been made to transform graph data into formats suitable for LLM input, such as tokens or text <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b83">[84]</ref>, <ref type="bibr" target="#b130">[131]</ref>. However, many nodes in graphs are enriched with diverse modalities of information, including text, images, and videos. Understanding this multi-modal data can potentially benefit graph learning. For example, on social media platforms, a user's post could include textual content, images, and videos, all of which are valuable for comprehensive user modeling. Given the importance of multi-modal data, a promising direction for future research is to empower LLMs to process and integrate graph structure with multi-modal data. Currently, TOUCHUP-G <ref type="bibr" target="#b85">[86]</ref> makes an initial exploration of multi-modal ( i.e., texts, images) in graph learning. In the future, we expect the development of a unified model capable of modeling universal modalities for more advanced GFMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Trustworthiness</head><p>The recent applications of LLMs for Graph ML have significantly enhanced graph modeling capabilities and broadened their utility in various fields. Despite these advancements, with the growing reliance on these models, it is important to ensure their trustworthiness, particularly in critical areas like healthcare, finance, and social network analysis <ref type="bibr" target="#b223">[224]</ref>, <ref type="bibr" target="#b224">[225]</ref>. Robustness is fundamental in safeguarding the models against adversarial attacks, ensuring consistent reliability. Explainability is essential for users to understand and trust the decisions made by these models. Fairness is crucial for the model's ethical and effective use in various applications. Privacy is important for legal compliance and key to maintaining user trust. Therefore, the development of trustworthy LLMs on graphs must be equipped with Robustness&amp;Safety, Explainability, Fairness, and Privacy, ensuring their safe and effective use in various applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.1">Robustness&amp;Safety</head><p>Recently, integrating LLMs into Graph ML has shown promising performance in various downstream tasks, but they are also highly vulnerable to adversarial perturbations, raising significant concerns about their robustness and safety. To enhance the resilience of these models, some studies add adversarial perturbations to GNNs <ref type="bibr" target="#b225">[226]</ref>, <ref type="bibr" target="#b226">[227]</ref> or LLMs <ref type="bibr" target="#b227">[228]</ref>, <ref type="bibr" target="#b228">[229]</ref> for adversarial training. However, these methods may not be effective for the new paradigm of Graph ML integrating LLMs, as vulnerabilities can arise from both graphs, such as graph poisoning attacks <ref type="bibr" target="#b229">[230]</ref>, <ref type="bibr" target="#b230">[231]</ref> and graph modification attacks <ref type="bibr" target="#b231">[232]</ref>, <ref type="bibr" target="#b232">[233]</ref>, and from the language model, like prompt attacks <ref type="bibr" target="#b233">[234]</ref> and misleading text data <ref type="bibr" target="#b234">[235]</ref>. To address these issues, more sophisticated detection and defense mechanisms need to be developed by considering both the intricacies of LLMs and graphs to ensure the comprehensive safety and robustness of Graph ML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.2">Explainability</head><p>Nowadays, LLMs are increasingly employed in Graph ML across various applications, such as recommender systems <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b235">[236]</ref> and molecular discovery <ref type="bibr" target="#b84">[85]</ref>, <ref type="bibr" target="#b138">[139]</ref>. However, due to privacy and security concerns, an application provider may prefer to provide an API version without revealing the architecture and parameters of the LLM, such as with ChatGPT. This lack of transparency can make it challenging for users to understand the model's results, leading to confusion and dissatisfaction. Therefore, it's important to enhance the explainability of Graph ML, especially with LLMs. Owing to their reasoning and interpretive capabilities, LLMs are promising to provide better explainability in graphrelated tasks. For example, P5 <ref type="bibr" target="#b235">[236]</ref> can provide reasons for its recommendations in recommendation tasks. Future efforts should be directed toward making the inner workings of these models more transparent and explainable to better comprehend their decision-making processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.3">Fairness</head><p>As LLMs become prevalent in enhancing Graph ML towards GFMs, concerns about their fairness are growing.</p><p>Fairness is crucial to ensure these models operate without biases or discrimination, especially when dealing with complex, interconnected graph data <ref type="bibr" target="#b224">[225]</ref>. Recent studies demonstrate that both language models <ref type="bibr" target="#b236">[237]</ref>, <ref type="bibr" target="#b237">[238]</ref> and GNN models <ref type="bibr" target="#b238">[239]</ref> can potentially be discriminatory and unfair <ref type="bibr" target="#b41">[42]</ref>. Therefore, it is necessary to maintain fairness in both textual and graph contexts. To enhance the fairness of LLMs, recent studies include retraining strategies that adjust model parameters for unbiased outputs <ref type="bibr" target="#b239">[240]</ref>, implementing alignment constraints <ref type="bibr" target="#b240">[241]</ref>, and adopting contrastive learning to diminish bias in model training <ref type="bibr" target="#b241">[242]</ref>. Concurrently, studies like FairNeg <ref type="bibr" target="#b238">[239]</ref> also explore improving the fairness of recommendation data. Despite these efforts, achieving fairness in GFMs is still a significant challenge that needs further exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.4">Privacy</head><p>Privacy is a critical issue in Graph ML, particularly given the risk of these models inadvertently leaking sensitive information contained in graph data <ref type="bibr" target="#b242">[243]</ref>- <ref type="bibr" target="#b244">[245]</ref>. For example, Graph ML integrated with LLMs could potentially expose private user data, like browsing histories or social connections when generating outputs. This concern is especially pressing in highly data-sensitive areas such as healthcare or finance. To mitigate these privacy risks, <ref type="bibr" target="#b245">[246]</ref> introduces Privacy-Preserving Prompt Tuning (RAPT) to protect user privacy through local differential privacy. Future exploration in LLM-enhanced Graph ML should also focus on integrating privacy-preserving technologies like differential privacy and federated learning to strengthen data security and user privacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Efficiency</head><p>While LLMs have proven effective in constructing GFMs, their operational efficiency, particularly in processing large and complex graphs, is still a significant challenge <ref type="bibr" target="#b246">[247]</ref>. For example, the use of APIs like GPT4 for large-scale graph tasks can lead to high costs under current billing models. Additionally, deploying open-source large models (e.g., LLaMa) for parameter updates or just inference in local environments demands substantial computational resources and storage. Therefore, enhancing the efficiency of LLMs for graph tasks remains a critical issue. Recent studies introduce techniques like LoRA <ref type="bibr" target="#b155">[156]</ref> and QLoRA <ref type="bibr" target="#b247">[248]</ref> to fine-tune LLM parameters more efficiently. Furthermore, model pruning <ref type="bibr" target="#b248">[249]</ref>, <ref type="bibr" target="#b249">[250]</ref> is also a promising method to increase efficiency by removing redundant parameters or structures from LLMs, thereby simplifying their application in graph machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this survey, we have thoroughly reviewed the recent progress of graph applications and Graph ML in the era of LLMs, an emerging field in graph learning. We first review the evolution of Graph ML, and then delve into various methods of LLMs enhancing Graph ML. Due to the remarkable capabilities in various fields, LLMs have great potential to enhance Graph ML towards GFMs. We further explore the augmenting of LLMs with graphs, highlighting their ability to enhance LLM pre-training and inference.</p><p>Additionally, we demonstrate their potential in diverse applications such as molecule discovery, knowledge graphs, and recommender systems. Despite their success, this field is still evolving and presents numerous opportunities for further advancements. Therefore, we further discuss several challenges and potential future directions. Overall, our survey aims to provide a systematic and comprehensive review to researchers and practitioners, inspiring future explorations in this promising field.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The outline of our survey. Section 3 Deep Learning on Graphs explores the development of DNN-based methods, focusing on the Backbone Architecture, Graph Pretext Tasks, and Downstream Adaption three aspects. Section 4 LLMs for Graph Models explore how current LLMs help the current Graph ML towards GFMs from Enhancing Feature Quality, Solving Vanilla GNN Training Limitations, and Heterophily and Generalization three aspects. Section 5 Graph for LLMs focuses on Knowledge Graph(KG)-enhanced LLM Pre-training and KG-enhanced LLM Inference. Section 6 Applications presents various applications, including Recommender System, Knowledge Graph, AI for Science, and Robot Task Planning. Section 7 Future Directions discusses potential future directions for LLMs in graph machine learning from the Generalization and Transferability, Multi-modal Graph Learning, Trustworthiness and Efficiency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A comparison of pre-training, fine-tuning, and prompt tuning. (a) Pre-training involves training the GNN model based on specific pre-training tasks. (b) Fine-tuning updates the parameters of the pre-trained GNN model according to the downstream tasks. (c) Prompt tuning generates and updates the features of the prompt according to the downstream tasks, while keeping the pre-trained GNN model fixed and without any modification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of LLMs for Graph ML. (1) Methods using LLMs for Enhancing Feature Quality by enhancing feature representation, generating augmented information, and aligning feature space. (2) Explorations for solving Vanilla GNN Training Limitations are categorized based on how structural information in the graph is processed: ignoring structural information, implicit structural information, and explicit structural information. (3) Research about employing LLMs to alleviate the limitations of Heterophily and Generalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The illustration of employing LLMs with implicit and explicit structural information. (1) Methods leveraging implicit structural information describe nodes and graph structure information in natural language and combine task-specific instructions to form a textual prompt, which is then input into the LLM to generate prediction results. (2) Methods employing explicit structural information use GNNs and LLMs to encode graph and instruction information separately. Then, fusion layers are added to align the graph and text modalities, and the fused embedding is input into the LLM for prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>A comparison of various DNN-based models. We present Models and their Architecture, Pretext Task, Adaptation Method, and Downstream Tasks. URL in Adaptation Method indicates Unsupervised Representation Learning.</figDesc><table><row><cell>Model</cell><cell>Architecture</cell><cell>Pretext Task</cell><cell>Adaptation Method</cell><cell>Downstream Tasks</cell></row><row><cell>DGI [76]</cell><cell>GNN</cell><cell>Contrastive Learning</cell><cell>URL</cell><cell>Node</cell></row><row><cell>GRACE [77]</cell><cell>GNN</cell><cell>Contrastive Learning</cell><cell>URL</cell><cell>Node</cell></row><row><cell>GraphMAE [78]</cell><cell>GNN</cell><cell>Graph Generation</cell><cell>URL</cell><cell>Node, Graph</cell></row><row><cell>MVGRL [79]</cell><cell>GNN</cell><cell>Contrastive Learning</cell><cell>URL</cell><cell>Node, Graph</cell></row><row><cell>GraphCL [10]</cell><cell>GNN</cell><cell>Contrastive Learning</cell><cell>Fine-tuning</cell><cell>Node, Graph</cell></row><row><cell>CSSL</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>• W. Fan is with the Department of Computing (COMP) and Department</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Revisiting link prediction: A data perspective</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shomer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.00793</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.03358</idno>
		<title level="m">A comprehensive survey on graph reduction: Sparsification, coarsening, and condensation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adversarial attacks for black-box recommender systems via copying transferable cross-domain user profiles</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Disentangled contrastive learning for social recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 31st ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4570" to="4574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Knowledge-enhanced black-box attacks for recommendations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="108" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Epidemic graph convolutional network</title>
		<author>
			<persName><forename type="first">T</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Web Search and Data Mining (WSDM)</title>
		<meeting>the 13th International Conference on Web Search and Data Mining (WSDM)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="160" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<title level="m">Deep learning on graphs</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Lineartime graph neural networks for scalable recommendations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.13973</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5812" to="5823" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Contrastive self-supervised learning for graph classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on Artificial Intelligence</title>
		<meeting>the AAAI conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="10" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ccgl: Contrastive cascade graph learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="4539" to="4554" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gcc: Graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unified vision-language pre-training for image captioning and vqa</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13" to="041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Recommender systems in the era of large language models (llms)</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-08">Aug. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Zerog: Investigating cross-dataset zero-shot transferability in graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.11235</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Graph foundation models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.02216</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<title level="m">On the opportunities and risks of foundation models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Natural language is all a graph needs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-08">Aug. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Gpt4graph: Can large language models understand graph structured data ? an empirical evaluation and benchmarking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-07">Jul. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Graphgpt: Graph instruction tuning for large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-10">Oct. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Llama: Open and efficient foundation language models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5485" to="5551" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Simteg: A frustratingly simple approach improves textual graph learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.02565</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Exploring the potential of large language models (llms) in learning on graphs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-08">Aug. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Milenkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.00064</idno>
		<title level="m">Node feature extraction by selfsupervised multi-scale neighborhood prediction</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Graphtext: Graph reasoning in text space</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-10">Oct. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A survey on rag meets llms: Towards retrieval-augmented large language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.06211</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Barack&apos;s wife hillary: Using knowledge-graphs for fact-aware language modeling</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07241</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lundberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.12712</idno>
		<title level="m">Sparks of artificial general intelligence: Early experiments with gpt-4</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Explainability for large language models: A survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Large language models can learn temporal reasoning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Payani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kompella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fekri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.06853</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Reasoning on graphs: Faithful and interpretable large language model reasoning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-10">Oct. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">keqing: knowledge-based question answering is a nature chain-of-thought mentor of llm</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>An</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.00426</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Mitigating large language model hallucinations via autonomous knowledge graph-based retrofitting</title>
		<author>
			<persName><forename type="first">X</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.13314</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Towards verifiable generation: A benchmark for knowledge-aware language model attribution</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao2</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-10">Oct. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Graph learning and its advancements on large language models: A holistic survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.08966</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Large graph models: A perspective</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-08">Aug. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Large language models on graphs: A comprehensive survey</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.02783</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.12399</idno>
		<title level="m">A survey of graph meets large language model: Progress and future directions</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Towards graph foundation models: A survey and beyond</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.11829</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Fast graph condensation with structure-based neural tangent kernel</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.11046</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Untargeted black-box attacks for social recommendations</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.07127</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Compound-protein interaction prediction with end-to-end learning of neural networks for graphs and sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tsubaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tomii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="309" to="318" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph neural networks for social recommendation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The world wide web conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A graph neural network framework for social recommendations</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on world wide web</title>
		<meeting>the 24th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Graph transformer</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Graph transformer networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Accurate learning of graph representations with graph multiset pooling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Heterogeneous graph transformer</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the web conference 2020</title>
		<meeting>the web conference 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2704" to="2710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Graph-bert: Only attention is needed for learning graph representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05140</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Graphformers: Gnn-nested transformers for representation learning on textual graph</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="28" to="798" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Do transformers really perform badly for graph representation?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="28" to="877" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Introducing self-attention to target attentive graph neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mitheran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Java</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.01516</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Rethinking graph transformers with spectral attention</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kreuzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Létourneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tossou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="21" to="618" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">A generalization of transformer networks to graphs</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09699</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Can large language models empower molecular property prediction?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-07">Jul. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Graphwiz: An instructionfollowing language model for graph problems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.16029</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<ptr target="https://vicuna.lmsys.org" />
		<title level="m">Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality</title>
		<imprint>
			<date type="published" when="2023-04-14">14 April 2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Learning on large-scale text-attributed graphs via variational inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.14709</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Deberta: Decoding-enhanced bert with disentangled attention</title>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03654</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Visual chatgpt: Talking, drawing and editing with visual foundation models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.04671</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Fashionregen: Llm-empowered fashion report generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.06660</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Harnessing the power of large language models for natural language to firstorder logic translation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Payani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shareghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fekri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.15541</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li Ò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10341</idno>
		<title level="m">Deep graph infomax</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Deep graph contrastive representation learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04131</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Graphmae: Self-supervised masked graph autoencoders</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="594" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Contrastive multi-view representation learning on graphs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Khasahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4116" to="4126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Pre-training of graph augmented transformers for medication recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00346</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Adaptergnn: Efficient delta tuning improves generalization ability in graph neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.09595</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Self-supervised graph transformer on large-scale molecular data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12" to="559" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">G-adapter: Towards structureaware parameter-efficient transfer learning for graph transformer networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.10329</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Graphgpt: Graph learning with generative pre-trained transformers</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.00529</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">A molecular multimodal foundation model associating molecule graphs with natural language</title>
		<author>
			<persName><forename type="first">B</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.05481</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.13885</idno>
		<title level="m">Touchup-g: Improving feature representation through graphcentric finetuning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Graphprompt: Unifying pre-training and downstream tasks for graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference</title>
		<meeting>the ACM Web Conference</meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
			<biblScope unit="page" from="417" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Gppt: Graph pretraining and prompt tuning to generalize graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1717" to="1727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Prompt tuning for multi-view graph contrastive learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.10362</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Universal prompt tuning for graph neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">All in one: Multi-task prompting for graph neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Ultra-dp: Unifying graph pre-training with multi-task graph dual prompt</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.14845</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Enhancing graph neural networks with structure-based prompt</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.17394</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Prodigy: Enabling in-context learning over graphs</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kržmanc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.12600</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Sgl-pt: A strong graph learner with graph prompt tuning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.12449</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Deep prompt tuning for graph transformers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shirkavand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.10131</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Lamda: Language models for dialog applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08239</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Palm: Scaling language modeling with pathways</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Reflexion: Language agents with verbal reinforcement learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cassano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.11116</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Train your own gnn teacher: Graph-aware distillation on textual graphs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mavromatis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adeshina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.10668</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Graphologue: Exploring large language model responses with interactive diagrams</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Dow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.11473</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Do large language models understand chemistry? a conversation with chatgpt</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Castro Nascimento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Pimentel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1649" to="1655" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Chemcrow: Augmenting large-language models with chemistry tools</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schwaller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.05376</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Chatgpt for good? on opportunities and challenges of large language models for education</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Seßler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>K Üchemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bannert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dementieva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Gasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Groh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ünnemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Üllermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning and Individual Differences</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page">102274</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">How does chatgpt perform on the united states medical licensing examination? the implications of large language models for medical education and knowledge assessment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Safranek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Socrates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chartash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMIR Medical Education</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">45312</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">Bloomberggpt: A large language model for finance</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dabravolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kambadur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.17564</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Finbert: A pretrained language model for financial communications</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C S</forename><surname>Uy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08097</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Graph chain-of-thought: Augmenting large language models by reasoning on graphs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.07103</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Provably powerful graph networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higherorder graph neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">An effective self-supervised framework for learning expressive molecular global representations to drug discovery</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">109</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Heterformer: Transformerbased deep node representation learning on heterogeneous textrich networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1020" to="1031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">Edgeformers: Graphempowered transformers for representation learning on textualedge networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.11050</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">Pre-training graph neural networks for generic structural feature extraction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13728</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<title level="m" type="main">Lkpnr: Llm and kg for personalized news recommendation framework</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Runfeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiangyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhanwei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-08">Aug. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">Grid: Scene-graph-based instruction-driven robotic task planning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-09">Sep. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<title level="m" type="main">Harnessing explanations: Llm-to-lm interpreter for enhanced textattributed graph representation learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hooi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-10">Oct. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title level="m" type="main">Representation learning with large language models for recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-10">Oct. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">Llmrec: Large language models with graph augmentation for recommendation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Llm-rec: Personalized recommendation via prompting large language models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.15780</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">Gpt-molberta: Gpt molecular features language model for molecular property prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Magar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jadhav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Farimani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-10">Oct. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">Empower text-attributed graphs learning with large language models (llms)</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-10">Oct. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">Large language models as topological structure enhancers for text-attributed graphs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.14324</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b128">
	<monogr>
		<title level="m" type="main">One for all: Towards training one graph model for all classification tasks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.00149</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b129">
	<monogr>
		<title level="m" type="main">Beyond text: A deep dive into large language models&apos; ability on understanding graph data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-10">Oct. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsvetkov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.10037</idno>
		<title level="m">Can language models solve graph problems in natural language?</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title level="m" type="main">Evaluating large language models on graphs: Performance insights and comparative analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-09">Sep. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">Graph agent: Explicit reasoning agent for graphs</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.16421</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">Llm-prop: Predicting physical and electronic properties of crystalline solids from their text descriptions</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Rubungo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Rand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Dieng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-10">Oct. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<title level="m" type="main">Exploring large language model for graph data understanding in online job recommendations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.05722</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b135">
	<monogr>
		<title level="m" type="main">Relm: Leveraging language models for enhanced chemical reaction prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.13590</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title level="m" type="main">Can llms effectively leverage graph structural information: When and why</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-09">Sep. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<monogr>
		<title level="m" type="main">Talk like a graph: Encoding graphs for large language models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Halcrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-10">Oct. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<title level="m" type="main">Empowering molecule discovery for molecule-caption translation with large language models: A chatgpt perspective</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.06615</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title level="m" type="main">Llaga: Large language and graph assistant</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.08170</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b140">
	<monogr>
		<title level="m" type="main">Graphedit: Large language models for graph structure learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.15183</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b141">
	<monogr>
		<title level="m" type="main">Graphllm: Boosting graph reasoning ability of large language model</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-10">Oct. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<title level="m" type="main">Graph neural prompting with large language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-09">Sep. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<monogr>
		<title level="m" type="main">Drugchat: Towards enabling chatgpt-like capabilities on drug molecule graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<monogr>
		<title level="m" type="main">Making large language models perform better in knowledge graph completion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-10">Oct. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<monogr>
		<title level="m" type="main">Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-06">Jun. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b146">
	<monogr>
		<title level="m" type="main">Git-mol: A multi-modal large language model for molecular science with graph</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-08">Aug. 2023</date>
		</imprint>
	</monogr>
	<note>image, and text</note>
</biblStruct>

<biblStruct xml:id="b147">
	<monogr>
		<title level="m" type="main">Biomedgpt: Open multimodal generative pre-trained transformer for biomedicine</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Nie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.09442</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b148">
	<monogr>
		<title level="m" type="main">Proteinchat: Towards achieving chatgpt-like functionalities on protein 3d structures</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<monogr>
		<title level="m" type="main">Disentangled representation learning with large language models for textattributed graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.18152</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b150">
	<monogr>
		<title level="m" type="main">G-retriever: Retrieval-augmented generation for textual graph understanding and question answering</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hooi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.07630</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b151">
	<monogr>
		<title level="m" type="main">Let your graph do the talking: Encoding structured data for llms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tsitsulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Halcrow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.05862</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b152">
	<monogr>
		<title level="m" type="main">Opengraph: Towards open graph foundation models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.01121</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b153">
	<monogr>
		<title level="m" type="main">Patton: Language model pretraining on text-rich networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.12268</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b154">
	<monogr>
		<title level="m" type="main">Learning multiplex embeddings on text-rich networks with one text encoder</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06684</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b155">
	<monogr>
		<title level="m" type="main">Lora: Low-rank adaptation of large language models</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b156">
	<monogr>
		<title level="m" type="main">Gml: Graph modelling language</title>
		<author>
			<persName><forename type="first">M</forename><surname>Himsolt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>University of Passau</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<monogr>
		<title level="m" type="main">Graph markup language (graphml)</title>
		<author>
			<persName><forename type="first">U</forename><surname>Brandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eiglsperger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paranjape</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.03172</idno>
		<title level="m">Lost in the middle: How language models use long contexts</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b159">
	<monogr>
		<title level="m" type="main">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.12597</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Good: A graph out-of-distribution benchmark</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2059" to="2073" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">A survey of knowledge enhanced pre-trained language models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<monogr>
		<title level="m" type="main">Can knowledge graphs reduce hallucinations in llms?: A survey</title>
		<author>
			<persName><forename type="first">G</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kumarage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Alghami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.07914</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b163">
	<monogr>
		<title level="m" type="main">Unifying large language models and knowledge graphs: A roadmap</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-06">Jun. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<monogr>
		<title level="m" type="main">Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<idno>arXiv-2107</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">A knowledgeenriched ensemble method for word embedding and multi-sense embedding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Oerl: Enhanced representation learning via open knowledge graphs</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<monogr>
		<title level="m" type="main">Skill: Structured knowledge infusion for large language models</title>
		<author>
			<persName><forename type="first">F</forename><surname>Moiseev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-05">May 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<monogr>
		<title level="m" type="main">E-bert: Efficientyet-effective entity embeddings for bert</title>
		<author>
			<persName><forename type="first">N</forename><surname>Poerner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Waltinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sch Ütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03681</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b169">
	<monogr>
		<title level="m" type="main">Knowledge-aware language model pretraining</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tiwary</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00655</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">K-bert: Enabling language representation with knowledge graph</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020-04">Apr. 2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2901" to="2908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<monogr>
		<title level="m" type="main">Ernie: Enhanced language representation with informative entities</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07129</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Cokebert: Contextual knowledge selection and embedding towards enhanced pre-trained language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Open</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="127" to="134" />
			<date type="published" when="2021-01">Jan. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Klmo: Knowledge graph enhanced pretrained language model with fine-grained relationships</title>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<editor>
			<persName><forename type="first">M.-F</forename><surname>Moens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Specia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>-T</surname></persName>
		</editor>
		<editor>
			<persName><surname>Yih</surname></persName>
		</editor>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">Nov. 2021</date>
			<biblScope unit="page" from="4536" to="4542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">K-adapter: Infusing knowledge into pretrained models with adapters</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Zong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Navigli</surname></persName>
		</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08">Aug. 2021</date>
			<biblScope unit="page" from="1405" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<monogr>
		<title level="m" type="main">Kala: knowledge-augmented language model adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.10555</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Parameter-efficient domain knowledge integration from multiple sources for biomedical pre-trained language models</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3855" to="3865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01057</idno>
		<title level="m">Luke: Deep contextualized entity representations with entityaware self-attention</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b178">
	<monogr>
		<title level="m" type="main">Kilm: Knowledge injection into encoder-decoder language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Namazifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Padmakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hakkani-T Ür</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-02">Feb. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<monogr>
		<title level="m" type="main">Exploiting structured knowledge in text via graph-guided representation learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14224</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the association for computational linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Jaket: Joint pre-training of knowledge graph and language understanding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022-06">Jun. 2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="11" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Deep bidirectional language-knowledge graph pretraining</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="37" to="309" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<monogr>
		<title level="m" type="main">Erica: Improving entity and relation understanding for pre-trained language models via contrastive learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Takanobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15022</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Kepler: A unified model for knowledge embedding and pretrained language representation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="176" to="194" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Ernie 2.0: A continual pre-training framework for language understanding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8968" to="8975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.18223</idno>
		<title level="m">A survey of large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b187">
	<monogr>
		<title level="m" type="main">Knowledge-augmented language model prompting for zero-shot knowledge graph question answering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Aji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.04136</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b188">
	<monogr>
		<title level="m" type="main">Retrieve-rewrite-answer: A kg-to-text enhanced llms framework for knowledge graph question answering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-09">Sep. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<monogr>
		<title level="m" type="main">Mindmap: Knowledge graph prompting sparks graph of thoughts in large language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-09">Sep. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<monogr>
		<title level="m" type="main">Knowledge solver: Teaching llms to search for domain knowledge from knowledge graphs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-09">Sep. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<monogr>
		<title level="m" type="main">Minimizing factual inconsistency and hallucination in large language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vaddina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.13878</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b192">
	<monogr>
		<title level="m" type="main">Chatgraph: Interpretable text classification by converting chatgpt knowledge to graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.03513</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b193">
	<monogr>
		<title level="m" type="main">Xplainllm: A qa explanation dataset for understanding llm decision-making</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gaidhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.08614</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Deep modeling of social relations for recommendation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Deep adversarial social recommendation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th International Joint Conference on Artificial Intelligence (IJCAI-19). International Joint Conferences on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1351" to="1357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Deep social collaborative filtering</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Recommender Systems</title>
		<meeting>the 13th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="305" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Heterogeneous knowledge fusion: A novel approach for personalized recommendation via llm</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM Conference on Recommender Systems</title>
		<meeting>the 17th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="599" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<monogr>
		<title level="m" type="main">One model for all: Large language models are domain-agnostic recommendation systems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-10">Oct. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<monogr>
		<title level="m" type="main">Uncovering chatgpt&apos;s capabilities in recommender systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.02182</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b200">
	<monogr>
		<title level="m" type="main">Rethinking large language model architectures for sequential recommendations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.09543</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b201">
	<monogr>
		<title level="m" type="main">Towards open-world recommendation with knowledge augmentation from large language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.10933</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b202">
	<monogr>
		<title level="m" type="main">Leveraging large language models (llms) to empower trainingfree dataset condensation for content-based recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.09874</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b203">
	<monogr>
		<title level="m" type="main">Is chatgpt a good recommender? a preliminary study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.10149</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b204">
	<monogr>
		<title level="m" type="main">Tallrec: An effective and efficient tuning framework to align large language model with recommendation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.00447</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b205">
	<monogr>
		<title level="m" type="main">Up5: Unbiased foundation model for fairness-aware recommendation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.12090</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b206">
	<monogr>
		<title level="m" type="main">Foundation models for recommender systems: A survey and new perspectives</title>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.11143</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b207">
	<monogr>
		<title level="m" type="main">Cp-kgc: Constrained-prompt knowledge graph completion with large language models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-10">Oct. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<monogr>
		<title level="m" type="main">Exploring large language models for knowledge graph completion</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-09">Sep. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<monogr>
		<title level="m" type="main">Kg-gpt: A general framework for reasoning on knowledge graphs using large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-10">Oct. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<monogr>
		<title level="m" type="main">Chatkbqa: A generate-thenretrieve framework for knowledge base question answering with fine-tuned large language models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-10">Oct. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<monogr>
		<title level="m" type="main">Structgpt: A general framework for large language model to reason over structured data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.09645</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b212">
	<monogr>
		<title level="m" type="main">Chatrule: Mining logical rules with large language models for knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-09">Sep. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<monogr>
		<title level="m" type="main">Developing a scalable benchmark for assessing large language models in knowledge graph engineering</title>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Junghanns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Brei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bulert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gr Ünder-Fahrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-08">Aug. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<monogr>
		<title level="m" type="main">Towards foundation models for knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mostafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.04562</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">A general single-cell analysis framework via conditional diffusion generative models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page" from="2023" to="2033" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">Identifying the kind behind smiles-anatomical therapeutic chemical classification using structure-only representations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">346</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<monogr>
		<title level="m" type="main">Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.16208</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b218">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.14818</idno>
		<title level="m">Chemdfm: Dialogue foundation model for chemistry</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main">Robot task planning using semantic maps</title>
		<author>
			<persName><forename type="first">C</forename><surname>Galindo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-A</forename><surname>Fernández-Madrigal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saffiotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and autonomous systems</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="955" to="966" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">Learning to reason over scene graphs: A case study of finetuning gpt-2 into a robot language model for grounded task planning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chalvatzaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Younes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nandha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F R</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Robotics and AI</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">1221739</biblScope>
			<date type="published" when="2023-08">Aug. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<monogr>
		<title level="m" type="main">Sayplan: Grounding large language models using 3d scene graphs for scalable robot task planning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Haviland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Abou-Chakra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Suenderhauf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-09">Sep. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<monogr>
		<title level="m" type="main">Robot task planning based on large language model representing knowledge with directed graph structures</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xing-Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wei-Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hai-Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zi-Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yi-Shu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-06">Jun. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<monogr>
		<title level="m" type="main">Trustworthy ai: A computational perspective</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06641</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b224">
	<monogr>
		<title level="m" type="main">A comprehensive survey on trustworthy recommender systems</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.10117</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">Jointly attacking graph neural network and its explanations</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2023 IEEE 39th International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">Attacking black-box recommendations via copying cross-domain user profiles</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 37th International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1583" to="1594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<monogr>
		<title level="m" type="main">Baseline defenses for adversarial attacks against aligned language models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schwarzschild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Somepalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kirchenbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>-Y. Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.00614</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">Towards building a robust toxicity predictor</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bespalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhabesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="581" to="598" />
		</imprint>
	</monogr>
	<note>: Industry Track)</note>
</biblStruct>

<biblStruct xml:id="b229">
	<monogr>
		<title level="m" type="main">Understanding and improving graph injection attack by promoting unnoticeability</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.08057</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b230">
	<analytic>
		<title level="a" type="main">Tdgia: Effective injection attacks on graph neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kharlamov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2461" to="2471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b231">
	<analytic>
		<title level="a" type="main">Adversarial attack on graph structured data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1115" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b232">
	<analytic>
		<title level="a" type="main">Adversarial attacks on neural networks for graph data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akbarnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Ünnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2847" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b233">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Haghtalab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.02483</idno>
		<title level="m">Jailbroken: How does llm safety training fail?</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b234">
	<monogr>
		<title level="m" type="main">Certified robustness for large language models with self-denoising</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.07171</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b235">
	<analytic>
		<title level="a" type="main">Recommendation as language processing (rlp): A unified pretrain, personalized prompt &amp; predict paradigm (p5)</title>
		<author>
			<persName><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM Conference on Recommender Systems</title>
		<meeting>the 16th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="299" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b236">
	<analytic>
		<title level="a" type="main">Does gender matter? towards fairness in dialogue systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dacon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4403" to="4416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b237">
	<analytic>
		<title level="a" type="main">Fairness reprogramming</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-sixth Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b238">
	<analytic>
		<title level="a" type="main">Fairly adaptive negative sampling for recommendations</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2023</title>
		<meeting>the ACM Web Conference 2023</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3723" to="3733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b239">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.06032</idno>
		<title level="m">Measuring and reducing gendered correlations in pre-trained models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b240">
	<analytic>
		<title level="a" type="main">Auto-debias: Debiasing masked language models with automated biased prompts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abbasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1012" to="1023" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b241">
	<monogr>
		<title level="m" type="main">Mabel: Attenuating gender bias using textual entailment data</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.14975</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b242">
	<analytic>
		<title level="a" type="main">Extracting training data from large language models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Erlingsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Security Symposium</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b243">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-C</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.12628</idno>
		<title level="m">Are large pre-trained language models leaking your personal information?</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b244">
	<monogr>
		<title level="m" type="main">Graph unlearning with efficient partial retraining</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.07353</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b245">
	<monogr>
		<title level="m" type="main">Privacy-preserving prompt tuning for large language model services</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.06212</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b246">
	<monogr>
		<title level="m" type="main">Efficient large language models fine-tuning on graphs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.04737</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b247">
	<monogr>
		<title level="m" type="main">Qlora: Efficient finetuning of quantized llms</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pagnoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14314</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b248">
	<monogr>
		<title level="m" type="main">Llm-pruner: On the structural pruning of large language models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.11627</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b249">
	<monogr>
		<title level="m" type="main">Sheared llama: Accelerating language model pre-training via structured pruning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06694</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
