<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ODGEN: Domain-specific Object Detection Data Generation with Diffusion Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-11-05">5 Nov 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jingyuan</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shiyu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuxuan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ping</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiulong</forename><surname>Shan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Huimin</forename><surname>Ma</surname></persName>
						</author>
						<title level="a" type="main">ODGEN: Domain-specific Object Detection Data Generation with Diffusion Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-11-05">5 Nov 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">3A7DCC4CAE00BAEF5539DE20992AC617</idno>
					<idno type="arXiv">arXiv:2405.15199v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-01-21T07:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern diffusion-based image generative models have made significant progress and become promising to enrich training data for the object detection task. However, the generation quality and the controllability for complex scenes containing multiclass objects and dense objects with occlusions remain limited. This paper presents ODGEN, a novel method to generate high-quality images conditioned on bounding boxes, thereby facilitating data synthesis for object detection. Given a domainspecific object detection dataset, we first fine-tune a pre-trained diffusion model on both cropped foreground objects and entire images to fit target distributions. Then we propose to control the diffusion model using synthesized visual prompts with spatial constraints and object-wise textual descriptions. ODGEN exhibits robustness in handling complex scenes and specific domains. Further, we design a dataset synthesis pipeline to evaluate ODGEN on 7 domain-specific benchmarks to demonstrate its effectiveness. Adding training data generated by ODGEN improves up to 25.3% mAP@.50:.95 with object detectors like YOLOv5 and YOLOv7, outperforming prior controllable generative methods. In addition, we design an evaluation protocol based on COCO-2014 to validate ODGEN in general domains and observe an advantage up to 5.6% in mAP@.50:.95 against existing methods. * H. Ma is the corresponding author.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object detection is one of the most widely used computer vision tasks in real-world applications. However, data scarcity poses a significant challenge to the performance of state-of-the-art models including eDiff-I <ref type="bibr" target="#b1">[2]</ref>, DALL•E <ref type="bibr" target="#b41">[42]</ref>, Imagen <ref type="bibr" target="#b45">[46]</ref>, and Stable Diffusion <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b43">44]</ref> have demonstrated unparalleled capabilities to produce diverse samples with high fidelity given unfettered text prompts.</p><p>Layout-to-image generation synthesizes images conditioned on graphical inputs of layouts. Pioneering works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">35]</ref> based on GANs <ref type="bibr" target="#b15">[16]</ref> and transformers <ref type="bibr" target="#b52">[53]</ref> successfully generate images consistent with given layouts. LayoutDiffusion <ref type="bibr" target="#b65">[66]</ref> fuses layout and category embeddings and builds object-aware cross-attention for local conditioning with traditional diffusion models. LayoutDiffuse <ref type="bibr" target="#b23">[24]</ref> employs layout attention modules for bounding boxes based on LDMs. MultiDiffusion <ref type="bibr" target="#b2">[3]</ref> and BoxDiff <ref type="bibr" target="#b57">[58]</ref> develop training-free frameworks to produce samples with spatial constraints. GLIGEN <ref type="bibr" target="#b32">[33]</ref> inserts trainable gated self-attention layers to pre-trained LDMs to fit specific tasks and is hard to generalize to scenes uncommon for pre-trained models. ReCo <ref type="bibr">[61]</ref> and GeoDiffusion <ref type="bibr" target="#b4">[5]</ref> extend LDMs with position tokens added to text prompts and fine-tunes both text encoders and diffusion models to realize region-aware controls. They need abundant data to build the capability of encoding the layout information in text prompts. ControlNet <ref type="bibr" target="#b61">[62]</ref> reuses the encoders of LDMs as backbones to learn diverse controls. However, it still struggles to deal with some complex cases. MIGC <ref type="bibr">[67]</ref> decomposes multi-instance synthesis to single-instance subtasks utilizing additional attention modules. InstanceDiffusion <ref type="bibr" target="#b54">[55]</ref> adds precise instance-level controls, including boxes, masks, and scribbles for text-to-image generation. Given annotations of bounding boxes, this paper introduces a novel approach applicable to both specific and general domains to synthesize high-fidelity complex scenes consistent with annotations for the object detection task.</p><p>Dataset synthesis for training object detection models. Copy-paste <ref type="bibr" target="#b8">[9]</ref> is a simple but effective data augmentation method for detectors. Simple Copy-Paste <ref type="bibr" target="#b13">[14]</ref> achieves improvements with a simple random placement strategy for objects. Following works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b64">65]</ref> generate foreground objects and then paste them on background images. However, generated images by these methods usually have obvious artifacts on the boundary of pasted regions. DatasetGAN <ref type="bibr" target="#b63">[64]</ref> takes an early step to generate labeled images automatically. Diffusion models have been used to synthesize training data and benefit downstream tasks including object detection <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b62">63]</ref>, image classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b50">51]</ref>, and semantic segmentation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60]</ref>. Modern approaches for image-label pairs generation can be roughly divided into two categories. One group of works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b62">63]</ref> first generate images and then apply a pre-trained perception model to generate pseudo labels on the synthetic images. The other group <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b38">39]</ref> uses the same strategy as our approach to synthesize images under the guidance of annotations as input conditions. The second group also overlaps with layout-to-image approaches <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr">61,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr">67]</ref>. Our approach should be assigned to the second group and is designed to address challenging cases like multi-class objects, occlusions between objects, and specific domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>This section presents ODGEN, a novel approach to generate high-quality images conditioned on bounding box labels for specific domains. Firstly, we propose a new method to fine-tune the diffusion model in Sec. 3.1. Secondly, we design an object-wise conditioning method in Sec. 3.2. Finally, we introduce a generation pipeline to build synthetic datasets for detector training enhancement in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Domain-specific Diffusion Model Fine-tuning</head><p>Modern Stable Diffusion <ref type="bibr" target="#b43">[44]</ref> models are trained on the LAION-5B <ref type="bibr" target="#b47">[48]</ref> dataset. Therefore, the textual prompts and generated images usually follow similar distributions to the web crawl data. However, real-world object detection datasets could come from very specific domains. We fine-tune the UNet of the Stable Diffusion to fit the distribution of an arbitrary object detection dataset. For training images, we use not only the entire images from the dataset but also the crops of foreground objects. In particular, we crop foreground objects and resize them to 512 × 512. In terms of the text input, as shown in Fig. <ref type="figure" target="#fig_3">2</ref> (a), we use templated condition "a &lt;scene&gt;" named c s for entire images and "a &lt;classname&gt;" named c o for cropped objects. If the names are terminologies, following the subject-driven approach in DreamBooth <ref type="bibr" target="#b44">[45]</ref>, we can employ identifiers like "[V]" to represent the target objects and scenes, improving the robustness to textual ambiguity under the domain-specific context. x t s and x t o represent the input of scenes x s and objects x t added to noises at time step t. ϵ s and ϵ o represent the added noises following normal Gaussian distributions. The pre-trained Stable  Diffusion parameterized by θ is fine-tuned with the reconstruction loss as:</p><formula xml:id="formula_0">L rec =E xo,t,ϵo∼N (0,1) ||ϵ o -ϵ θ (x t o , t, τ (c o ))|| 2 + λE xs,t,ϵs∼N (0,1) ||ϵ s -ϵ θ (x t s , t, τ (c s ))|| 2<label>(1)</label></formula><p>where λ controls the relative weight for the reconstruction loss of scene images. τ is the frozen CLIP text encoder. Our approach guides the fine-tuned model to capture more details of foreground objects and maintain its capability of synthesizing the complete scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Object-wise Conditioning</head><p>ControlNet <ref type="bibr" target="#b61">[62]</ref> can perform controllable synthesis with visual conditions. However, the control can be challenging with an increasing object number and category number due to the "concept bleeding" phenomenon. Stronger conditions are needed to ensure high-fidelity generation. To this end, we propose a novel object-wise conditioning strategy with ControlNet.</p><p>Text list encoding. As shown in Fig. <ref type="figure" target="#fig_3">2</ref> (b) and (c), given a set of object class names and their bounding boxes, we first build a text list consisting of each object with the fixed template "a &lt;classname&gt;".</p><p>Then the text list is padded with empty texts to a fixed length N and converted to a list of embeddings by a pre-trained CLIP text tokenizer and encoder. Image list encoding. As shown in Fig. <ref type="figure" target="#fig_3">2</ref> (b) and (c), for each item in the text list, we use the fine-tuned diffusion model to generate images for each foreground object. We then resize each generated image and paste it on an empty canvas based on the corresponding bounding box. The set of pasted images is denoted as an image list, which contains both conceptual and spatial information of the objects. The image list is concatenated and zero-padded to N in the channel dimension and then encoded to the size of latent space by an image encoder. Applying an image list rather than pasting all objects on a single image can effectively avoid the influence of object occlusions.</p><p>Optimization. With a pair of image and object detection annotation, we generate the text list c tl and image list c il as introduced in this section. The input global text prompt c t of the diffusion model is composed of the object class names and a scene name, which is usually related to the dataset name and fixed for each dataset. The ControlNet, along with the integrated encoders, are trained with the reconstruction loss. In addition, the weights of foreground regions are enhanced to produce more realistic foreground objects in complex scenes. The overall loss function is formulated as:</p><formula xml:id="formula_1">L recon = E x,t,ct,c tl ,c il ,ϵ∼N (0,1) ||ϵ -ϵ θ (x t , t, c t , c tl , c il )|| 2 L control = L recon + γL recon ⊙ M<label>(2)</label></formula><p>where M represents a binary mask with 1 on foreground pixels 0 on background pixels. ⊙ represents element-wise multiplication, and γ controls the foreground re-weighting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dataset Synthesis Pipeline for Object Detection</head><p>Our dataset synthesis pipeline is summarized in Fig. <ref type="figure">3</ref>. We generate random bounding boxes and classes as pseudo labels based on the distributions of the training set. Then, the pseudo labels are converted to triplets: &lt;image list, text list, global text prompt&gt;. ODGEN uses the triplets as inputs to synthesize images with the fine-tuned diffusion model. In addition, we filter out the pseudo labels in which areas the foreground objects fail to be generated.</p><p>Object distribution estimation. To simulate the training set distribution, we compute the mean and variance of bounding box attributes and build normal distributions. In particular, given a dataset with K categories, we calculate the class-wise average occurrence number per image µ = (µ 1 , µ 2 , . . . , µ K ) and the covariance matrix Σ to build a multi-variable joint normal distribution N (µ, Σ). In addition, for each category k, we build normal distributions N (µ x , σ  Corrupted label filtering. There is a certain possibility that some objects fail to be generated in synthetic images. While utilizing image lists alleviates the concern to some extent, the diffusion model may still neglect some objects in complex scenes containing dense or overlapping objects. As a result, the synthesized pixels could not match the pseudo labels and will undermine downstream training performance. We fine-tune a ResNet50 <ref type="bibr" target="#b16">[17]</ref> with foreground and background patches cropped from the training set to classify whether the patch contains an object. The discriminator checks whether objects are successfully synthesized in the regions of pseudo labels, rejects nonexistent ones, and further improves the accuracy of synthetic datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct extensive experiments to demonstrate the effectiveness of the proposed ODGEN in both specific and general domains. FID <ref type="bibr" target="#b18">[19]</ref> is used as the metric of fidelity. Mean average precisions (mAP) of YOLOv5s <ref type="bibr" target="#b24">[25]</ref> and YOLOv7 <ref type="bibr" target="#b53">[54]</ref> trained with synthetic data are used to evaluate the trainability, which concerns the usefulness of synthetic data for detector training. Our approach is implemented with Stable Diffusion v2.1 and compared with prior controllable generation methods based on Stable Diffusion, including ReCo <ref type="bibr">[61]</ref>, GLIGEN <ref type="bibr" target="#b32">[33]</ref>, ControlNet <ref type="bibr" target="#b61">[62]</ref>, GeoDiffusion <ref type="bibr" target="#b4">[5]</ref>, InstanceDiffusion <ref type="bibr" target="#b54">[55]</ref>, and MIGC <ref type="bibr">[67]</ref>. Native ControlNet doesn't support bounding box conditions. Therefore, we convert boxes to a mask C sized H ×W ×K, where H and W are the image height and width, and K is the class number. If one pixel (i, j) is in n boxes of class k, the C[i, j, k] = n. YOLO models are trained with the same recipe as Roboflow-100 <ref type="bibr" target="#b5">[6]</ref> for 100 epochs to ensure convergence. The length of text and image lists N used in our ODGEN is set to the maximum object number per image in the training set.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Specific Domains</head><p>Roboflow-100 <ref type="bibr" target="#b5">[6]</ref> is used for the evaluation in specific domains. It consists of 100 object detection datasets of various domains. We select 7 representative datasets (RF7) covering video games, medical imaging, and underwater scenes. To simulate data scarcity, we only sample 200 images as the training set for each dataset. The whole training process including the fine-tuning on both cropped objects and entire images and the training of the object-wise conditioning module, only depends on the 200 images. λ in Eq. ( <ref type="formula" target="#formula_0">1</ref>) and γ in Eq. ( <ref type="formula" target="#formula_1">2</ref>) are set as 1 and 25. We first fine-tune the diffusion model according to Fig. <ref type="figure" target="#fig_3">2</ref> (a) for 3k iterations. Then we train the object-wise conditioning modules on a V100 GPU with batch size 4 for 200 epochs, the same as the other methods to be compared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Fidelity</head><p>For each dataset in RF7, we compute the FID scores of 5000 synthetic images against real images, respectively. As shown in Tab. 1, our approach outperforms all the other methods. We visualize the generation quality in Fig. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Trainability</head><p>To explore the effectiveness of synthetic data for detector training, we train YOLO models pre-trained on COCO with different data and compare their mAP@.50:.95 scores in Tab. 2. In the baseline setting, we only use the 200 real images for training. For the other settings, we use a combination of 200 real and 5000 synthetic images. Our approach gains improvement over the baseline and outperforms all the other methods.</p><p>In addition, we add experiments with larger-scale datasets with 1000 images sampled from the Apex Game and the Underwater datasets. The training setups are kept the same as the training on 200 images. We conduct experiments on ODGEN, ReCo, and GeoDiffusion. ReCo and GeoDiffusion   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">General Domains</head><p>COCO-2014 <ref type="bibr" target="#b36">[37]</ref> is used to evaluate our method in general domains. As the scenes in this dataset are almost covered by the pre-training data of Stable Diffusion, we skip the domain-specific fine-tuning stage and train the diffusion model together with the object-wise conditioning modules. We train our ODGEN on the COCO training set with batch size 32 for 60 epochs on ×8 V100 GPUs as well as GLIGEN, ControlNet, and GeoDiffusion. Officially released checkpoints of ReCo (trained for 100 epochs on COCO), MIGC (trained for 300 epochs on COCO), and InstanceDiffusion (trained on self-constructed datasets) are employed for comparison. γ in Eq. ( <ref type="formula" target="#formula_1">2</ref>) is set as 10 for our ODGEN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Fidelity</head><p>We use the annotations of the COCO validation set as conditions to generate 41k images. The FID scores in Tab. 4 are computed against the COCO validation set. ODGEN achieves better FID results than the other methods. We provide a typical visualized example in Fig. <ref type="figure" target="#fig_6">5</ref>. Given overlapping bounding boxes of multiple categories, ODGEN synthesizes all the objects of correct categories and accurate positions, outperforming the other methods. More samples are added in Appendix G. We observe that YOLO models trained on synthetic data only fall behind models trained on real data.</p><p>We add experiments with different training and validation data combinations in Tab. 6. YOLO models trained on real images show better generalization ability and achieve close results when tested on real and synthetic data. YOLO models trained on synthetic data only get significantly better results when tested on synthetic data than when generalized to real data. It indicates that noticeable domain gaps still exist between real and synthetic data, which may be limited by the generation quality of modern Stable Diffusion models and are promising to be narrowed with future models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>Image list and text list in object-wise conditioning modules. ODGEN enables object-wise conditioning with an image list and a text list as shown in Fig. <ref type="figure" target="#fig_3">2</ref>. We test the performance of From the results in Tab. 7, it can be found that using image lists or text lists brings moderate improvement on MRI images and significant improvement on the more complex Road Traffic dataset. Visualization in Fig. <ref type="figure">6</ref> shows that the image list improves the fidelity under occlusion, and the text list mitigates the mutual interference of multiple objects. Foreground region enhancement. We introduce a reweighting term controlled by γ for foreground objects in Eq. (2). Tab. 8 shows that appropriate γ values can improve both the fidelity and the trainability since the foreground details are better generated. However, increasing the value will undermine the background quality, as visualized in Fig. <ref type="figure">6</ref>.</p><p>Corrupted label filtering. Results in Tab. 9 show that the corrupted label filtering step helps improve the mAP@.50:95 of ODGEN by around 1-2%. The results of ODGEN without the corrupted label filtering still significantly outperform the baseline setting in Tab. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper introduces ODGEN, a novel approach aimed at generating domain-specific object detection datasets to enhance the performance of detection models. We propose to fine-tune the diffusion model on both cropped foreground objects and entire images. We design a mechanism to control the foreground objects with object-wise synthesized visual prompts and textual descriptions. Our work significantly enhances the performance of diffusion models in synthesizing complex scenes characterized by multiple categories of objects and bounding box occlusions. Extensive experiments demonstrate the superiority of ODGEN in terms of fidelity and trainability across both specific and general domains. We believe this work has taken an essential step toward more robust image synthesis and highlights the potential of benefiting the object detection task with synthetic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Limitations</head><p>Despite the significant improvement achieved by this work, we find it remains challenging to achieve comparable training performance as real data with merely synthetic data. We consider the limitation to be mainly caused by two reasons. Firstly, the generation fidelity is still limited by modern diffusion models. For instance, VAEs in Stable Diffusion are not good enough to deal with complex text in images, constraining the performance on video game datasets. ODGEN is promising to achieve further improvements with more powerful diffusion models in the future. Secondly, this paper mainly focuses on designing generative models with given conditions. However, the other parts in the dataset synthesis pipeline are not fully optimized, which are interesting to be explored in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Broader Impact</head><p>We introduce a novel approach for object detection datasets synthesis, achieving improvement in fidelity and trainability for both specific and general domains. Our approach is more prone to biases caused by training data than typical AI generative models since it shows robustness with limited data of specific domains. Therefore, we recommend practitioners to apply abundant caution when dealing with sensitive applications to avoid problems of races, skin tones, or gender identities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Supplemental Ablations</head><p>Detector training w/o real data. In the main paper, we train the detectors with 200 real images and 5000 synthetic images for RF7 benchmarks. In this ablation study, we conduct experiments with the 5000 synthetic images only to explore the impact of the absence of real data for training detectors. As shown in Tab. 10, for easy datasets with fewer objects, like MRI image and Cotton, training with 5000 synthetic data only achieves better results than training with 200 real samples only. Increasing the dataset size can improve the model performance because the generation quality is good enough in these cases. Nevertheless, for complex scenes like Robomaster and Aquarium, training with synthetic data only fails to achieve better results. It may be caused by the limited generation quality of modern diffusion models. For instance, the VAE in Stable Diffusion is not good at synthesizing texts in images, which is common in the Robomaster dataset. Besides, FID results in Tab. 1 indicate that there still exist gaps between the distributions of real and synthetic images. The performance of our approach is promising to be further improved in the future with more powerful generative diffusion models. In addition, the current dataset synthesis pipeline is not fully optimized and cannot reproduce the distributions of foreground objects perfectly, which may also lead to worse performance of training on synthetic data only with the RF7 datasets. Number of synthesized samples. We ablate the number of synthesized samples used for detector training and provide quantitative results in Tab. 11. With increasing amounts of synthesized samples, detectors achieve higher performance and get very close results between 5000 and 10000 synthetic samples.</p><p>Category isolation for datasets synthesis. As illustrated in the main paper, modern diffusion models have the limitation of "concept bleeding" when multiple categories are involved in the generation of one image. An alternative method could be only generating objects of the same category in one image. We select 3 multi-class datasets and compare this approach against the proposed method in ODGEN. As shown in Tab. 12, ODGEN achieves better results on most benchmarks than generating samples with isolated categories. This is an expected result since generating objects of various categories in one image should be better aligned with the test set.   All the methods included for comparison in our experiments share the same textual descriptions of the foreground objects and entire scenes in the global text prompts for a fair comparison. ReCo <ref type="bibr">[61]</ref> and GeoDiffusion <ref type="bibr" target="#b4">[5]</ref> further add the location information of foreground objects to their global text prompts following their approaches. For general domains, we concatenate the object class names into a sentence as the global text prompt. For specific domains, we concatenate the object class names and the scene name into a sentence as the global text prompt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 ODGEN (ours)</head><p>We follow the default training setting used by ControlNet <ref type="bibr" target="#b61">[62]</ref>, including the learning rates and the optimizer to train our ODGEN models and provide fair comparison with ControlNet.</p><p>Offline foreground objects synthesis. To improve the training efficiency, we synthesize 500 foreground object images for each category offline and build a synthetic foreground pool. During the training of object-wise conditioning modules, foreground images are randomly sampled from the pool to build the image list.</p><p>Image encoder. ControlNet uses a 4-layer convolutional encoder to encode input conditions sized 512 to 64 × 64 latents. The input channels are usually set as 1 or 3 for input images. Our ODGEN follows similar methods to convert the input conditions. The key difference is that we use image lists of synthesized objects as input conditions. The image lists are padded to N and concatenated in the channel dimension for encoding. In this case, we expand the input channels of the encoder to 3N and update the channels of the following layers to comparable numbers. We provide the channels of each layer in Tab. 17. The convolutional kernel size is set as 3 for all layers. We design different encoder architectures according to the maximum object numbers that can be found in a single image. For datasets like MRI in which most images contain only one object, we can use a smaller encoder to make the model more lightweight. For a generic model trained on large-scale datasets, we can fix the architectures of encoders with a large N to make it applicable to images containing many objects. ControlNet <ref type="bibr" target="#b61">[62]</ref> is initially designed to control diffusion models by fixing pre-trained models, reusing the diffusion model encoder and fine-tuning it to provide control. In our experiments, to make fair comparisons, we also fine-tune the diffusion model part to fit it to specific domains that can be different from the pre-training data of Stable Diffusion. The diffusion model remains trainable for experiments on COCO as well. The native ControlNet is originally designed as an image-to-image generation method. As illustrated in Sec. 4, we provide a simple way to convert boxes to masks. Given a dataset containing K categories, an image sized H × W , and B objects with their bounding boxes bbox and classes cls, we convert them to a mask M in the following process: </p><formula xml:id="formula_2">Algorithm</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Supplemental Related Works</head><p>Copy-paste <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b64">65]</ref> method requires segmentation masks to get the cropped foreground objects, which are not provided by the RF7 datasets used in this paper. Besides, our approach also serves as a controllable image generation method that only needs bounding box labels to produce realistic images, which is hard to achieve by copy-paste. InstaGEN <ref type="bibr" target="#b10">[11]</ref> is mainly designed for the open-vocabulary object detection task. It generates images randomly and annotates them with a grounding head trained with another pre-trained detector. Earlier works like LayoutDiffusion <ref type="bibr" target="#b65">[66]</ref> are not implemented with latent diffusion models and thus are not included for comparison. Works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b55">56]</ref> are designed for semantic segmentation, while our ODGEN is designed for object detection. G Supplemental Experiments mAP@.50 on RF7. Tab. 20 provides quantitative results for trainability in terms of mAP@.50 as supplements to Tab. 2. Our approach achieves improvement over training on real data only and outperforms other methods.</p><p>Visual relationship between image lists and synthesized images. The image list is designed to provide information on category and localization for controlling foreground object layouts. The synthesized images share the same category with objects in generated images and are pasted to   Quantitative evaluation for concept bleeding. This work mainly focuses on the concept of the bleeding problem of different categories of objects. The mAP results of YOLO models trained on synthetic images only in Tab. 4 can be a proxy task to prove that the concept bleeding is alleviated since our ODGEN achieves state-of-the-art performance in the layout-image consistency of complex scene generation conditioned on bounding boxes. This section adds quantitative evaluation with BLIP-VQA <ref type="bibr" target="#b20">[21]</ref>, which employs the BLIP <ref type="bibr" target="#b31">[32]</ref>   Visualization. We provide extensive visualized examples of ODGEN to demonstrate its effectiveness. In Fig. <ref type="figure" target="#fig_12">13</ref>, we provide visualized samples produced by models fine-tuned on both cropped foreground objects and entire images. It shows that the fine-tuned models are capable of generating diverse foreground objects and complete scenes. In Fig. <ref type="figure" target="#fig_13">14</ref>, we show generated samples of the specific domains in RF7. It can be seen that ODGEN is robust to complex scenes composed of multiple categories, dense and overlapping objects. Besides, ODGEN shows strong generalization capability to various domains.        </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1</head><label></label><figDesc>Tsinghua University, China 2 Apple 3 University of Science and Technology Beijing 'motorcycles in a traffic scene' and a holothurian in an underwater scene' echinus starfish 'an echinus and a starfish in an underwater scene' negative 'an avatar and an object in a screen shot of the Apex game' 'a negative area in an MRI image' of aquarium' starfish 'a starfish and a shark in a photo of aquarium'</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The proposed ODGEN enables controllable image generation from bounding boxes and text prompts. It can generate high-quality data for complex scenes, encompassing multiple categories, dense objects, and occlusions, which can be used to enrich the training data for object detection.</figDesc><graphic coords="1,220.30,261.99,182.62,139.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: ODGEN training pipeline: (a) A pre-trained diffusion model is fine-tuned on a detection dataset with both entire images and cropped foreground patches. (b) A text list is built based on class labels. The fine-tuned diffusion model in stage (a) is used to generate a synthetic object image for each text. Generated object images are resized and pasted on empty canvases per box positions, constituting an image list. (c) The image list is concatenated in the channel dimension and encoded as conditions for ControlNet. The text list is encoded by the CLIP text encoder, stacked, and encoded again by the text embedding encoder as inputs for ControlNet.</figDesc><graphic coords="4,425.97,158.58,269.98,210.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>2 x ) and N (µ y , σ 2 y )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison between ODGEN and other methods under the same condition shown in the first column. ODGEN can be generalized to specific domains and enables accurate layout control.</figDesc><graphic coords="6,386.64,255.89,53.10,53.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visualized results comparison for models trained on COCO. ODGEN is better qualified for synthesizing complex scenes with multiple categories of objects and bounding box occlusions.</figDesc><graphic coords="8,167.71,237.10,50.66,50.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Visualized ablations. Left: using neither image nor text lists, a traffic light is generated in the position of a bus, and a car is generated in the position of a traffic light; Middle: not using image list, two occluded motorcycles are merged as one; Right: not using text list, a motorcycle is generated in the position of a vehicle. Using both image and text lists generates correct results.</figDesc><graphic coords="10,108.00,72.00,396.00,80.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Visualized examples produced by the model fine-tuned on entire images from the Road Traffic dataset only. It fails to produce images of foreground objects correctly.</figDesc><graphic coords="17,108.00,72.00,396.00,94.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Visualized examples with object detection annotations of RF7 datasets.</figDesc><graphic coords="20,108.00,72.00,396.01,507.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Left: images used to build image lists. Right: corresponding synthesized samples. The synthesized images of foreground objects share only the same category with objects in synthesized samples but not colors or shapes.</figDesc><graphic coords="22,112.41,72.00,205.92,248.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Detection results of YOLO detectors trained w/ or w/o ODGEN synthetic data. The synthesized samples of ODGEN help detectors perform better on partially occluded objects.</figDesc><graphic coords="22,333.27,82.09,166.32,228.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Visualized samples generated by the models fine-tuned on both cropped foreground objects and entire images from RF7 datasets (200 images for each dataset). Text prompts are provided under figures. Columns 1-6 are generated foreground objects and columns 7-9 are generated entire images. Models fine-tuned on entire images only cannot synthesize foreground objects required by image lists since they cannot bind the prompts to corresponding objects in entire images that may contain multiple categories of objects.</figDesc><graphic coords="23,108.00,363.69,395.97,156.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>'a negative areaFigure 14 :</head><label>14</label><figDesc>Figure14: Additional visualized samples produced by our approach in various specific domains. Annotations of bounding boxes are marked on samples and global text prompts are provided below samples. Our approach can be generalized to various domains and is compatible with complex scenes with multiple objects, multiple categories, and bounding box occlusions.</figDesc><graphic coords="24,265.31,372.34,194.01,151.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>, we provide supplemental visualized comparisons between ODGEN and the other methods on the COCO-2014 dataset. ODGEN achieves the best image-annotation consistency and maintains high generation fidelity. More generated samples of ODGEN are shown in Fig.18. In addition, we also show different examples produced by ODGEN on the same conditions in Fig.19to show its capability of generating diverse results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Visualized comparison results on the COCO-2014 dataset. The annotations used as conditions to synthesize images are shown in the first column.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Visualized comparison results on the COCO-2014 dataset. The annotations used as conditions to synthesize images are shown in the first column.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Visualized comparison results on the COCO-2014 dataset. The annotations used as conditions to synthesize images are shown in the first column.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Visualized results of ODGEN on the COCO-2014 dataset. In every grid, the annotation used as conditions to synthesize images are shown in the top-left corner.</figDesc><graphic coords="28,177.66,584.09,58.46,58.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Visualized results of our ODGEN on the COCO-2014 dataset generated from the same conditions. The annotation is placed on the first column.</figDesc><graphic coords="29,179.24,578.89,58.46,58.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Object distribution estimation Dataset Synthesis Corrupted label filtering</head><label></label><figDesc>Pipeline for object detection dataset synthesis. Yellow block: estimate Gaussian distributions for the bounding box number, area, aspect ratio, and location based on the training set. Blue block: sample pseudo labels from the Gaussian distributions and generate conditions including text and image lists to synthesize novel images. Pink block: train a classifier with foreground and background patches randomly cropped from the training set and use it to filter pseudo labels that failed to be synthesized. Finally, the filtered labels and synthetic images compose datasets.</figDesc><table><row><cell></cell><cell></cell><cell>Bounding box number</cell><cell></cell><cell></cell><cell>Filtered Labels</cell></row><row><cell></cell><cell></cell><cell>Bounding box area</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Bounding box aspect ratio</cell><cell></cell><cell>Fore/Back-</cell></row><row><cell></cell><cell></cell><cell>Bounding box location</cell><cell></cell><cell></cell><cell>ground</cell></row><row><cell cols="2">Detection Datasets</cell><cell>Class-wise Statistics</cell><cell cols="2">Gaussian Distributions</cell><cell>Discriminator</cell></row><row><cell></cell><cell></cell><cell>sample</cell><cell></cell><cell></cell></row><row><cell cols="2">classname</cell><cell>'a &lt;classname 1&gt;' 'a &lt;classname 2&gt;'</cell><cell cols="2">'&lt;classname 1&gt; and &lt;classname 2&gt; and &lt;classname 3&gt; in a &lt;scene&gt;'</cell><cell>Cropped Objects</cell></row><row><cell>1</cell><cell>classname</cell><cell>'a &lt;classname 2&gt;' 'a &lt;classname 3&gt;'</cell><cell cols="2">Text Prompt</cell></row><row><cell cols="2">Pseudo Labels classname 3 2 classname 2</cell><cell>Image List Text List</cell><cell>ControlNet</cell><cell>Fine-tuned Diffusion Model</cell><cell>Synthesized Samples</cell></row><row><cell cols="2">Figure 3:</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">The embeddings are stacked and encoded by a</cell></row><row><cell cols="6">4-layer convolutional text embedding encoder, and used as the textual condition for ControlNet. The</cell></row><row><cell cols="6">text encoding of native ControlNet compresses the information in global text prompts altogether,</cell></row><row><cell cols="6">resulting in mutual interference between distinct categories. To alleviate this "concept bleeding"</cell></row><row><cell cols="6">problem of multiple categories, our two-step text encoding enables the ControlNet to capture the</cell></row><row><cell cols="4">information of each object with separate encoding.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>FID (↓) scores computed over 5000 images synthesized by each approach on RF7 datasets. ODGEN achieves better results than the other on all 7 domain-specific datasets.</figDesc><table><row><cell>Datasets</cell><cell>ReCo</cell><cell cols="4">GLIGEN ControlNet GeoDiffusion ODGEN</cell></row><row><cell>Apex Game</cell><cell>88.69</cell><cell>125.27</cell><cell>97.32</cell><cell>120.61</cell><cell>58.21</cell></row><row><cell>Robomaster</cell><cell>70.12</cell><cell>167.44</cell><cell>134.92</cell><cell>76.81</cell><cell>57.37</cell></row><row><cell>MRI Image</cell><cell>202.36</cell><cell>270.52</cell><cell>212.45</cell><cell>341.74</cell><cell>93.82</cell></row><row><cell>Cotton</cell><cell>108.55</cell><cell>89.85</cell><cell>196.87</cell><cell>203.02</cell><cell>85.17</cell></row><row><cell>Road Traffic</cell><cell>80.18</cell><cell>98.83</cell><cell>162.27</cell><cell>68.11</cell><cell>63.52</cell></row><row><cell>Aquarium</cell><cell>122.71</cell><cell>98.38</cell><cell>146.26</cell><cell>162.19</cell><cell>83.07</cell></row><row><cell>Underwater</cell><cell>73.29</cell><cell>147.33</cell><cell>126.58</cell><cell>125.32</cell><cell>70.20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>mAP@.50:.95 (↑) of YOLOv5s / YOLOv7 on RF7. Baseline models are trained with 200 real images only, whereas the other models are trained with 200 real + 5000 synthetic images from various methods. ODGEN leads to the biggest improvement on all 7 domain-specific datasets.</figDesc><table><row><cell></cell><cell>Baseline</cell><cell>ReCo</cell><cell>GLIGEN</cell><cell cols="2">ControlNet GeoDiffusion</cell><cell>ODGEN</cell></row><row><cell>real + synth #</cell><cell>200 + 0</cell><cell cols="3">200 + 5000 200 + 5000 200 + 5000</cell><cell>200 + 5000</cell><cell>200 + 5000</cell></row><row><cell>Apex Game</cell><cell>38.3 / 47.2</cell><cell>25.0 / 31.5</cell><cell>24.8 / 32.5</cell><cell>33.8 / 42.7</cell><cell>29.2 / 35.8</cell><cell>39.9 / 52.6</cell></row><row><cell>Robomaster</cell><cell>27.2 / 26.5</cell><cell>18.2 / 27.9</cell><cell>19.1 / 25.0</cell><cell>24.4 / 32.9</cell><cell>18.2 / 22.6</cell><cell>39.6 / 34.7</cell></row><row><cell>MRI Image</cell><cell>37.6 / 27.4</cell><cell>42.7 / 38.3</cell><cell>32.3 / 25.9</cell><cell>44.7 / 37.2</cell><cell>42.0 / 38.9</cell><cell>46.1 / 41.5</cell></row><row><cell>Cotton</cell><cell>16.7 / 20.5</cell><cell>29.3/ 37.5</cell><cell>28.0 / 39.0</cell><cell>22.6 / 35.1</cell><cell>30.2 / 36.0</cell><cell>42.0 / 43.2</cell></row><row><cell>Road Traffic</cell><cell>35.3 / 41.0</cell><cell>22.8 / 29.3</cell><cell>22.2 / 29.5</cell><cell>22.1 / 30.5</cell><cell>17.2 / 29.4</cell><cell>39.2 / 43.8</cell></row><row><cell>Aquarium</cell><cell>30.0 / 29.6</cell><cell>23.8 / 34.3</cell><cell>24.1 / 32.2</cell><cell>18.2 / 25.6</cell><cell>21.6 / 30.9</cell><cell>32.2 / 38.5</cell></row><row><cell>Underwater</cell><cell>16.7 / 19.4</cell><cell>13.7 / 15.8</cell><cell>14.9 / 18.5</cell><cell>15.5 / 17.8</cell><cell>13.8 / 17.2</cell><cell>19.2 / 22.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>4. GLIGEN only updates gated self-attention layers inserted to Stable Diffusion, making it hard to be generalized to specific domains like MRI images and the Apex game. ReCo and GeoDiffusion integrate encoded bounding box information into text tokens, which require more data for diffusion model and text encoder fine-tuning. With only 200 images, they fail to generate objects in the given box regions. ControlNet, integrating the bounding box information into the visual condition, presents more consistent results with the annotation. However, it may still miss some objects or generate wrong categories in complex scenes. ODGEN achieves superior performance on both visual effects and consistency with annotations.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>mAP@.50 / mAP@.50:.95 (↑) results of ODGEN trained on larger-scale datasets of 1000 real images. The top 3 rows show results of YOLOv5s and the bottom 3 rows show results of YOLOv7. Baseline models are trained with 1000 real images only, whereas the other models are trained with 1000 real + 5000 / 10000 synthetic images from various methods. ODGEN leads to more significant improvement than other methods.</figDesc><table><row><cell>Datasets</cell><cell cols="2">Apex Game Apex Game</cell><cell cols="4">Apex Game Underwater Underwater</cell><cell>Underwater</cell></row><row><cell>real + synth #</cell><cell>1000 + 0</cell><cell cols="2">1000 + 5000 1000 + 10000</cell><cell>1000 + 0</cell><cell></cell><cell cols="2">1000 + 5000 1000 + 10000</cell></row><row><cell>ReCo</cell><cell>83.2 / 53.5</cell><cell>78.7 / 46.9</cell><cell>82.0 / 46.9</cell><cell cols="2">55.6 / 29.2</cell><cell>55.1 / 28.4</cell><cell>55.9 / 29.1</cell></row><row><cell cols="2">GeoDiffusion 83.2 / 53.5</cell><cell>80.0 / 47.2</cell><cell>82.5 / 47.5</cell><cell cols="2">55.6 / 29.2</cell><cell>54.2 / 27.9</cell><cell>54.3 / 28.0</cell></row><row><cell>ODGEN</cell><cell>83.2 / 53.5</cell><cell>83.3 / 53.5</cell><cell>83.6 / 53.6</cell><cell cols="2">55.6 / 29.2</cell><cell>59.6 / 32.5</cell><cell>56.3 / 29.8</cell></row><row><cell>ReCo</cell><cell>83.8 / 55.0</cell><cell>80.5 / 50.7</cell><cell>79.2 / 49.9</cell><cell cols="2">54.6 / 28.3</cell><cell>56.5 / 28.7</cell><cell>56.4 / 30.1</cell></row><row><cell cols="2">GeoDiffusion 83.8 / 55.0</cell><cell>81.2 / 51.0</cell><cell>81.0 / 50.5</cell><cell cols="2">54.6 / 28.3</cell><cell>57.0 / 28.9</cell><cell>55.8 / 28.9</cell></row><row><cell>ODGEN</cell><cell>83.8 / 55.0</cell><cell>84.4 / 55.2</cell><cell>84.0 / 55.0</cell><cell cols="2">54.6 / 28.3</cell><cell>58.2 / 29.8</cell><cell>62.1 / 31.8</cell></row><row><cell>Annotation</cell><cell>ReCo</cell><cell>GLIGEN</cell><cell>ControlNet</cell><cell cols="2">GeoDiffusion</cell><cell>MIGC</cell><cell>ODGEN</cell></row><row><cell></cell><cell>cat</cell><cell>bottle</cell><cell>cup</cell><cell>cell phone</cell><cell>bed</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>FID (↓) and mAP (↑) of YOLOv5s / YOLOv7 on COCO. FID is computed with 41k synthetic images. For mAP, YOLO models are trained from scratch on 10k synthetic images and validated on 31k real images. ODGEN outperforms all the other methods in terms of both fidelity and trainability. .<ref type="bibr" target="#b59">60</ref> / 11.01 6.70 / 9.42 1.43 / 1.15 5.94 / 9.21 9.54 / 16.01 10.00 / 17.10 18.90 / 24.40 mAP@.50:.95 3.82 / 5.29 3.56 / 4.60 0.52 / 0.38 2.37 / 4.44 4.67 / 8.65 5.42 / 10.20 9.70 / 14.20</figDesc><table><row><cell>Metrics</cell><cell>ReCo</cell><cell cols="2">GLIGEN ControlNet</cell><cell>Geo-Diffusion</cell><cell>MIGC</cell><cell>Instance-Diffusion</cell><cell>ODGEN</cell></row><row><cell>FID</cell><cell>18.36</cell><cell>26.15</cell><cell>25.54</cell><cell>30.00</cell><cell>21.82</cell><cell>23.29</cell><cell>16.16</cell></row><row><cell>mAP@.50 7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>mAP@.50 (↑) and mAP@.50:.95 (↑) of YOLOv5s / YOLOv7 on the COCO dataset. Baseline models are trained with 80k images from the COCO training set, whereas the other models are trained with the same 80k real + 20k synthetic images. ODGEN outperforms baseline and the other methods.We use 10k annotations randomly sampled from the COCO validation set to generate a synthetic dataset of 10k images. The YOLO models are trained on this synthetic dataset from scratch and evaluated on the other 31k images in the COCO validation set. As shown in Tab. 4, ODGEN achieves significant improvement over the other methods in terms of mAP.We further conduct experiments by adding 20k synthetic images to the 80k training images. We train YOLO models from scratch on the COCO training set (80k images) as the baseline and on the same 80k real images + 20k synthetic images generated by different methods for comparison. We use the labels of 20k images from the COCO validation set as conditions to generate the synthetic set and use the other 21k real images for evaluation. The results are shown in Tab. 5. It shows that ODGEN improves the mAP@.50:95 by 0.5% and outperforms the other methods.</figDesc><table><row><cell>Metrics</cell><cell>Baseline</cell><cell>ReCo</cell><cell>GLIGEN</cell><cell>ControlNet</cell></row><row><cell>mAP@.50</cell><cell>51.5 / 64.5</cell><cell>50.8 / 64.3</cell><cell>50.9 / 64.2</cell><cell>50.1 / 64.3</cell></row><row><cell>mAP@.50:.95</cell><cell>32.6 / 45.4</cell><cell>31.8 / 45.2</cell><cell>31.9 / 45.2</cell><cell>31.0 / 45.2</cell></row><row><cell>Metrics</cell><cell>GeoDiffusion</cell><cell>MIGC</cell><cell>InstanceDiffusion</cell><cell>ODGEN</cell></row><row><cell>mAP@.50</cell><cell>51.2 / 64.4</cell><cell>51.5 / 64.6</cell><cell>51.5 / 64.6</cell><cell>52.1 / 65.0</cell></row><row><cell>mAP@.50:.95</cell><cell>32.1 / 45.2</cell><cell>32.5 / 45.5</cell><cell>32.6 / 45.6</cell><cell>33.1 / 45.9</cell></row><row><cell>4.2.2 Trainability</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>mAP@.50 (↑) and mAP@.50:.95 (↑) of YOLOv5s / YOLOv7 trained from scratch and validated on real or synthetic COCO validation set. 10k images are used for training and the other 31k images are used for validation. Real represents real images in the COCO validation set and synthetic represents images synthesized by our ODGEN using the same labels.</figDesc><table><row><cell cols="4">Train Validate mAP@.50 (↑) mAP@.50:95 (↑)</cell><cell>Train</cell><cell cols="3">Validate mAP@.50 (↑) mAP@.50:95 (↑)</cell></row><row><cell>Real</cell><cell>Real</cell><cell>29.40 / 41.70</cell><cell>15.80 / 26.30</cell><cell>Synthetic</cell><cell>Real</cell><cell>18.90 / 24.40</cell><cell>9.70 / 14.20</cell></row><row><cell cols="3">Real Synthetic 29.40 / 39.90</cell><cell>15.00 / 23.20</cell><cell cols="3">Synthetic Synthetic 37.90 / 45.40</cell><cell>20.40 / 27.10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Using the image list (IL) and the text list (TL) benefits FID and mAP (YOLOv5s / YOLOv7). They are especially helpful for the Road Traffic dataset which has more categories and occlusions.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Table 8: Proper γ values in Eq. (2) benefit</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">both FID and mAP (YOLOv5s / YOLOv7) of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">synthetic images, while overwhelming values</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">lead to degeneration.</cell><cell></cell></row><row><cell cols="5">Datasets IL TL FID(↓) mAP@.50:.95 (↑)</cell><cell>Datasets</cell><cell>γ</cell><cell cols="2">FID (↓) mAP@.50:.95 (↑)</cell></row><row><cell></cell><cell>×</cell><cell>×</cell><cell>98.29</cell><cell>44.6 / 39.9</cell><cell></cell><cell>0</cell><cell>83.94</cell><cell>30.8 / 34.5</cell></row><row><cell>MRI</cell><cell>×</cell><cell>✓</cell><cell>95.67</cell><cell>44.9 / 41.4</cell><cell>Aqua-</cell><cell>25</cell><cell>83.07</cell><cell>32.2 / 38.5</cell></row><row><cell>Image</cell><cell>✓</cell><cell>×</cell><cell>99.16</cell><cell>46.2 / 41.0</cell><cell>rium</cell><cell>50</cell><cell>87.00</cell><cell>32.1 / 38.0</cell></row><row><cell></cell><cell>✓</cell><cell>✓</cell><cell>93.82</cell><cell>46.1 / 41.5</cell><cell></cell><cell>100</cell><cell>90.13</cell><cell>29.9 / 35.7</cell></row><row><cell></cell><cell>×</cell><cell>×</cell><cell>67.40</cell><cell>25.5 / 35.4</cell><cell></cell><cell>0</cell><cell>66.65</cell><cell>36.9 / 39.5</cell></row><row><cell>Road</cell><cell>×</cell><cell>✓</cell><cell>66.48</cell><cell>26.5 / 37.0</cell><cell>Road</cell><cell>25</cell><cell>63.52</cell><cell>39.2 / 43.8</cell></row><row><cell>Traffic</cell><cell>✓</cell><cell>×</cell><cell>65.80</cell><cell>32.3 / 39.1</cell><cell>Traffic</cell><cell>50</cell><cell>66.46</cell><cell>36.6 / 38.9</cell></row><row><cell></cell><cell>✓</cell><cell>✓</cell><cell>63.52</cell><cell>39.2 / 43.8</cell><cell></cell><cell>100</cell><cell>72.18</cell><cell>32.9 / 37.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell cols="3">: mAP (YOLOv5s / YOLOv7) of</cell></row><row><cell cols="3">the corrupted label filtering for ODGEN.</cell></row><row><cell>Datasets</cell><cell>Label filtering</cell><cell>mAP@.50:.95 (↑)</cell></row><row><cell>Cotton</cell><cell>✓ ×</cell><cell>42.0 / 43.2 40.5 / 42.1</cell></row><row><cell>Robo-</cell><cell>✓</cell><cell>39.6 / 34.7</cell></row><row><cell>master</cell><cell>×</cell><cell>39.0 / 33.3</cell></row><row><cell>Under-</cell><cell>✓</cell><cell>19.2 / 22.0</cell></row><row><cell>water</cell><cell>×</cell><cell>18.9 / 21.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Ablations of using real data only and using synthetic data only during detector training.</figDesc><table><row><cell cols="2">(Metric: YOLOv5s / YOLOv7)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Metrics</cell><cell></cell><cell>mAP@.50 (↑)</cell><cell></cell><cell></cell><cell cols="2">mAP@.50:.95 (↑)</cell></row><row><cell>real + synth #</cell><cell>200 + 0</cell><cell>0 + 5000</cell><cell>200 + 5000</cell><cell>200 + 0</cell><cell>0 + 5000</cell><cell>200 + 5000</cell></row><row><cell>Robomaster</cell><cell>50.3 / 48.8</cell><cell>35.9 /37.2</cell><cell>67.4 / 65.4</cell><cell cols="2">27.2 / 26.5 18.7 / 21.0</cell><cell>39.6 / 34.7</cell></row><row><cell>MRI Image</cell><cell cols="2">55.1 / 44.7 59.8 / 55.2</cell><cell>68.1 / 61.3</cell><cell cols="2">37.6 / 27.4 41.3 / 35.3</cell><cell>46.1 / 41.5</cell></row><row><cell>Cotton</cell><cell cols="2">29.4 / 32.0 43.1 / 43.2</cell><cell>63.9 / 55.0</cell><cell cols="2">16.7 / 20.5 29.5 / 28.1</cell><cell>42.0 / 43.2</cell></row><row><cell>Aquarium</cell><cell cols="2">53.8 / 53.0 45.2 / 41.1</cell><cell>62.6 / 66.5</cell><cell cols="2">30.0 / 29.6 23.8 / 20.9</cell><cell>32.2 / 38.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 15 :</head><label>15</label><figDesc>FID (↓) results of foreground objects in the Robomaster dataset synthesized by models fine-tuned on different data.</figDesc><table><row><cell>Object categories</cell><cell>watcher</cell><cell>armor</cell><cell>car</cell><cell>base</cell><cell>rune</cell></row><row><cell>Entire images only</cell><cell>384.94</cell><cell cols="4">532.56 364.46 325.79 340.11</cell></row><row><cell>Entire images and cropped objects</cell><cell>124.45</cell><cell cols="4">136.27 135.66 146.50 128.95</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 16 :</head><label>16</label><figDesc>FID (↓) results of foreground objects in the Road Traffic dataset synthesized by models fine-tuned on different data.</figDesc><table><row><cell>Object categories</cell><cell>vehicle</cell><cell>traffic light</cell><cell>motor-cycle</cell><cell>fire hydrant</cell><cell>cross-walk</cell><cell>bus</cell><cell>bicycle</cell></row><row><cell>Entire images only</cell><cell cols="3">241.64 240.55 213.90</cell><cell>205.22</cell><cell cols="3">253.40 238.63 153.76</cell></row><row><cell cols="2">Entire images and cropped objects 105.27</cell><cell>90.71</cell><cell>143.30</cell><cell>53.05</cell><cell>97.33</cell><cell>118.58</cell><cell>65.61</cell></row><row><cell>D Implementation Details</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>D.1 Global Text Prompt</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 17 :</head><label>17</label><figDesc>Detailed input and output channels of each convolutional layer in the image encoder used in ODGEN. As the maximum object number per image for different datasets varies, we select different N and thus different channel numbers.</figDesc><table><row><cell>Datasets</cell><cell>N</cell><cell cols="8">Layer 1 Input Output Input Output Input Output Input Output Layer 2 Layer 3 Layer 4</cell></row><row><cell>Apex Game</cell><cell>6</cell><cell>18</cell><cell>32</cell><cell>32</cell><cell>96</cell><cell>96</cell><cell>128</cell><cell>128</cell><cell>256</cell></row><row><cell cols="2">Robomaster 17</cell><cell>51</cell><cell>64</cell><cell>64</cell><cell>96</cell><cell>96</cell><cell>128</cell><cell>128</cell><cell>256</cell></row><row><cell>MRI Image</cell><cell>2</cell><cell>6</cell><cell>16</cell><cell>16</cell><cell>32</cell><cell>32</cell><cell>96</cell><cell>96</cell><cell>256</cell></row><row><cell>Cotton</cell><cell>27</cell><cell>81</cell><cell>96</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>1</head><label></label><figDesc>Convert bounding boxes to a mask for ControlNet M = zeros(H, W , K) for i = 1, . . . , B do coord lef t , coord top , coord right , coord bottom = bbox[i] k = cls[i] M [coord top : coord bottom , coord lef t : coord right , k]+ = 1 end for</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 18 :</head><label>18</label><figDesc>The number of images in the 7 subsets of Roboflow-100 used to compose the RF7 datasets. We employ the RF7 datasets from Roboflow-100<ref type="bibr" target="#b5">[6]</ref> to evaluate the performance of our ODGEN on specific domains. Here, we add more details of the employed datasets, including Apex Game, Robomaster, MRI Image, Cotton, Road Traffic, Aquarium, and Underwater. We provide the number of images for each dataset, including the train, validation, and test sets in Tab. 18. We use the first 200 samples in the image list annotation provided by Roboflow-100 for each dataset to build the training set for our experiments. The full validation set is used as the standard to choose the best checkpoint from YOLO models trained for 100 epochs. The full test set is used to evaluate the chosen checkpoint and provide mAP results shown in this paper. Visualized examples with annotations are shown in Fig.9. The object categories of each dataset are provided in Tab.<ref type="bibr" target="#b18">19</ref>. Some images in the Apex Game and Robomaster datasets contain complex and tiny text that are hard to generate by current Stable Diffusion.</figDesc><table><row><cell>Datasets</cell><cell cols="3">Train Validation Test</cell></row><row><cell cols="2">Apex Game 2583</cell><cell>415</cell><cell>691</cell></row><row><cell cols="2">Robomaster 1945</cell><cell>278</cell><cell>556</cell></row><row><cell>MRI Image</cell><cell>253</cell><cell>39</cell><cell>79</cell></row><row><cell>Cotton</cell><cell>367</cell><cell>20</cell><cell>19</cell></row><row><cell>Road Traffic</cell><cell>494</cell><cell>133</cell><cell>187</cell></row><row><cell>Aquarium</cell><cell>448</cell><cell>63</cell><cell>127</cell></row><row><cell cols="2">Underwater 5320</cell><cell>760</cell><cell>1520</cell></row><row><cell>E Datasets Details</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RF7 datasets.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 22 :</head><label>22</label><figDesc>FID (↓) and mAP (↑) of YOLOv5s / YOLOv7 for ODGEN trained on COCO. FID is computed with 41k synthetic images. For mAP, YOLO models are trained from scratch on 10k synthetic images and validated on 31k real images. Different offline image libraries are applied for inference.</figDesc><table><row><cell>Offline Image Library</cell><cell cols="3">FID (↓) mAP@.50 (↑) mAP@.50:95 (↑)</cell></row><row><cell>Same as training</cell><cell>16.01</cell><cell>18.90 / 9.70</cell><cell>24.40 / 14.20</cell></row><row><cell>Different from training</cell><cell>16.16</cell><cell>18.60 / 9.52</cell><cell>24.20 / 14.10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>model to identify whether the contents in synthesized images are consistent with text prompts. The results are provided in Tab. 23. Our ODGEN outperforms other methods and gets results close to ground truth (real images from the COCO validation set sharing the same labels with synthetic images).</figDesc><table /><note><p>ODGEN trained on COCO with Stable Diffusion v1.5. Our ODGEN is implemented with Stable Diffusion v2.1 in Sec. 4. We add experiments of implementing ODGEN with Stable Diffusion v1.5 Figure 12: Visualized samples containing novel categories of foreground objects generated by ODGEN trained on the COCO dataset. Stable Diffusion is used to generate images of the foreground objects to build image lists for inference. It shows that our ODGEN can control the layout of novel categories that were never seen in its training process.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 23 :</head><label>23</label><figDesc>BLIP-VQA (↑) results averaged over 41k images synthesized with different methods using the labels from the COCO validation set.</figDesc><table><row><cell>Method</cell><cell>ReCo</cell><cell>GLIGEN</cell><cell>ControlNet</cell><cell>GeoDiffusion</cell></row><row><cell cols="2">BLIP-VQA (↑) 0.2027</cell><cell>0.2281</cell><cell>0.2461</cell><cell>0.2114</cell></row><row><cell>Method</cell><cell cols="2">MIGC InstanceDiffusion</cell><cell>ODGEN</cell><cell>Ground Truth (reference)</cell></row><row><cell cols="2">BLIP-VQA (↑) 0.2314</cell><cell>0.2293</cell><cell>0.2716</cell><cell>0.2745</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 24 :</head><label>24</label><figDesc>FID (↓) and mAP (↑) of YOLOv5s / YOLOv7 on COCO. FID is computed with 41k synthetic images. For mAP, YOLO models are trained from scratch on 10k synthetic images and validated on 31k real images.</figDesc><table><row><cell>Method</cell><cell cols="3">FID (↓) mAP@.50 (↑) mAP@.50:95 (↑)</cell></row><row><cell>ODGEN based on Stable Diffusion v2.1</cell><cell>16.16</cell><cell>18.90 / 24.40</cell><cell>9.70 / 14.20</cell></row><row><cell>ODGEN based on Stable Diffusion v1.5</cell><cell>15.93</cell><cell>18.20 / 24.50</cell><cell>9.39 / 14.30</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Corrupted label filtering. We design this step to filter some labels when objects are not generated successfully, which may be caused by some unreasonable boxes obtained with the pipeline in Fig. <ref type="figure">3</ref>.</p><p>For experiments on COCO, this step is not applied to any method since we directly use labels from the COCO validation set and hope to synthesize images consistent with the real-world labels.</p><p>For RF7 experiments, this step is only applied to our ODGEN in Tab. 2. For fair comparison, we skip this step on RF7 to compare the generation capability of different methods fairly and provide results in Tab. <ref type="bibr" target="#b12">13</ref>. It shows that the corrupted label filtering step only contributes a small part of the improvement. Without this step, our method still outperforms the other methods significantly. In addition, as illustrated in Appendix A, the current dataset synthesis pipeline is designed to compare different methods and can be improved further in future work.</p><p>Foreground region enhancement on COCO. We set γ as 10 in Eq. ( <ref type="formula">2</ref>) in our experiments trained on COCO. We add ablations with γ value of 0 to ablate foreground region enhancement on COCO and provide quantitative results in Tab. <ref type="bibr" target="#b13">14</ref>. It shows that the foreground region enhancement helps ODGEN achieve better results, especially the layout-image consistency reflected by mAP results. the same position as those in generated images to build image lists. The objects in image lists and synthesized images may all be apples or cakes but have different shapes and colors, as shown by the visualized samples provided in Fig. <ref type="figure">10</ref>.</p><p>Detector performance improvement with synthetic data. We provide several visualized samples in Fig. <ref type="figure">11</ref> and show that the synthetic data helps detectors detect some occluded objects.</p><p>Generalization to novel categories. We use ODGEN trained on COCO to synthesize samples containing subjects not included in COCO, like moon, ambulance, tiger, et al. We show visualized samples in Fig. <ref type="figure">12</ref>. It shows that ODGEN trained on COCO can control the layout of novel categories. However, we find that the generation quality is not very stable, which may be influenced by the fine-tuning process on the COCO dataset. In future work, we hope to get a powerful and generic model that is robust to the most common categories in daily life.</p><p>Computational cost. Our ODGEN needs to synthesize images of foreground objects to build image lists, leading to higher computational costs for training and inference. Therefore, we propose to generate an offline image library of foreground objects to accelerate the process of building image lists. With the offline library, we can randomly pick images from it to build image lists instead of synthesizing new images of foreground objects every time. In the inference stage (Fig. <ref type="figure">2(c</ref>)), our ODGEN pads both the image and text lists to a fixed length. Therefore, the computational cost for inference with an offline library doesn't increase significantly with more foreground objects. Other methods like InstanceDiffusion <ref type="bibr" target="#b54">[55]</ref> and MIGC <ref type="bibr">[67]</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Synthetic data from diffusion models improves imagenet classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.01324</idno>
		<title level="m">Text-to-image diffusion models with an ensemble of expert denoisers</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multidiffusion: fusing diffusion paths for controlled image generation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bar-Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yariv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning</title>
		<meeting>the 40th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1737" to="1752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Geodiffusion: Text-prompted geometric con-trol for object detection data generation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Ciaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Zuppichini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Guerrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcquade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Solawetz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.13523</idno>
		<title level="m">Roboflow 100: A rich, multi-domain object detection benchmark</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Diversify your vision datasets with automatic diffusion-based augmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dunlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Umino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modeling visual context is key to augmenting object detection datasets</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="364" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Data augmentation for object detection via controllable diffusion models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-M</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1257" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Instagen: Enhancing object detection by training on synthetic dataset</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="14121" to="14130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Training-free structured diffusion guidance for compositional text-to-image synthesis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Akula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Narayana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">E</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Dall-e for detection: Language-driven compositional image synthesis for object detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.09592</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2918" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Prompting diffusion representations for cross-domain semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mangas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.02138</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Is synthetic data from generative models ready for image recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">High-resolution complex scene synthesis with transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.06458</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dginstyle: Domaingeneralizable semantic segmentation with image diffusion models and stylized semantic control</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Obukhov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Synthetic Data for Computer Vision Workshop@ CVPR 2024</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Jiaxin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tianjun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.08908</idno>
		<title level="m">Layoutdiffuse: Adapting foundational diffusion models for layout-to-image generation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Jocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stoken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Borovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yifu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Montes</surname></persName>
		</author>
		<title level="m">ultralytics/yolov5: v6. 2-yolov5 classification models, apple m1, reproducibility, clearml and deci. ai integrations</title>
		<imprint>
			<publisher>Zenodo</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Alias-free generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Härkönen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="852" to="863" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Variational diffusion models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="21696" to="21707" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Text-image alignment for diffusionbased perception</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kondapaneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Knott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guimaraes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="13883" to="13893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Blip: Bootstrapping language-image pre-training for unified visionlanguage understanding and generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12888" to="12900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gligen: Open-set grounded text-to-image generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="22511" to="22521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Is synthetic data from diffusion models ready for knowledge distillation?</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.12954</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image synthesis from layout with locality-aware mask adaption</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="13819" to="13828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Open-vocabulary object segmentation with diffusion models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7667" to="7676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014: 13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">September 6-12, 2014. 2014</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8162" to="8171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Diffusion-based image translation with label guidance for domain adaptive semantic segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="808" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sdxl: Improving latent diffusion models for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Podell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lacey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Penna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<title level="m">Hierarchical text-conditional image generation with clip latents</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dreambooth: Fine tuning text-toimage diffusion models for subject-driven generation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="22500" to="22510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Photorealistic text-to-image diffusion models with deep language understanding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ghasemipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Gontijo</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Karagol Ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="36479" to="36494" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fake it till you make it: Learning transferable representations from synthetic imagenet clones</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Sarıyıldız</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="8011" to="8021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Laion-5b: An open large-scale dataset for training next generation image-text models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cherti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mullis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="25278" to="25294" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Improved techniques for training score-based generative models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12438" to="12448" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Effective data augmentation with diffusion models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Trabucco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Gurinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Nvae: A deep hierarchical variational autoencoder</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="19667" to="19679" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Yolov7: Trainable bag-of-freebies sets new state-of-theart for real-time object detectors</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7464" to="7475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Instancediffusion: Instance-level control for image generation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Rambhatla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="6232" to="6242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Datasetdm: Synthesizing data with perception annotations using diffusion models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="54683" to="54695" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Diffumask: Synthesizing images with pixellevel annotations for semantic segmentation using diffusion models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1206" to="1217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7452" to="7461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Open-vocabulary panoptic segmentation with text-to-image diffusion models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">De</forename><surname>Mello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2955" to="2966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Freemask: Synthetic images with dense annotations make stronger segmentation models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Reco: Regioncontrolled text-to-image generation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="14246" to="14255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Adding conditional control to text-to-image diffusion models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3836" to="3847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Diffusionengine: Diffusion model is scalable data engine for object detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.03893</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Datasetgan: Efficient labeled data factory with minimal human effort</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Lafleche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10145" to="10155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">X-paste: revisiting scalable copy-paste for instance segmentation using clip and stablediffusion</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="42098" to="42109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Layoutdiffusion: Controllable diffusion model for layout-to-image generation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="22490" to="22499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Migc: Multi-instance generation controller for text-to-image synthesis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="6818" to="6828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Isolated diffusion: Optimizing multi-concept text-to-image generation training-freely with isolated diffusion guidance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.16954</idno>
		<idno>. 96 128 128 192 192 256</idno>
	</analytic>
	<monogr>
		<title level="j">Road Traffic</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">57</biblScope>
			<biblScope unit="page" from="297" to="297" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>COCO</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">-layer convolutional layer to compress the information into one embedding</title>
		<imprint/>
	</monogr>
	<note>The input channel is N and the output channels of following layers are ⌊N/2⌋, ⌊N/4⌋, ⌊N/8⌋, 1. We set the convolutional kernel size as 3, the stride as 1, and use zero padding to maintain the sizes of the last two dimensions. Foreground/Background discriminator. We train discriminators by fine-tuning ImageNet pretrained ResNet50 on foreground and background patches randomly cropped from training sets. They are split for training, validating, and testing by 70%, 10%, and 20%. The model is a binary classification model that only predicts whether a patch contains any object. The accuracy on test datasets is over 99% on RF7. Therefore, we can confidently use it to filter the pseudo labels.</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">3 Other Methods ReCo</title>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title/>
		<author>
			<persName><surname>Migc</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title/>
		<author>
			<persName><surname>Geodiffusion</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
